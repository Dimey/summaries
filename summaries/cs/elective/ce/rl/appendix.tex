\chapter{Self-Test Questions}
	\section{Questions}
		\subsection{Introduction}
			\begin{enumerate}
				\item Why is \ac{RL} crucial for \ac{AI}?
				\item Why are all other approaches probably doomed?
				\item What are the basic characteristics of \ac{RL}?
				\item How can \ac{RL} problems be classified?
				\item What are the core components of \ac{RL} algorithms?
			\end{enumerate}
		% end

		\subsection{Markov Decision Processes}
			\begin{enumerate}
				\item What is a \ac{MRP}?
				\item What is a \ac{MDP}?
				\item What is a value function and how to compute it?
				\item What is an optimal policy?
				\item What is the Bellman equation and how to compute it?
				\item What is the Bellman expectation equation?
				\item What is the Bellman optimality equation?
				\item RoLe: What is an \ac{MDP}, a policy, a value function, a state-action value function?
				\item RoLe: What is the Bellman equation?
				\item Sutton (3.2): Is the \ac{MDP} framework adequate to usefully represent \emph{all} goal-directed learning tasks? Can you think of any clear exceptions?
				\item Sutton (3.6): Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except \(-1\) upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?
				\item Sutton (3.7): Image that you are designing a robot to run a maze. You decide to give it a reward of \(+1\) for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes---the successive runs through the maze---so you decide to treat it as an episodic task, where the goal is to maximize expected total reward. After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?
				\item Sutton (3.8): Suppose \(\gamma = 0.5\) and the following sequence of rewards is received: \( r_1 = -1 \), \( r_2 = 2 \), \( r_3 = 6 \), \( r_4 = 3 \), and \( r_5 = 2 \), with \( T = 5 \). What are \( J_0 \) to \( J_5 \)? Hint: Work backwards.
				\item Sutton (3.9): Suppose \(\gamma = 0.9\) and the reward sequence is \(r_1 = 2\) followed by an infinite sequence of \(7\)s. What are \(J_1\) and \(J_0\)?
				\item Sutton (3.12): Give an equation for \(V^\pi\) in terms of \(Q^\pi\).
				\item Sutton (3.13): Give an equation for \(Q^\pi\) in terms of \(V^\pi\).
				\item Sutton (3.25): Give an equation for \(V^\ast\) in terms of \(Q^\ast\).
				\item Sutton (3.26): Give an equation for \(Q^\ast\) in terms of \(V^\ast\).
				\item Sutton (3.27): Give an equation for \(\pi^\ast\) in terms of \(Q^\ast\).
				\item Sutton (3.28): Give an equation for \(\pi^\ast\) in terms of \(V^\ast\).
			\end{enumerate}
		% end

		\subsection{Dynamic Programming}
			\begin{enumerate}
				\item What is \ac{DP}?
				\item How to compute optimal policies and value functions for environments with known dynamics?
				\item How to approximate value functions with unknown dynamics?
				\item What are the differences, advantages, and disadvantages of \ac{DP} methods compared to \ac{MC} and \ac{TD} methods?
				\item RoLe: What is policy evaluation, policy improvement, \ac{PI}, and \ac{VI}?
				\item RoLe: What are the main difference between \ac{PI} vs. \ac{VI}?
				\item RoLe: What are the differences of finite and infinite horizon objectives? Give an example in both settings.
			\end{enumerate}
		% end

		\subsection{Monte-Carlo Methods}
			\begin{enumerate}
				\item How to approximate value functions with unknown dynamics?
				\item What are the differences, advantages, and disadvantages of \ac{MC} methods compared to \ac{DP} and \ac{TD} methods?
			\end{enumerate}
		% end

		\subsection{Temporal Difference Learning}
			\begin{enumerate}
				\item What are eligibility traces?
				\item How to compute \acs{TD}(\(\lambda\))?
				\item What are the differences, advantages, and disadvantages of \ac{TD} methods compared to \ac{DP} and \ac{MC} methods?
				\item RoLe: What are \(Q\) and \(V\) functions?
				\item RoLe: What is \ac{TD} learning? How to derive it?
				\item RoLe: What does on- and off-policy mean?
				\item Sutton (6.11): Why is Q-learning considered an \emph{off-policy} control method?
				\item Sutton (6.12): Suppose action selection is greedy. Is Q-learning then exactly the same algorithm as \ac{SARSA}? Will they make exactly the same action selections and weight updates?
			\end{enumerate}
		% end

		\subsection{Tabular Reinforcement Learning}
			\begin{enumerate}
				\item What is the difference between on- and off-policy learning?
				\item What is the relation of model-free control to generalized \ac{PI}?
				\item What are the sufficient conditions for an effective exploration strategy?
				\item How can \(\varepsilon\)-greedy be used for exploration?
				\item What is \ac{SARSA} and how is it used to do on-policy control?
				\item How to perform off-policy learning with importance sampling?
				\item How to perform off-policy learning with Q-learning without importance sampling?
				\item What are the relationships of the Bellman equations and the \ac{TD} targets?
				\item RoLe: What is the difference between Q-learning and \ac{SARSA}?
				\item RoLe: When do value function methods work well?
				\item Sutton (2.1): In \(\varepsilon\)-greedy action selection, for the case of two actions and \(\varepsilon = 0.5\), what is the probability that the greedy action is selected?
			\end{enumerate}
		% end

		\subsection{Function Approximation}
			\begin{enumerate}
				\item What are continuous problems in \ac{RL}?
				\item Why do we need function approximation?
				\item How can function approximation be used in \ac{RL}?
				\item What are the consequences of using function approximation in \ac{RL}?
				\item What are the challenges of off-policy training with function approximation?
				\item RoLe: What are the problems of \ac{DP}?
				\item RoLe: Can we use function approximation?
				\item RoLe: How do batch methods work?
				\item RoLe: How to derive \ac{LSTD}?
				\item RoLe: Why do value function methods often fail for high-dimensional continuous actions?
			\end{enumerate}
		% end

		\subsection{Policy Search}
			\begin{enumerate}
				\item What are the differences between value-based, policy search, and actor-critic methods?
				\item Why is exploration important in policy search? Why do we use Gaussian policies?
				\item What are the three big approaches for computing policy gradients? What is there core idea?
				\item How can we use the \ac{FIM} to compute the \ac{NG}?
				\item What is the \ac{PGT} and its connection to value-based and actor-critic methods?
				\item How to derive the \ac{eNAC} algorithm?
				\item RoLe: How do finite difference gradient estimators work?
				\item RoLe: What are likelihood-ratio gradient estimators?
				\item RoLe: Why do baselines lower the variance of the gradient estimate?
				\item RoLe: Why is the \ac{FIM} so important? How does it relate to the \ac{KL}?
				\item RoLe: What is the \ac{NG}? Why is the \ac{NG} invariant to reparametrization?
				\item RoLe: What is the \ac{CFA}? How is it connected to the \ac{NG}?
			\end{enumerate}
		% end

		\subsection{Deep Value-Function Methods}
			\begin{enumerate}
				\item What is the curse of dimensionality? How does it affect \ac{RL}?
				\item How can deep learning methods be used in \ac{RL}?
				\item What are the problems of deep \ac{RL} and what are some techniques to address them?
				\item What is the \ac{DQN} algorithm?
				\item How to enhance \ac{DQN} by improving function estimation and use of samples?
				\item How can we combine techniques from deep learning with \ac{DQN} to improve key problems of \ac{RL}, e.g., exploration?
			\end{enumerate}
		% end

		\subsection{Deep Actor-Critic}
			\begin{enumerate}
				\item What are the difficulties of using \ac{PGT} in practice?
				\item How does deep \ac{RL} simplify the problem using surrogate objectives?
				\item What are the three big on-policy approaches we discussed and what are their core features?
				\item How can we compute the \ac{NG} for large \acp{NN}?
				\item What are the three big off-policy approaches we discussed and what are their core features?
			\end{enumerate}
		% end

		\subsection{Frontiers}
			\begin{enumerate}
				\item What are \acp{POMDP}?
				\item What are belief states and how to update them?
				\item What is an \ac{SMDP}?
				\item What are the three big frameworks for hierarchical \ac{RL} and what are their core features?
				\item How to extend the concept of (optimal) value functions to the option framework?
				\item What is an \ac{MDP} without rewards?
				\item What is intrinsic motivation and why is it useful in \ac{RL} in general?
				\item What is inverse \ac{RL}? What is the basic scheme of it?
				\item Why is inverse \ac{RL} ill-posed?
				\item What is a model and how does \ac{MBRL} exploit it?
				\item What type of planner can we select in \ac{MBRL}?
				\item What are the key issues of \ac{MBRL}?
			\end{enumerate}
		% end
	% end
% end
