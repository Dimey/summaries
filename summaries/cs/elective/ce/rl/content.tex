\chapter{Introduction}
	In this course we will look at lots of methods from the domain of \emph{\ac{RL}.} \ac{RL} is an approach for agent-oriented learning where the agent learns by repeatedly acting with the environment and from rewards. Also, it does not know how the world works in advance. \ac{RL} is therefore close to how humans learn and tries to tackle the fundamental challenge of \ac{AI}:
	\begin{center}
		"The fundamental challenge in \acl{AI} and \acl{ML} is learning to make good decisions under uncertainty." (Emma Brunskill)
	\end{center}
	\ac{RL} is so general that every \ac{AI} problem can be phrased in its framework of learning by interacting. However, the typical setting is that at every time step, an agent perceives the state of the environment and chooses an action based on these perceptions. Subsequently, the agent gets a numerical reward and tries to maximize this reward by finding a suitable strategy. This procedure is illustrated in \autoref{fig:rl}.

	\section{Artificial Intelligence}
		The core question of \ac{AI} is how to build "intelligent" machines, requiring that the machine is able to adapt to its environment and handle unstructured and unseen environments. Classically, \ac{AI} was an "engine" producing answers to various queries based on rules designed by a human expert in the field. In (supervised) \ac{ML}, the rules are instead learned from a (big) data set and the "engine" produces answers based on the data. However, this approach (leaning from labeled data) is not sufficient for \ac{RL} as demonstrations might be imperfect, the correspondence problem, and that we cannot demonstrate everything. We can break these issues down as follows: supervised learning does not allow "interventions" (trial-and-error) and evaluative feedback (reward).

		The core idea leading to \ac{RL} was to not program machines to simulate an adult brain, but to simulate a child's brain that is still learning. \ac{RL} formalizes this idea of intelligence to interpret rich sensory input and choosing complex actions. We know that this may be possible as us humans do it all the time. This lead to the \ac{RL} view on \ac{AI} depicted in \autoref{fig:rl} and is based on the hypothesis that learning from a scalar reward is sufficient to yield intelligent behavior (Sutton and Barto, 2018).

		\begin{figure}
			\centering
			\begin{tikzpicture}[->, cmp/.style = { draw, rectangle, minimum width = 2.5cm, minimum height = 0.75cm }]
				\node [cmp] (agent) {Agent};
				\node [cmp, right = 2 of agent] (env) {Environment};
				\coordinate [above = 0.5 of agent] (a);
				\coordinate [above = 0.5 of env] (b);

				\coordinate [left = 0.5 of agent.south] (agentL);
				\coordinate [right = 0.5 of agent.south] (agentR);
				\coordinate [below = 1.25 of agentL] (fL);
				\coordinate [below = 0.50 of agentR] (fR);

				\coordinate [left = 0.5 of env.south] (envL);
				\coordinate [right = 0.5 of env.south] (envR);
				\coordinate [below = 0.50 of envL] (eL);
				\coordinate [below = 1.25 of envR] (eR);

				\draw (agent) to (a) to node[above]{Action \(a_t\)} (b) to (env);
				\draw (envL) to (eL) to node[below]{Reward \( r_t \gets r_{t + 1} \)} (fR) to (agentR);
				\draw (envR) to (eR) to node[below]{State \( s_t \gets s_{t + 1} \)} (fL) to (agentL);
			\end{tikzpicture}
			\caption{The Reinforcement Learning Cycle}
			\label{fig:rl}
		\end{figure}
	% end

	\section{Reinforcement Learning Formulation}
		\ac{RL} tries to \emph{maximize the long-term reward} by finding a strategy/policy with the general assumption that it is easier to assess a behavior by specifying a cost than specifying the behavior directly. In general, we have the following things different to most (un)supervised settings:
		\begin{itemize}
			\item no supervision, but only a reward signal
			\item feedback (reward) is always delayed and not instantaneous
			\item time matters, the data is sequential and by no means i.i.d.
			\item the agent's actions influence the subsequent data, i.e., the agent generates its own data
		\end{itemize}
		In addition to this, \ac{RL} is challenged by a numerous complicated factors and issues, e.g., dynamic state-dependent environments, stochastic and unknown dynamics and rewards, exploration vs. exploitation, delayed rewards (how to assign a temporal credit), and very complicated systems (large state spaces with unstructured dynamics). For designing an \ac{RL}-application, we usually have to choose the state representation, decide how much prior knowledge we want to put into the agent, choose an algorithm for learning, design an objective function, and finally decide how we evaluate the resulting agent. By all these decisions, we want to reach a variety of goals, e.g., convergence, consistency, good generalization abilities, high learning speed (performance), safety, and stability. However, we are usually pretty restricted in terms of computation time, available data, restrictions in the way we act (e.g., safety constraints), and online vs. offline learning.

		This sounds like a lot and, in fact, is! We therefore often limit ourselves onto specific (probably simpler) sub-problems and solve them efficiently under some assumptions. Some common flavors of the \ac{RL} problem are, for instance:
		\begin{itemize}
			\item \emph{Full:} no additional assumptions, the agent can only probe the environment through the state dynamics and its actions; the agent has to understand the environment
			\item \emph{Filtered State and Sufficient Statistics:} assumption of a local Markov property (i.e., the next state only depends on the current state and action, and not on the past), decomposable rewards (into specific time steps); we can show that every problem is a (probably infinite) instance of this assumption, but how to filter the state to get such properties?
			\item \emph{Markovian Observable State:} assume that we can observe the state fulfilling the Markov property directly
			\item \emph{Further Simplifications:} contextual bandits (the dynamics do not depend on the action or the past and current state at all); bandits (only a single state)
		\end{itemize}
		We can summarize the different \ac{RL}-like problems in a matrix, see \autoref{tab:problemClassification}.

		\begin{table}
			\centering
			\begin{tabular}{c||cc}
				\toprule
				            & actions \emph{do not} change the state of the world & actions change the state of the world \\ \midrule
				 no model   &                (Multi-Armed) Bandits                &        Reinforcement Learning         \\
				known model &                   Decision Theory                   &       Optimal Control, Planning       \\ \bottomrule
			\end{tabular}
			\caption{Problem Classification}
			\label{tab:problemClassification}
		\end{table}

		\subsection{Components}
			To solve an \ac{RL} problem, we need three ingredients:
			\begin{enumerate}
				\item Model Learning
					\begin{itemize}
						\item we want to approximate and learn the state transfer using methods from supervised learning
						\item need to generate actions for model identification
						\item estimation of the model or the model's parameters
					\end{itemize}
				\item Optimal Control/Planning
					\begin{itemize}
						\item generation of optimal control inputs
					\end{itemize}
				\item Performance Evaluation
			\end{enumerate}
		% end
	% end

	\section{Wrap-Up}
		\begin{itemize}
			\item why \ac{RL} is crucial for \ac{AI} and why all other approaches are ultimately doomed
			\item background and characteristics of \ac{RL}
			\item classification of \ac{RL} problems
			\item core components of \ac{RL} algorithms
		\end{itemize}
		% TODO: Reading Assignment: Sutton, Chapter 1
	% end
% end

\chapter{Preliminaries}
	In this chapter we cover some preliminaries that are necessary for understanding the rest of the course. Note that most of this content is dense and should be used as a reference throughout this course as oppose to an actual introduction to the topic.

	\section{Functional Analysis}
		\begin{definition}[Normed Vector Space]
			A \emph{normed vector space} is a vector space \(\mathcal{X}\) over \(X\) equipped with a \emph{norm} \( \lVert \cdot \rVert : \mathcal{X} \to \R \) that has the following properties:
			\begin{enumerate}
				\item \( \lVert x \rVert \geq 0 \) for all \( x \in \mathcal{X} \) and \( \lVert x \rVert = 0 \) iff \( x = 0 \) (non-negativity)
				\item \( \lVert \alpha x \rVert = \lvert \alpha \rvert \lVert x \rVert \) for all \( \alpha \in X \) and \( x \in \mathcal{X} \) (homogenity)
				\item \( \lVert x_1 + x_2 \rVert \leq \lVert x_1 \rVert + \lVert x_2 \rVert \) for all \( x_1, x_2 \in \mathcal{X} \) (triangle inequality)
			\end{enumerate}
		\end{definition}
		For the rest of this course we usually use real finite-dimensional vectors spaces \( \mathcal{X} = \R^d \), \( d \in \N^+ \), the \(L_\infty\)-norm \( \lVert \cdot \rVert_\infty \), and (weighted) \(L_2\)-norms \( \lVert \cdot \rVert_{2, \rho} \).

		\begin{definition}[Complete Vector Space]
			A vector space \(\mathcal{X}\) is \emph{complete} if every Cauchy sequence\footnote{This section is already overflowing with mathematical rigor compared to the rest of the course, so we will skip the definition of a Cauchy sequence here.} in \(\mathcal{X}\) has a limit in \(\mathcal{X}\).
		\end{definition}

		\begin{definition}[Contraction Mapping]
			Let \(\mathcal{X}\) be a vector space equipped with a norm \(\lVert \cdot \rVert\). An operator \( T : \mathcal{X} \to \mathcal{X} \) is called an \emph{\(\alpha\)-contraction mapping} if \( \exists \alpha \in [0, 1) : \forall x_1, x_2 \in \mathcal{X} : \lVert T x_1 - T x_2 \rVert \leq \alpha \lVert x_1 - x_2 \rVert \). If only \( \exists \alpha \in [0, 1] : \forall x_1, x_2 \in \mathcal{X} : \lVert T x_1 - T x_2 \rVert \leq \alpha \lVert x_1 - x_2 \rVert \), \(T\) is called \emph{non-expanding.}
		\end{definition}

		\begin{definition}[Lipschitz Continuity]
			Let \(\mathcal{X}\) and \(\mathcal{Y}\) be vector spaces equipped with norms \(\lVert \cdot \rVert_X\) and \(\lVert \cdot \rVert_Y\), respectively. A function \( f : \mathcal{X} \to \mathcal{Y} \) is called \emph{Lipschitz-continuous} if \( \exists L \geq 0 : \forall x_1, x_2 \in \mathcal{Y} : \lVert f(x_1) - f(x_2) \rVert_Y \leq L \lVert x_1 - x_2 \rVert_X \).
		\end{definition}

		\begin{remark}
			Obviously, every contraction mapping is also Lipschitz-continuous with Lipschitz-constant \(L \triangleq \alpha\) and is therefore continuous. Also, the product of two Lipschitz-continuous mappings is Lipschitz-continuous and therefore \(T^n = T \circ \dots \circ T\) is Lipschitz-continuous, too.
		\end{remark}

		\begin{definition}[Fixed Point]
			Let \(\mathcal{X}\) be a vector space equipped and let \( T : \mathcal{X} \to \mathcal{X} \) be an operator. Then \(x \in \mathcal{X}\) is a \emph{fixed point} of \(T\) if \( T x = x \).
		\end{definition}

		\begin{theorem}[Banach Fixed Point Theorem]
			Let \(\mathcal{X}\) be a complete vector space with a norm \(\lVert \cdot \rVert\) and let \( T : \mathcal{X} \to \mathcal{X} \) be an \(\alpha\)-contraction mapping. Then \(T\) has a unique fixed point \(x^\ast \in \mathcal{X}\) and for all \(x_0 \in \mathcal{X}\) the sequence \( x_{n + 1} = T x_n \) converges to \(x^\ast\) geometrically, i.e., \( \lVert x_n - x^\ast \rVert \leq \alpha^n \lVert x_0 - x^\ast \rVert \).
		\end{theorem}
	% end

	\section{Statistics}
		This section introduces some concepts of statistics, but you should

		\subsection{Monte-Carlo Estimation}
			Let \(X\) be a random variable with mean \( \mu = \E[X] \) and variance \( \sigma^2 = \Var[X] \) and let \( \{ x_i \}_{i = 1}^{n} \) be i.i.d. realizations of \(X\). We then have the \emph{empirical mean} \( \hat{\mu}_n = \frac{1}{n} \sum_{i = 1}^{n} x_i \) and we can show that \( \E[\hat{\mu}_n] = \mu \) and \( \Var[\hat{\mu}_n] = \sigma^2/n \). Also, if the sample size \(n\) goes to infinity, we have the \emph{strong} and \emph{weak law of large numbers,} respectively:
			\begin{align}
				P\bigl( \lim\limits_{n \to \infty} \hat{\mu}_n = \mu \bigr) &= 1 &
				\lim\limits_{n \to \infty} P\bigl( \lvert \hat{\mu}_n - \mu \rvert > \epsilon \bigr) &= 0
			\end{align}
			Also, we have the \emph{central limit theorem:} no matter the distribution of \(P\), its mean value converges to a normal distribution, \( \sqrt{n} (\hat{\mu}_n - \mu) \overset{D}{\to} \mathcal{N}(0, \sigma^2) \).
		% end

		\subsection{Bias-Variance Trade-Off}
			When evaluating/training a \ac{ML} model, the error is due to two factors (illustrated in \autoref{fig:biasVariance}):
			\begin{itemize}
				\item \emph{bias,} i.e., the distance to the expected prediction
				\item \emph{variance,} i.e., the variability of a prediction for a given data point
			\end{itemize}
			In general, we want to minimize both, but we can only minimize one of them! This is known as the \emph{bias-variance trade-off.}

			\begin{figure}
				\centering
				\begin{subfigure}{0.3\linewidth}
					\centering
					\includegraphics[width=\linewidth]{figures/low-bias-low-variance}
					\caption{Low Bias, Low Variance}
				\end{subfigure}
				~
				\begin{subfigure}{0.3\linewidth}
					\centering
					\includegraphics[width=\linewidth]{figures/low-bias-high-variance}
					\caption{Low Bias, High Variance}
				\end{subfigure}
				\\[5pt]
				\begin{subfigure}{0.3\linewidth}
					\centering
					\includegraphics[width=\linewidth]{figures/high-bias-low-variance}
					\caption{High Bias, Low Variance}
				\end{subfigure}
				~
				\begin{subfigure}{0.3\linewidth}
					\centering
					\includegraphics[width=\linewidth]{figures/high-bias-high-variance}
					\caption{High Bias, High Variance}
				\end{subfigure}
				\caption[Bias-Variance Trade-Off]{Bias-Variance Trade-Off; Source: Bernhard Thiery (CC BY-SA 3.0)}
				\label{fig:biasVariance}
			\end{figure}
		% end

		\subsection{Important Sampling}
			If we want to estimate the expectation of some function \(f(x)\) for \( x \sim p(x) \), but cannot sample from \(p(x)\) (which is often the case for complicated models), we can instead use the following relation(s):
			\begin{gather}
				\E_{x \sim p}\bigl[ f(x) \bigr]
					= \sum_x f(x) p(x)
					= \sum_x f(x) \frac{p(x)}{q(x)} q(x)
					= \E_{x \sim q} \biggl[ f(x) \frac{p(x)}{q(x)} \biggr] \\
				\E_{x \sim p}\bigl[ f(x) \bigr]
					= \int\! f(x) p(x) \dd{x}
					= \int\! f(x) \frac{p(x)}{q(x)} p(x) \dd{x}
					= \E_{x \sim q} \biggl[ f(x) \frac{p(x)}{q(x)} \biggr]
			\end{gather}
			and sample from a surrogate distribution \(q(x)\). This approach obviously has problems if \(q\) does not cover \(p\) sufficiently well along with other problems. See Bishop, 2006, Chapter 11 for details.
		% end

		\subsection{Linear Function Approximation}
			A basic approximator we will need often is the linear function approximator \( f(\vec{x}) = \vec{w}^\transposed \vec{\phi}(\vec{x}) \) with weights \(\vec{w}\) and features \(\vec{\phi}(\vec{x})\). As the weights are optimized and the features are designed, we have lots of variability here. Actually, constructing useful features is the influential step on the approximation quality. Most importantly, features are the only point where we can introduce interactions between different dimensions. A good representations therefore captures all dimensions and all (possibly complex) interaction.

			We will now go over some frequently used features.

			\paragraph{Polynomial Features}
				\emph{Polynomial features} are particularly simple and capture the interaction between dimensions by multiplication. For instance, the first- and second-order polynomial features of a two-dimensional state \( \vec{x} = (x_1, x_2)^\transposed \) are:
				\begin{align}
					\vec{\phi}_\mathit{P1}(\vec{x}) &= (1, x_1, x_2, x_1 x_2)^\transposed &
					\vec{\phi}_\mathit{P2}(\vec{x}) &= (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2, x_1 x_2^2, x_1^2 x_2, x_1^2, x_2^2)
				\end{align}
				However, the number of features grows \emph{exponentially} with the dimension!
			% end

			\paragraph{Fourier Basis}
				Fourier series can be used to approximate periodic functions by adding sine and cosine waves with different frequencies and amplitudes. Similarly, we can use them for general function approximation of functions with bounded domain. As it is possible to approximate any even function with just cosine waves and we are only interested in bounded domains, we can set this domain to positive numbers only and can therefore approximate any function. For one dimension, the \(n\)-th order \emph{Fourier (cosine) basis} is
				\begin{equation}
					\phi_m(x) = \cos(\pi m \tilde{x}),\quad m = 0, 1, \dots, n.
				\end{equation}
				and \( \tilde{x} \) is a normalized version of \(x\), i.e., \( \tilde{x} = (x - x_\mathrm{max}) / (x_\mathrm{max} - x_\mathrm{min}) \).
			% end

			\paragraph{Coarse Coding}
				\emph{Coarse coding} divides the space into \(M\) different regions and produced \(M\)-dimensional coding features for which the \(j\)-th entry is \num{1} iff the data point lies withing the respective region; all values the data point does not lie in are \num{0}. Features with this codomain are also called \emph{sparse.}
			% end

			\paragraph{Tile Coding}
				\emph{Tile coding} is a computationally efficient form of coarse coding which use square \emph{tilings} of space. It uses \(N\) tilings, each composed of \(M\) tiles. The features "vector" is then an \(N \times M\) matrix where a single value is \num{1} iff \(x\) lies inside the tile and \num{0} otherwise. \autoref{fig:tileCoding} shows an illustration of this coding.

				\begin{figure}
					\centering
					\includegraphics[width=0.9\linewidth]{figures/tile-coding}
					\caption[Tile Coding]{Tile Coding; Source: {\small \url{https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b}}}
					\label{fig:tileCoding}
				\end{figure}
			% end

			\paragraph{Radial Basis Functions}
				\emph{Radial basis functions (\acsp{RBF}\acused{RBF})} are a generalization of coarse coding where the features are in the interval \((0, 1]\). A typical \ac{RBF} is the Gaussian
				\begin{equation}
					\phi_j(\vec{x}) = \exp\Biggl\{ -\frac{\lVert \vec{x} - \vec{c}_j \rVert_2^2}{2 \sigma_j^2} \Biggr\}
				\end{equation}
				with center \(\vec{c}_j\) and bandwidth \( \sigma_j^2 \).
			% end

			\paragraph{Neural Networks}
				A very powerful alternative to hand-crafting features are \emph{\aclp{NN} (\acsp{NN}\acused{NN}).} By stacking multiple layers of learned features, they are very powerful prediction machines.
			% end
		% end

		\subsection{Likelihood-Ratio Trick}
			Suppose we need to differentiate the expectation of some function \( f(x) \) w.r.t. \(\theta\) where \( x \sim p_\theta(\cdot) \). However, we cannot directly calculate \( \E_{x \sim p_\theta}[f(x)] \) or "differentiate through sampling." Instead, we can use the identity
			\begin{equation}
				\dv{z} \log h(z) = \frac{h'(z)}{h(z)}
				\qquad\implies\qquad
				f'(z) = h(z) \, \dv{z} \log h(z)
			\end{equation}
			to reformulate the derivative of the expectation as
			\begin{equation}
				\pdv{\theta} \E_{x \sim p_\theta}[f(x)]
					= \int\! f(x) \pdv{\theta} p_\theta(x) \dd{x}
					= \int\! f(x) \biggl( \pdv{\theta} p_\theta(x) \biggr) p_\theta(x) \dd{x}
					= \E_{x \sim p_\theta} \biggl[ f(x) \, \pdv{\theta} p_\theta(x) \biggr].
			\end{equation}
			While this is a very powerful approach, the gradient estimator exhibits high variance!
		% end

		\subsection{Reparametrization Trick}
			Suppose we need to differentiate the expectation of some function \( f(x) \) w.r.t. \(\theta\) where \( x \sim p_\theta(\cdot) \). However, we cannot directly calculate \( \E_{x \sim p_\theta}[f(x)] \) or "differentiate through sampling." Instead, we reformulate the expectation with a function \( x = g_\theta(\varepsilon) \) that separates the random components \(\varepsilon\) from the deterministic ones \(\theta\) such that we can reparameterize the expectation as
			\begin{equation}
				\E_{x \sim p_\theta}[f(x)] = \E_\varepsilon\bigl[ f\bigl( g_\theta(\varepsilon) \bigr) \bigr].
			\end{equation}
			For instance, if \( p_\theta(x) = \mathcal{N}(\mu_\theta, \sigma_\theta^2) \) is a Gaussian, \( g_\theta(\varepsilon) = \mu_\theta + \sigma_\theta \varepsilon \) with \( \varepsilon \sim \mathcal{N}(0, 1) \). We can now simply use the chain rule to take the derivative w.r.t. \(\theta\). Compared to the likelihood-ratio trick, this estimator has less variance!
		% end
	% end

	\section{Miscellaneous}
		Finally, this section contains all the stuff that does not fit into the categories before.

		\subsection{Useful Integrals}
			The following hold for a distribution \( p_\theta(x) \):
			\begin{align}
				\int\! \pdv{\theta} p_\theta(x) \dd{x} &= 0 &
				\int\! \pdv{\theta} \log p_\theta(x) \dd{x} &= \int\! \frac{\pdv{\theta} p_\theta(x)}{p_\theta(x)} \dd{x} = 0
			\end{align}
			The first identity can be shown by swapping the integral and derivative and using the normalization condition of probability densities. For the second we use integration by parts with \( f' = \pdv{\theta} p_\theta(x) \), for which \(f = 0\) due to the first integral. Hence, the second follows.
		% end
	% end
% end

\chapter{Markov Decision Processes and Policies} % 2.1
	\todo{Content}

	\section{Markov Decision Processes} % 2.2, 2.3, 2.4, 2.7, 2.8, 2.9, 2.10, 2.14
		\todo{Content}

		\subsection{Continuous State-Action-Space} % 5.3, 5.4
			\todo{Content}
		% end

		\subsection{Example} % 2.11, 2.12, 2.13
			\todo{Content}
		% end
	% end

	\section{Markov Reward Processes} % 2.15, 2.16
		\todo{Content}

		\subsection{Return and Discount} % 2.18, 2.19, 2.20
			\todo{Content}
		% end

		\subsection{Value Function} % 2.21
			\todo{Content}

			\subsubsection{Bellman Equation} % 2.25, 2.27, 2.28
				\todo{Content}
			% end
		% end

		\subsection{Example} % 2.17, 2.22, 2.23, 2.24, 2.26
			\todo{Content}
		% end
	% end

	\section{Markov Decision Processes} % 2.29, 2.30, 2.31, 2.32
		\todo{Content}

		\subsection{Policies} % 2.33, 2.34
			\todo{Content}

			\subsubsection{Value Functions} % 2.35
				\todo{Content}

				\paragraph{Bellman Expectation Equation} % 2.37, 2.38
					\todo{Content}
				% end

				\paragraph{Bellman Operator} % 2.39, 2.40, 2.50
					\todo{Content}
				% end
			% end

			\subsubsection{Optimality} % 2.41, 2.42, 2.44
				\todo{Content}

				\paragraph{Bellman Optimality Equation} % 2.46, 2.47, 2.51
					\todo{Content}
				% end

				\paragraph{Bellman Optimality Operator} % 2.49, 2.50
					\todo{Content}
				% end
			% end
		% end

		\subsection{Example} % 2.31, 2.36, 2.43, 2.45, 2.48
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 2.52, 2.53, 2.54
		\todo{Content}
	% end
% end

\chapter{Dynamic Programming} % 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7
	\todo{Content}

	\section{Finite Horizon DP} % 3.8, 3.9
		\todo{Content}
	% end

	\section{Policy Iteration} % 3.19
		\todo{Content}

		\subsection{Policy Evaluation} % 3.10, 3.11, 3.12
			\todo{Content}
		% end

		\subsection{Policy Improvement} % 3.16, 3.17, 3.18
			\todo{Content}
		% end

		\subsection{Using the Action-Value Function} % 4.9
			\todo{Content}
		% end

		\subsection{Examples} % 3.13, 3.14, 3.15; 3.20, 3.10
			\todo{Content}
		% end
	% end

	\section{Value Iteration} % 3.24, 3.25, 3.27
		\todo{Content}

		\subsection{Principle of Optimality} % 3.22, 3.23, 3.26
			\todo{Content}
		% end

		\subsection{Convergence} % 3.30
			\todo{Content}
		% end

		\subsection{Example} % 3.29
			\todo{Content}
		% end
	% end

	\section{Policy vs. Value Iteration} % 3.28
		\todo{Content}
	% end

	\section{Efficiency} % 3.31, 3.32
		\todo{Content}
	% end
% end

\chapter{Monte-Carlo Algorithms} % 3.33, 3.34, 3.35, 3.36, 3.37
	\todo{Content}

	\section{Policy Evaluation} % 3.39, 3.40, 3.41
		\todo{Content}
	% end

	\section{Example} % 3.42, 3.43, 3.44, 3.45, 3.46, 3.47, 3.48, 3.49, 3.50, 3.51, 3.52, 3.53, 3.54, 3.55, 3.56, 3.57
		\todo{Content}
	% end
% end

\chapter{Temporal Difference Learning} % 3.58, 3.59, 3.60
	\todo{Content}

	\section{Temporal Differences vs. Monte-Carlo} % 3.63
		\todo{Content}

		\subsection{Bias-Variance Trade-Off} % 3.66, 3.67
			\todo{Content}
		% end

		\subsection{Markov Property} % 3.68
			\todo{Content}
		% end

		\subsection{Backup} % 3.69, 3.70, 3.71
			\todo{Content}
		% end
	% end

	\section{Bootstrapping and Sampling} % 3.72
		\todo{Content}
	% end

	\section{\( \text{TD}(\lambda) \)} % 3.73
		\todo{Content}

		\subsection{\(n\)-Step Return} % 3.74, 3.75, 3.76
			\todo{Content}
		% end

		\subsection{\(\lambda\)-Return} % 3.77, 3.78, 3.79
			\todo{Content}
		% end

		\subsection{Eligibility Traces} % 3.80, 3.81, 3.82, 3.83, 3.84, 3.85, 3.86
			\todo{Content}
		% end
	% end

	\section{Example} % 3.61, 3.62
		\todo{Content}
	% end

	\section{Wrap-Up} % 3.87, 3.88, 3.89
		\todo{Content}
	% end
% end

\chapter{Tabular Reinforcement Learning} % 4.1, 4.2, 4.3, 4.4
	\todo{Content}

		\subsection{Monte-Carlo Methods} % 4.5
			\todo{Content}

			\subsubsection{Generalized Policy Iteration} % 4.6, 4.7, 4.8, 4.10
				\todo{Content}
			% end

			\subsubsection{Greediness and Exploration vs. Exploitation} % 4.11, 4.12, 4.13, 4.14
				\todo{Content}

				\paragraph{\(\epsilon\)-Greedy Exploration and Policy Improvement} % 4.15, 4.16
					\todo{Content}
				% end

				\paragraph{Monte-Carlo Policy Iteration and Control} % 4.17, 4.18
					\todo{Content}
				% end
			% end

			\subsubsection{GLIE Monte-Carlo Control} % 4.19, 4.20
				\todo{Content}
			% end
		% end

		\subsection{TD-Learning: SARSA} % 4.22, 4.23, 4.24, 4.25, 4.26
			\todo{Content}

			\subsubsection{Convergence} % 4.27
				\todo{Content}
			% end

			\subsubsection{\(n\)-Step} % 4.30
				\todo{Content}
			% end

			\subsubsection{Eligibility Traces and \( \text{SARSA}(\lambda) \)} % 4.31, 4.32
				\todo{Content}
			% end

			\subsubsection{Example} % 4.28, 4.29, 4.33
				\todo{Content}
			% end
		% end
	% end

	\section{Off-Policy Methods} % 4.34, 4.35
		\todo{Content}

		\subsection{Monte-Carlo} % 3.37, 3.38
			\todo{Content}
		% end

		\subsection{TD-Learning} % N/A
			\todo{Content}

			\subsubsection{Importance Sampling} % 3.39
				\todo{Content}
			% end

			\subsubsection{Q-Learning} % 3.40, 3.41, 3.43
				\todo{Content}

				\paragraph{Convergence} % 3.42
					\todo{Content}
				% end

				\paragraph{Example} % 3.44
					\todo{Content}
				% end
			% end
		% end
	% end

	\section{Remarks} % 3.45, 3.46
		\todo{Content}
	% end

	\section{Wrap-Up} % 3.47, 3.48
		\todo{Content}
	% end
% end

\chapter{Function Approximation} % 5.1
	\todo{Content}

	\section{On-Policy Methods} % 5.5, 5.6, 5.7, 5.13
		\todo{Content}

		\subsection{Stochastic Gradient Descent} % 5.8
			\todo{Content}
		% end

		\subsection{Gradient Monte-Carlo} % 5.9, 5.10
			\todo{Content}

			\subsubsection{\dots with Linear Function Approximation} % 5.14
				\todo{Content}
			% end
		% end

		\subsection{Semi-Gradient Methods} % 5.11, 5.12
			\todo{Content}

			\subsubsection{\dots with Linear Function Approximation} % 5.15
				\todo{Content}
			% end
		% end

		\subsection{Least-Squares TD} % 5.16
			\todo{Content}

			\subsubsection{Semi-Gradient SARSA} % 5.17, 5.18
				\todo{Content}
			% end
		% end
	% end

	\section{Off-Policy Methods} % 5.31
		\todo{Content}

		\subsection{Semi-Gradient TD} % 5.32
			\todo{Content}
		% end

		\subsection{Divergence} % 5.33, 5.34+, 5.35, 5.36
			\todo{Content}
		% end
	% end

	\section{The Deadly Triad} % 5.37
		\todo{Content}
	% end

	\section{Offline Methods} % N/A
		\todo{Content}

		\subsection{Batch Reinforcement Learning} % 5.38
			\todo{Content}
		% end

		\subsection{Least-Squares Policy Iteration} % 5.39
			\todo{Content}
		% end

		\subsection{Fitted Q-Iteration} % 5.40
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 5.41, 5.42, 5.43
		\todo{Content}
	% end
% end

\chapter{Policy Search} % 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8
	\todo{Content}

	\section{Policy Gradient} % 6.9, 6.10, 6.11
		\todo{Content}

		\subsection{Computing the Gradient} % N/A
			\todo{Content}

			\subsubsection{Finite Differences} % 6.12
				\todo{Content}
			% end

			\subsubsection{Least-Squares-Based Finite Differences} % 6.13, 6.14
				\todo{Content}
			% end

			\subsubsection{Likelihood-Ratio Trick} % 6.15, 6.16, 6.17
				\todo{Content}
			% end
		% end

		\subsection{REINFORCE} % 6.18, 6.19
			\todo{Content}

			\subsubsection{Gradient Variance and Baselines} % 6.20, 6.21, 6.22+, 6.23
				\todo{Content}
			% end

			\subsubsection{Example} % 6.24, 6.25, 6.26, 6.27, 6.28
				\todo{Content}
			% end
		% end

		\subsection{GPOMDP} % 6.29, 6.30, 6.31
			\todo{Content}
		% end
	% end

	\section{Natural Policy Gradient} % 6.32, 6.35, 6.36; 6.57, 6.58+, 6.59
		\todo{Content}
	% end

	\section{The Policy Gradient Theorem} % 6.37, 6.38, 6.39, 6.40; 6.60, 6.61+
		\todo{Content}

		\subsection{Actor-Critic} % 6.41
			\todo{Content}
		% end

		\subsection{Compatible Function Approximation} % 6.42, 6.43, 6.44+
			\todo{Content}

			\subsubsection{Example} % 6.45
				\todo{Content}
			% end
		% end

		\subsection{Advantage Function} % 6.46
			\todo{Content}
		% end

		\subsection{Episodic Actor-Critic} % 6.47, 6.48, 6.49, 6.50
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 6.51, 6.52, 6.53
		\todo{Content}
	% end
% end

\chapter{Deep Reinforcement Learning} % 7.1, 7.2, 7.3, 7.4, 7.5
	\todo{Content}

	\section{Deep Q-Learning: DQN} % 7.6, 7.7, 7.8, 7.13
		\todo{Content}

		\subsection{Replay Buffer} % 7.9
			\todo{Content}
		% end

		\subsection{Target Network} % 7.10
			\todo{Content}
		% end

		\subsection{Minibatch Updates} % 7.11
			\todo{Content}
		% end

		\subsection{Reward- and Target-Clipping} % 7.12
			\todo{Content}
		% end

		\subsection{Examples} % 7.14, 7.15, 7.16, 7.17, 7.18
			\todo{Content}
		% end
	% end

	\section{DQN Enhancements} % 7.19
		\todo{Content}

		\subsection{Overestimation and Double Deep Q-Learning} % 7.20, 7.21, 7.22, 7.23
			\todo{Content}

		\subsection{Prioritized Replay Buffer} % 7.24, 7.25, 7.26
			\todo{Content}
		% end

		\subsection{Dueling DQN} % 7.27
			\todo{Content}
		% end

		\subsection{Noisy DQN} % 7.28, 7.29
			\todo{Content}
		% end

		\subsection{Distributional DQN} % 7.30, 7.31, 7.32, 7.33, 7.34, 7.35
			\todo{Content}
		% end

		\subsection{Rainbow} % 7.36, 7.37
			\todo{Content}
		% end
	% end

	\section{Other DQN-Bases Methods} % 7.38
		\todo{Content}

		\subsection{Count-Based Exploration} % 7.39, 7.40, 7.41, 7.42
			\todo{Content}
		% end

		\subsection{Curiosity-Driven Exploration} % 7.43, 7.44
			\todo{Content}
		% end

		\subsection{Ensemble-Driven Exploration} % 7.45, 7.46, 7.47
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 7.48, 7.49, 7.50
		\todo{Content}
	% end
% end

\chapter{Deep Actor-Critic} % 8.1, 8.6, 8.7, 8.8, 8.9; 8.15
	\todo{Content}

	\section{Surrogate Loss} % 8.10, 8.11, 8.12
		\todo{Content}

		\subsection{Kakade-Langford-Lemma} % 8.13+
			\todo{Content}
		% end

		\subsection{Practical Surrogate Loss} % 8.14
			\todo{Content}
		% end
	% end

	\section{Advantage Actor-Critic (A2C)} % 8.16, 8.17, 8.18
		\todo{Content}
	% end

	\section{On-Policy Methods} % 8.19
		\todo{Content}

		\subsection{Trust-Region Policy Optimization (TRPO)} % 8.20, 8.21, 8.28
			\todo{Content}

			\subsubsection{Practical Implementation} % 8.22, 8.23
				\todo{Content}
			% end

			\subsubsection{Conjugate Gradient} % 8.24, 8.25, 8.26, 8.27
				\todo{Content}
			% end
		% end

		\subsection{Proximal Policy Optimization (PPO)} % 8.29, 8.30
			\todo{Content}
		% end
	% end

	\section{Off-Policy Methods} % 8.31
		\todo{Content}

		\subsection{Deep Deterministic Policy Gradient (DDPG)} % 8.32, 8.33, 8.34, 8.35
			\todo{Content}
		% end

		\subsection{Twin Delayed DDPG (TD3)} % 8.36, 8.37
			\todo{Content}
		% end

		\subsection{Soft Actor-Critic (SAC)} % 8.38, 8.39, 8.40, 8.43, 8.44, 8.45
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 8.46, 8.47, 8.48
		\todo{Content}
	% end
% end

\chapter{Frontiers} % 9.1
	\todo{Content}

	\section{Partial Observability} % 9.2, 9.3, 9.4, 9.5, 9.6, 9.7
		\todo{Content}
	% end

	\section{Hierarchical Control} % 9.8, 9.9, 9.10, 9.17
		\todo{Content}

		\subsection{The Options Framework} % 9.11, 9.12, 9.13, 9.14, 9.15, 9.16
			\todo{Content}
		% end
	% end

	\section{Markov Decision Processed Without Reward} % 9.18, 9.19
		\todo{Content}

		\subsection{Intrinsic Motivation} % 9.20, 9.21, 9.22
			\todo{Content}
		% end

		\subsection{Inverse Reinforcement Learning} % 9.23, 9.24, 9.25, 9.26, 9.27
			\todo{Content}
		% end
	% end

	\section{Model-Based Reinforcement Learning} % 9.28, 9.29, 9.30, 9.31, 9.32, 9.33, 9.34
		\todo{Content}
	% end

	\section{Wrap-Up} % 9.35, 9.36
		\todo{Content}
	% end
% end
