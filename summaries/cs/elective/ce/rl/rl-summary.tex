\documentclass[english, notodo]{fdsummary}

\usepackage{amsthm}
\let\definition\undefined
\let\theorem\undefined
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{plain}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{postulate}{Postulate}
\newtheorem{remark}{Remark}
\usepackage{acronym}
\usepackage{xfrac}
\usepackage{multirow}

\let\realsfrac=\sfrac
\renewcommand{\sfrac}[2]{\realsfrac{#1}{#2}}  % Just to make the command known to TeXStudio.

\newcommand{\transposed}{{\!\top\!}}
\newcommand{\MSVE}{\overline{\mathit{VE}}}

\newacro{AI}{artificial intelligence}
\newacro{RL}{reinforcement learning}
\newacro{RBF}{radial basis function}
\newacro{ML}{machine learning}
\newacro{NN}{neural network}
\newacro{MDP}{Markov decision process}  \newacroplural{MDP}{Markov decision processes}
\newacro{MRP}{Markov reward process}  \newacroplural{MRP}{Markov reward processes}
\newacro{DP}{dynamic programming}
\newacro{PI}{policy iteration}
\newacro{VI}{value iteration}
\newacro{MC}{Monte-Carlo}
\newacro{TD}{temporal difference}
\newacro{GLIE}{greedy in the limit of infinite exploration}
\newacro{SARSA}{state, action, reward, state, action}  \acused{SARSA}
\newacro{DQN}{deep Q-network}
\newacro{A2C}{advantage actor-critic}
\newacro{TRPO}{trust-region actor-critic}
\newacro{PPO}{proximal policy optimization}
\newacro{DDPG}{deep deterministic policy gradient}
\newacro{TD3}{twin delayed \acs{DDPG}}
\newacro{SAC}{soft actor-critic}
\newacro{MSVE}{mean squared value error}
\newacro{SGD}{stochastic gradient descent}
\newacro{LSTD}{least-squares \ac{TD}}
\newacro{LSPI}{least-squares \ac{PI}}
\newacro{LSTDQ}{\ac{LSTD} for action-value functions}  \acused{LSTDQ}

\begin{document}
	\maketitle
	\listoftodos
	\tableofcontents
	\listoffigures
	\listoftables
	\listofalgorithms

	\include{content}
\end{document}
