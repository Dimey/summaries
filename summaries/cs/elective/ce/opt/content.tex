% !TeX spellcheck = en_US

\chapter{EinfÃ¼hrung} % 1.5
	\todo{Content}

	\section{Beispiele} % 1.6, 1.7, 1.8, 1.10, 1.11
		\todo{Content}
	% end

	\section{Fragestellungen} % 1.12
		\todo{Content}
	% end

	\section{Allgemeine Formulierung eines Optimierungsproblems} % 1.13, 1.14
		\todo{Content}
	% end

	\section{Statische vs. Dynamische Optimierung} % 1.15, 1.16, 1.17, 1.18
		\todo{Content}
	% end

	\section{Klassifizierung von Optimierungsverfahren} % 1.23
		\todo{Content}
	% end

	\section{Typische Struktur} % 1.24
		\todo{Content}
	% end
% end

\chapter{Gradient-Based Optimization without Constraints}
	\section{Solution Characterization}
		This section covers the theoretical results for solving a nonlinear optimization problem using calculus.
	
		\subsection{One-Dimensional Optimization}
			For a one-dimensional function \( \varphi(p) : \R \to \R \) the first-order necessary condition for a minimum is that the derivative of \( \varphi(p) \) w.r.t. the parameter \(p\) vanishes:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} \! = 0
			\end{align*}
			Where \( p^\ast \) denotes the optimal solution, i.e. the minimum.
			
			All solutions that fulfill this condition are \emph{candidates} for a minimum. If \( \varphi \) is twice continuous differentiable, the sufficient condition for a minimum is that the second-order derivative is positive:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} > 0
			\end{align*}
			Then \(p^\ast\) is called a \emph{strict minimum}. This condition is sufficient, but not necessary! The second-order necessary condition for a minimum is that the second-order derivative is non-negative, i.e. \( \dv{\varphi(p^\ast)}{p} \geq 0 \).
			
			\subsubsection{Possibilities for a Minimum}
				There are three cases for a minimum:
				\begin{itemize}
					\item \(\varphi(p)\) is twice continuously differentiable everywhere
					\item \(\varphi'(p)\) is not continuous everywhere but at \(p^\ast\)
					\item \(\varphi'(p)\) is not continuous everywhere, not even at \(p^\ast\)
				\end{itemize}
				While the latter case is common, it is problematic as the solution can typically not be determined analytically (if a function is not continuous at one point, it is rarely invertible).
			% end
		% end

		\subsection{Multi-Dimensional Optimization}
			For multi-dimensional objective functions \( \varphi : \R^{n_p} \to \R \), where \(n_p\) is the dimensionality of the parameters, the first-order necessary condition is that the gradient vanishes:
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) =
					\begin{bmatrix}
						\pdv{\varphi}{p_1} \\
						\vdots \\
						\pdv{\varphi}{p_{n_p}}
					\end{bmatrix}
				=
					\begin{bmatrix}
						0 \\
						\vdots \\
						0
					\end{bmatrix}
			\end{align*}
			
			If \(\varphi(\vec{p})\) is twice continuously differentiable, the second-order sufficient condition is that the Hessian of \(\varphi(\vec{p})\) is positive definite. Analogous to the one-dimensional case, the second-order necessary condition is that the Hessian is positive semi-definite, i.e.:
			\begin{align*}
				\mat{H}_\varphi(\vec{p}^\ast) =
					\begin{bmatrix}
						\pdv[2]{\varphi}{p_1}        & \cdots & \pdv{\varphi^2}{p_{n_p} p_1} \\
						\vdots                       & \ddots & \vdots                       \\
						\pdv{\varphi^2}{p_1 p_{n_p}} & \cdots & \pdv[2]{\varphi}{p_{n_p}}
					\end{bmatrix}
				> 0
				\quad\text{or respectively}\quad
				\mat{H}_\varphi(\vec{p}^\ast) \geq 0
			\end{align*}

			\paragraph{Example} % 2.7, 2.8, 2.9
				\todo{Content}
			% end
		% end
	% end

	\section{Numerical Gradient-Based Methods}
		\subsection{Starting Point}
			\subsubsection{Structure of Gradient-Based Methods}
				Given a initial approximation \( \vec{p}^{(0)} \), an approximation of the minimum \( \vec{p}^\ast \) is wanted. Gradient-based methods are iteration methods based on the iteration rule
				\begin{align*}
					\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)},\quad k = 0, 1, 2, \cdots
				\end{align*}
				where
				\begin{itemize}
					\item \(\vec{d}^{(k)}\) is the search direction found as the solution of a linear sub problem and
					\item \(\alpha^{(k)}\) is the step size found by a one-dimensional \emph{line search}.
				\end{itemize}
				The iteration terminates once \( \vec{p}^{(k + 1)} \) is "close to" \(\vec{p}^\ast\), e.g. when the gradient nearly vanishes.
			% end

			\subsubsection{Descent Direction}
				Gradient-based methods have to ensure the the local search direction \(\vec{d}^{(k)}\) really is a descent direction (the algorithm shall not "run up the hill"). This property is ensured iff the angle \( \delta \) between the search direction and the gradient \( \grad{\varphi}\big(\vec{}^{(k)}\big) \) greater than \SI{90}{\degree}, i.e.
				\begin{align}
					\cos \delta = \frac{\big( \vec{d}^{(k)} \big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)}{\big\lVert \vec{d}^{(k)} \big\rVert \cdot \big\lVert \grad{\varphi}\big(\vec{p}^{(k)}\big) \big\rVert} < 0 \quad\iff\quad \big( \vec{d}^{(k)} \big)^T \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big) < 0  \label{eq:descentDirection}
				\end{align}
				This is called the "necessary descent condition".
			% end

			\subsubsection{Algorithmic Structure}
				The \autoref{alg:gradientBasedAlgorithmStructure} shows the basic structure of any gradient-based optimization algorithm.
			
				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
					\While{not converged}{
						Determine new search direction: \tabto{6cm} \( \vec{d}^{(k)} \in \R^{n_p} \) \;
						Determine new step size: \tabto{6cm} \( \alpha^{(k)} \in \R^+ \) \;
						Update the approximation: \tabto{6cm} \( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( k \gets k + 1 \) \;
					}
				
					\caption{Algorithmic structure of a gradient-based optimization algorithms.}
					\label{alg:gradientBasedAlgorithmStructure}
				\end{algorithm}
			% end
		% end

		\subsection{Steepest Descent}
			Steepest descent is the straightforward way for getting a search direction. The search direction is just set to the negative of the gradient:
			\begin{align*}
				\vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Often quickly reaches areas around the local minimum.
						\item No second derivatives needed.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Very slow in areas around the local minimum compared to (Quasi-) Newton Methods.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Conjugate Gradient}
			Basic approach for conjugate gradient:
			\begin{align*}
				\vec{d}^{(0)} &= -\grad{\varphi}\big(\vec{p}^{(0)}\big) \\
				\vec{d}^{(k)} &= \text{Component of } -\grad{\varphi}\big(\vec{p}^{(k)}\big) \text{ that is conjugate to } \vec{d}^{(0)}, \vec{d}^{(1)}, \cdots, \vec{d}^{(k - 1)}
			\end{align*}

			For a quadratic objective function
			\begin{align*}
				\varphi(\vec{p}) = \frac{1}{2} \vec{p}^T \mat{A} \vec{p} - \vec{b}^T \vec{p}
			\end{align*}
			with a positive semi-definite matrix \(\mat{A}\) and constant \(\mat{A}\), \(\vec{b}\), the search direction is given as the solution of:
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(j)} = 0,\quad j = 1, \cdots, k - 1
			\end{align*}
			
			With an optimal step size \( \alpha^{(k)} \), i.e.
			\begin{align*}
				\alpha^{(k)} = \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big)
					\quad\implies\quad \alpha^{(k)} = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(k)}} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \big(\vec{d}^{(k)}\big)
			\end{align*}
			the minimum of \(\varphi\) is reached in \(n_p\) steps.
			
			The extension for nonlinear objective functions is given in~\autoref{alg:conjugateGradient}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \vec{d}^{(0)} \gets -\grad{\varphi}\big(\vec{p}^{(0)}\big) \) and \( k \gets 0 \) \;
				\While{not converged}{
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( \beta^{(k + 1)} \gets \frac{\big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)}{\big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)} \) \;
					\( \vec{d}^{(k + 1)} \gets -\grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \beta^{(k + 1)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}
				
				\caption{Conjugate Gradient for nonlinear Objective Function.}
				\label{alg:conjugateGradient}
			\end{algorithm}
		
			\begin{itemize}
				\item Exact line search necessary.
				\item Different variants of GC-algorithms mainly distinguish in the choice of of \( \beta^{(k)} \).
				\item Advantages:
					\begin{itemize}
						\item Faster then steepest descent.
						\item No explicit storing of the Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \) necessary.
						\item No explicit matrix-vector multiplication.
						\item Useful even for extreme high dimensions \( n_p \).
						\item Exact for quadratic objectives \( \varphi(\vec{p}) \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item A lot slower then (Quasi-) Newton Methods.
						\item In general not useful for optimizing simulation models.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Newton Method}
			Assuming the approximation of each iteration, \( \vec{p}^{(k)} \), is close to the minimum \( \vec{p}^\ast \), the gradient \( \grad{\varphi}(\vec{p}^\ast) \) can be taylor-expanded around \( \vec{p}^{(k)} \):
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) \overset{T\big(\!\vec{p}^{(k)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \big( \vec{p}^\ast - \vec{p}^{(k)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			By leaving our the higher order terms the search direction \( \vec{d}^{(k)} \coloneqq \vec{p}^\ast - \vec{p}^{(k)} \) is given by the solution of the system of linear equations
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The realization is shown in~\autoref{alg:newtonMethod}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
				\While{not converged}{
					Solve \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}
				
				\caption{Newton Method}
				\label{alg:newtonMethod}
			\end{algorithm}
		
			When plugging the search direction into the necessary descent condition~\eqref{eq:descentDirection}
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
					= -\Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)^T\Big) \Big(\!\mat{H}_\varphi\big(\vec{p}^{(k)}\big)\!\Big)^{-1} \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
					< 0
			\end{align*}
			it is clear that this is only fulfilled iff the Hessian is positive definite. But this is only the case in a region around the minimum! If the approximation is far away from the minimum, the search direction might also be an ascent direction causing the Newton method to diverge. There are two main solutions to this problem:
			\begin{enumerate}
				\item If the Hessian is not positive definite, replace it by an identity matrix. That is, set the search direction to the steepest descent.
				\item Regularize the equation system with a weight \( \nu > 0 \) such that the new matrix is positive definite (this "rotates" the matrix in the direction of the steepest descent such that the new search direction always fulfills the descent condition):
			\end{enumerate}
			\begin{align*}
				\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \nu \mat{I} \Big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Near to strong local minima of twice continuous differentiable objective, the Newton method is quadratic convergent.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Computationally expensive as a linear system has to be solved in every iteration.
						\item Not only first, but also second-order derivatives have to be available. This is a big disadvantage:
							\begin{itemize}
								\item In practice, the first derivative is rarely and the second derivative is never available.
								\item Even a single wrong component in the gradient or the Hessian destroys the quadratic convergence.
							\end{itemize}
					\end{itemize}
			\end{itemize}

			\subsubsection{Availability of Second-Order Derivatives}
				The obvious idea is to approximate the Hessian using finite differences. The approximated Hessian is then given as
				\begin{align*}
					\mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T )  \label{eq:approxHessianSymm}
				\end{align*}
				where \(\tilde{\mat{H}}\) is given by
				\begin{align*}
					\tilde{\mat{H}}_i = \pdv{p_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big) \approx \frac{1}{h_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)} + h_i \vec{e}_i\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big)
				\end{align*}
				where \( \tilde{\mat{H}}_i \) is the \(i\)-th column of \(\tilde{\mat{H}}\). The equation~\eqref{eq:approxHessianSymm} is used to force the Hessian to be symmetric.
				
				Problems:
				\begin{itemize}
					\item The Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T ) \) is not necessarily positive definite.
					\item In every iteration the Gradient has to be evaluated \(n_p\) times more.
					\item The linear system still needs to be solved.
					\item Only useful for high-dimensional problems with sparse gradients!
				\end{itemize}
				Another possibility are \emph{Quasi-Newton Methods}.
			% end
		% end

		\subsection{Quasi-Newton Methods}
			Quasi-Newton methods are equivalent to the Newton method, however, the Hessian (or its inverse) is approximated by a positive definite matrix
			\begin{align*}
				\hat{\mat{H}}^{(k)} \approx \mat{H}_\varphi\big(\vec{p}^{(k)}\big)
			\end{align*}
			that is updated in every iteration. This yields a lot of advantages over the classic Newton method:
			\begin{itemize}
				\item Only first-order derivatives needed.
				\item As \( \hat{\mat{H}} \) constructed positive definite, the descent condition is fulfilled anytime.
				\item If the inverse Hessian is directly approximated, only \( \mathcal{O}(n_p^2) \) multiplications instead of \( \mathcal{O}(n_p^3) \) for solving the linear system.
			\end{itemize}
		
			But how to do the Hessian update? By Taylor-expanding the gradient \( \grad{\varphi}\big(\vec{p}^{(k)}\big) \) around \( \vec{p}^{(k + 1)} \)
			\begin{align*}
				\grad{\varphi}\big(\vec{p}^{(k)}\big) \overset{T\big(\!\vec{p}^{(k + 1)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \big( \vec{p}^{(k)} - \vec{p}^{(k + 1)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			and cutting off the higher-order terms, the following approximation holds:
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \vec{d}^{(k)} \approx \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The approximation of the Hessian must therefore fulfill the \emph{secant condition}
			\begin{align*}
				\tilde{\mat{H}}^{(k + 1)} \vec{d}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			There exist a lot of different approaches for doing the Hessian updates \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \mat{U}{(k)} \) for rank-1 or rank-2 matrices \(\mat{U}^{(k)}\):
			\begin{itemize}
				\item Approach for rank-1 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T \)
				\item Approach for rank-2 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T + \beta_2 \vec{v} \vec{v}^T \)
			\end{itemize}
			The vectors \( \vec{u}, \vec{v} \in \R^{n_p} \) and scalars \( \beta_1, \beta_2 \in \R \) must have to be chosen such that \( \tilde{\mat{H}}^{(k + 1)} \) is
			\begin{itemize}
				\item positive definite,
				\item symmetric,
				\item fulfills the secant condition and
				\item adding up the matrices is efficient and robust.
			\end{itemize}

			\subsubsection{BFGS-Update}
				The most known rank-2 update for the Hessian is the \emph{BFGS-Update}\footnote{"BFGS" stands for the authors Broyden, Fletcher, Goldfarb and Shanno.}
				\begin{align*}
					\vec{u} & = \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} &   \beta_1 & = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \\
					\vec{v} & = \vec{g}^{(k)}                       &   \beta_2 & = \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}}
				\end{align*}
				where \( \vec{g}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \). Plugging that into the general approach for rank-2 updates yields the update rule for BFGS-approximations:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)}
							- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
							+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T
				\end{align*}
				\begin{itemize}
					\item The direct approximation of the Hessian inverse is not really robust (e.g. for a non-optimal step size rule).
					\item A better alternative is to directly approximate a useful factorization, e.g. the Cholesky decomposition. This is more robust and equally efficient (\( \mathcal{O}(n_p^2) \)).
				\end{itemize}
			
				The pseudo code for the BFGS update is shown in~\autoref{alg:bfgs}.
				
				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \tilde{\mat{H}}^{(0)} = \mat{I} \) and \( k \gets 0 \) \;
					\While{not converged}{
						Solve \( \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
						\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
						\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( \tilde{\mat{H}}^{(k + 1)} \gets \tilde{\mat{H}}^{(k)} - \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T + \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T \) \;
						\( k \gets k + 1 \) \;
					}
					
					\caption{Quasi-Newton Method with BFGS Update.}
					\label{alg:bfgs}
				\end{algorithm}
			% end
		% end

		\subsection{Comparison} % 2.29, 2.30, 2.31, 2.32, 2.33, 2.34, 2.35, 2.36
			\todo{Content}
			
			\begin{itemize}
				\item 
			\end{itemize}
		% end
		
		\subsection{Notes and Discussion}
			\begin{itemize}
				\item The convergence of gradient-based methods can be shown under weak preconditions.
				\item As the search direction is only a local descent direction, gradient-based algorithms only yields local minima.
				\item There is no algorithm that can guarantee to find the global minimum!
				\item Some approaches for determining a global minimum:
					\begin{itemize}
						\item Choose the initialization well, i.e. close to the global minimum.
						\item Execute the algorithm multiple times with different starting points.
						\item Validate the solution against properties of the original problem.
						\item Execute direct search methods beforehand to find promising regions for the local minimum search.
					\end{itemize}
			\end{itemize}
		
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item If gradient-based algorithms converge, they converge utterly fast.
						\item Efficient also for high-dimensional problems, i.e. a large \( n_p \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Only applicable for functions that are differentiable almost everywhere.
						\item Require gradient information exact up to four to eight decimal points.
						\item Convergence to a local minimum near the initialization \( \vec{p}^{(0)} \).
						\item Require some expert knowledge.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Step Size Rules, Line Search}
		In every iteration of gradient-based algorithms, the step size has to be determined by minimizing the one-dimensional function:
		\begin{align*}
			\psi(\alpha) = \varphi\big(\vec{p}^{(k)} + \alpha \vec{d}^{(k)}\big)
		\end{align*}
		As the necessary first-order condition for a minimum, the derivative w.r.t. \(\alpha\) has to vanish:
		\begin{align*}
			\dv{\psi\big(\alpha^{(k)}\big)}{\alpha} = \dv{\alpha} \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) = \Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \vec{d}^{(k)} \overset{!}{=} 0
		\end{align*}
		Thus the gradient of \(\varphi\) at the minimum \(\alpha^{(k)}\) as to be orthogonal to the search direction \(\vec{d}^{(k)}\). Intuitively, the optimal step size has to be chosen such that the iteration step cannot go any further without ascending again ("hitting an ascending contour line").
		
		Goal of the line search is to reach the minimum of \(\psi\) with least invocations of \(\psi\) as possible. Most of the existing search methods can be classified into
		\begin{itemize}
			\item \emph{Polynomial approximation}, e.g. quadratic or cubic interpolation
			\item \emph{Direct search methods}, e.g. Fibonacci-search, golden ratio search
			\item \emph{Optimal vs. non-optimal search methods}, e.g. by finding an improvement but not the minimum
			\item Usage of the gradient information \( \psi' \) or not.
		\end{itemize}
	
		Requirements:
		\begin{itemize}
			\item Finding the \(\alpha^{(k)}\) with a minimal value of \(\psi\).
			\item Do not waste too much computation time on the line search.
		\end{itemize}
		In general, an exact line search requires lots of \(\psi\)-evaluations. But how far does \(\psi\) need to be reduced in order to guarantee convergence? In general, the condition \( \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) < \varphi\big(\vec{p}^{(k)}\big) \) is not enough!
		
		\subsection{Inexact Line Search}
			Procedure: Generate and inspect a series of candidates for \(\alpha^{(k)}\) and terminate once one of the candidates fulfills specific criteria, e.g. the Armijo rule or Wolfe conditions.
			
			\subsubsection{Armijo Rule}
				The \emph{Armijo rule} guarantees a sufficient reduction in \(\varphi\):
				\begin{align*}
					\varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) \leq \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \psi'(0)
				\end{align*}
				Where \( 0 < c_1 < 1 \) is any constant, e.g. \( c_1 = 10^{-4} \).
				
				Hence, the minimal reduction has to be proportional to \(\alpha^{(k)}\) and the derivative \( \psi'(0) \).
			% end
			
			\subsubsection{Curvature Condition}
				But a sufficient descent condition is not enough as the step sizes must not be too small (otherwise progress would stop). Thus a second condition has to be employed, the \emph{curvature condition} that requires a minimum curvature on \(\psi\):
				\begin{align*}
					\Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \geq c_2 \cdot \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = c_2 \psi'(0) \quad\iff\quad \psi'\big(\vec{p}^{(k)}\big) \geq c_2 \psi'(0)
				\end{align*}
				Where \( c_1 < c_2 < 1 \) is any constant, e.g. \( c_2 = 0.9 \).
			% end
			
			\subsubsection{Wolfe and Goldstein Conditions}
				Combining the Armijo rule and the curvature condition yields the Wolfe conditions that guarantee both a minimal reduction and a minimal curvature. They are especially useful for Quasi-Newton methods as the Wolfe conditions are scale invariant, i.e. independent of
				\begin{itemize}
					\item multiplying \(\varphi\) with any constant and
					\item affine transformations of \(\vec{p}\).
				\end{itemize}
			
				There are other possibilities are, e.g. the \emph{Goldstein conditions}
				\begin{align*}
					\varphi\big(\vec{p}^{(k)}\big) + (1 - c) \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
						\leq \varphi\big( \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \big)
						\leq \varphi\big( \vec{p}^{(k)} \big) + c \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
				\end{align*}
				with any \( 0 < c < 1/2 \), that are useful for Newton, but not for Quasi-Newton methods.
			% end
		% end
		
		\subsection{Notes}
			\begin{itemize}
				\item For gradient-based methods a step size \( \alpha^{(k)} > 1 \) is in general not useful because:
					\begin{itemize}
						\item The search direction \( \vec{d}^{(k)} \) is determined using a linear or quadratic Taylor approximation.
						\item The Taylor approximation is only valid in a small region around the current approximation \( \vec{p}^{(k)} \), i.e. for \( 0 < \alpha^{(k)} \leq 1 \).
					\end{itemize}
				\item The local quadratic or super-linear convergence of Newton-type methods is visible in practice as the last step can be executed with full step size \( \alpha^{(k)} = 1 \).
			\end{itemize}
		% end
	% end

	\section{Trust Region Methods}
		Gradient-based methods with line search determine a fixed search direction and adjust the step size \( \alpha^{(k)} \) according to that search direction to reach global convergence.
		
		Another approach is to determine both the length and direction of \( \vec{d}^{(k)} \). The iteration step then becomes
		\begin{align*}
			\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \vec{d}^{(k)}
		\end{align*}
		without any explicit step size. The length and direction of \( \vec{d}^{(k)} \) are then determined as a solution of the quadratic sub-problem
		\begin{align*}
			\min_{\vec{d} \in \R^{n_p}} &\, \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d} + \frac{1}{2} \vec{d}^T \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d} \\
			\mathrm{s.t.} &\,
				\begin{alignedat}{2}
					\lVert \vec{d} \rVert_2 & \leq \delta
				\end{alignedat}
		\end{align*}
		where \(\delta\) describes the area around the current approximation \( \vec{p}^{(k)} \) where the quadratic approximation of \( \varphi\big(\vec{p^{(k)}} + \vec{d}\big) \) makes sense, i.e. the \emph{trust region}.
		
		The value of \(\delta\) is extremely important for the efficiency of the method.
		\begin{itemize}
			\item If \(\delta\) is too small, opportunities for large steps are missed.
			\item If \(\delta\) is too large, the minimum of the quadratic approximation might be far off the minimum of the objective if the Hessian is indefinite or negative definite.
		\end{itemize}
		It is possible to add a regularization parameter \(\beta \geq 0\) to the quadratic approximation
		\begin{align*}
			\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \beta \mat{I} \Big) \vec{d} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
		\end{align*}
		such that the matrix is positive semidefinite. The solution of this regularized problem also solves the trust region problem if either \( \beta = 0 \), \( \lVert \vec{d} \rVert \leq \delta \) or \( \beta \geq 0 \), \( \lVert \vec{d} \rVert = \delta \).
	% end

	\section{Rate of Convergence}
		A common criteria to measure the performance of a gradient method are \emph{rates of convergence}. These provide information on how fast an algorithm converges, i.e. how fast \( \vec{p}^{(k)} \to \vec{p}^\ast \) or \( \big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert \to 0 \).
		
		\textbf{Definition:}
		Let \( \big(\{\, \vec{p}^{(k)} \,\big\} \) be the series of approximations produced by an optimization method. Then this series has rate of convergence \(r\) if \(r\) is the greatest positive number such that the limit
		\begin{align*}
			0 \leq \liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert^r} = \gamma < \infty
		\end{align*}
		converges (where \( \vec{p}^\ast = \liminfty[k] \vec{p}^{(k)} \)). If \( r = 1 \), then \( \gamma < 1 \) has to hold for the method to converge.
		
		A sequence is said to converge superlinearly if
		\begin{align*}
			\liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert} = 0
		\end{align*}
		holds. Even though technically this condition holds for \( r > 1 \), in practice only methods with \( 1 < r < 2 \) are said to converge superlinearly (e.g. for \(r = 2\), the sequence is called to converge quadratically).

		\paragraph{Examples} % 2.53
			\todo{Content}
		% end
		
		\subsection{Gradient-Based Methods}
			Under ideal conditions (i.e. \(\varphi\) is twice continuously differentiable and \( \mat{H}_\varphi(\vec{p}^\ast) \) is positive definite), all of the following hold:
			\begin{itemize}
				\item Steepest descent is (locally) linearly convergent (with exact line search).
				\item The Newton method is (locally) quadratically convergent.
				\item Quasi-Newton methods with BFGS-update are (locally) superlinearly convergent (for inexact line search using the Wolfe conditions).
			\end{itemize}
			But: Even a single wrong component in the Hessian reduced the quadratic convergence of the Newton method to linear convergence!
		% end
	% end
% end

\chapter{Gradientenfreie Optimierung ohne BeschrÃ¤nkungen} % 3.1, 3.2
	\todo{Content}

	\section{Einleitung} % N/A
		\todo{Content}

		\subsection{Historische Entwicklung} % 3.3
			\todo{Content}
		% end

		\subsection{Problemstellung} % 3.4
			\todo{Content}
		% end

		\subsection{Simulationsbasierte Optimierung} % 3.5, 3.6, 3.7, 3.8
			\todo{Content}
		% end

		\subsection{Black-Box Optimierung} % 3.9
			\todo{Content}
		% end
	% end

	\section{Metaheuristiken} % 3.10
		\todo{Content}

		\subsection{EvolutionÃ¤re Algorithmen (EA)} % 3.11
			\todo{Content}
		% end

		\subsection{Genetische Algorithmen (GA)} % 3.12
			\todo{Content}

			\paragraph{Beispiel} % 3.13, 3.14
				\todo{Content}
			% end
		% end

		\subsection{Weitere Metaheursitiken} % 3.15
			\todo{Content}
		% end
	% end

	\section{Deterministische Sampling Verfahren (Mustersuchverfahren)} % 3.16, 3.17
		\todo{Content}

		\subsection{Nelder-Mead Simplexverfahren} % 3.18, 3.26, 3.27
			\todo{Content}

			\subsubsection{Iterationsphase} % 3.19, 3.20
				\todo{Content}
			% end

			\subsubsection{Algorithmus} % 3.21, 3.22, 3.23
				\todo{Content}
			% end
		% end

		\subsection{Multidirektionales Suchverfahren (MDS)} % 3.28
			\todo{Content}
		% end

		\subsection{Asynchronous Parallel Pattern Search (APPS)} % 3.29, 3.30, 3.31
			\todo{Content}

			\paragraph{Beispiel} % 3.32, 3.33, 3.34
				\todo{Content}
			% end
		% end

		\subsection{Implizites Filtern} % 3.35, 3.36, 3.37, 3.38
			\todo{Content}
		% end
	% end

	\section{Optimierung mit Ersatzfunktionen} % 3.39, 3.40, 3.41
		\todo{Content}

		\subsection{Approximationsmethoden} % 3.42
			\todo{Content}

			\subsubsection{Response Surface Methoden (RSM)} % 3.43
				\todo{Content}
			% end

			\subsubsection{Radial Basis FUnctions (RBF)} % 3.44
				\todo{Content}
			% end

			\subsubsection{Design and Analysis of Computer Experiments (DACE)} % 3.45
				\todo{Content}
			% end
		% end

		\subsection{Auswahl der StÃ¼tzstellen/Datenbasis/Anfangswerte} % 3.46
			\todo{Content}

			\subsubsection{Design of Experiments (DoE)} % 3.47
				\todo{Content}
			% end
		% end

		\subsection{Minimierung der Ersatzfunktion} % 3.48
			\todo{Content}

			\subsubsection{Optimierung am Ersatzproblem} % 3.49
				\todo{Content}

				\paragraph{Strawman} % 3.48
					\todo{Content}
				% end

				\paragraph{Shoemaker} % 3.48
					\todo{Content}
				% end

				\paragraph{DACE-basierte, sequentielle Update-Strategien} % 3.50
					\todo{Content}
				% end
			% end
		% end

		\subsection{Diskussion} % 3.54
			\todo{Content}
		% end
	% end

	\section{Vergleich der Verfahren} % 3.55
		\todo{Content}

		\subsection{Design eines Magnetlagers} % 3.56, 3.57, 3.58, 3.59, 3.60, 3.61
			\todo{Content}
		% end

		\subsection{Laufoptimierung eines Humanoidroboters} % 3.62, 3.63
			\todo{Content}
		% end
	% end

	\section{Diskussion} % 3.65, 3.66
		\todo{Content}
	% end
% end

\chapter{Gradientenbasierte Optimierung mit BeschrÃ¤nkungen} % 4.1, 4.2, 4.3, 4.4, 4.5
	\todo{Content}

	\section{Charakterisierung der LÃ¶sung} % 4.6
		\todo{Content}

		\subsection{Motivation} % 4.7, 4.8
			\todo{Content}
		% end

		\subsection{Lagrange-Funktion} % 4.9
			\todo{Content}
		% end

		\subsection{Notwendige OptimalitÃ¤tsbedingungen 1. Ordnung (Karush-Kuhn-Tucker, KKT)} % 4.10
			\todo{Content}
		% end

		\subsection{Notwendige OptimalitÃ¤tsbedingungen 2. Ordnung} % 4.12
			\todo{Content}
		% end

		\subsection{Beispiel} % 4.11, 4.13
			\todo{Content}
		% end
	% end

	\section{Einfache Schranken} % 4.14, 4.15, 4.16, 4.17
		\todo{Content}
	% end

	\section{Straffunktionsverfahren} % 4.18, 4.19
		\todo{Content}

		\subsection{ÃuÃere Straffunktionsverfahren} % 4.20, 4.21
			\todo{Content}

			\paragraph{Beispiel} % 4.22, 4.23
				\todo{Content}
			% end
		% end

		\subsection{Innere Straffunktionsverfahren} % 4.24, 4.25
			\todo{Content}

			\paragraph{Beispiel} % 4.26
				\todo{Content}
			% end
		% end

		\subsection{Exakte Straffunktionen} % 4.27
			\todo{Content}

			\paragraph{Beispiel 1} % 4.28
				\todo{Content}
			% end

			\paragraph{Beispiel 2} % 4.29
				\todo{Content}
			% end
		% end

		\subsection{Erweiterte Lagrange-Funktion} % 4.30
			\todo{Content}

			\paragraph{Beispiel 1} % 4.31, 4.32
				\todo{Content}
			% end

			\paragraph{Beispiel 2} % 4.33
				\todo{Content}
			% end

			\subsubsection{Eigenschaften} % 4.34, 4.35
				\todo{Content}
			% end
		% end
	% end

	\section{Elimination von BeschrÃ¤nkungen} % 4.36, 4.37, 4.41
		\todo{Content}

		\paragraph{Beispiel 1} % 4.38
			\todo{Content}
		% end

		\paragraph{Beispiel 2} % 4.39
			\todo{Content}
		% end

		\paragraph{Beispiel 3} % 4.40
			\todo{Content}
		% end
	% end

	\section{Verfahren der Sequentiellen Quadratischen Optimierung (SQP)} % 4.42
		\todo{Content}

		\subsection{Einleitung} % 4.43, 4.44
			\todo{Content}
		% end

		\subsection{Bestimmung der Suchrichtung} % 4.45, 4.46, 4.47
			\todo{Content}

			\subsubsection{Quadratisches Problem (QP)} % 4.48
				\todo{Content}
			% end
		% end

		\subsection{Bestimmung der Schrittweite} % 4.50
			\todo{Content}
		% end

		\subsection{Approximation der Lagrange-Multiplikatoren} % 4.52
			\todo{Content}
		% end

		\subsection{Terminierungskriterien} % 4.54, 4.55
			\todo{Content}
		% end

		\subsection{Approximation der Hesse-Matrix} % 4.58, 4.59
			\todo{Content}

			\subsubsection{Naiver Ansatz} % 4.60
				\todo{Content}
			% end

			\subsubsection{Reduzierte Hesse-Matrix} % 4.61, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68
				\todo{Content}

				\paragraph{Beispiel} % 4.62
					\todo{Content}
				% end
			% end

			\subsubsection{Approximation der reduzierten Hesse-Matrix} % 4.69
				\todo{Content}
			% end
		% end

		\subsection{SQP-Verfahren} % 4.52, 4.53, 4.57, 4.75
			\todo{Content}
		% end

		\subsection{Bemerkungen} % 4.49, 4.70, 4.75, 4.76, 4.77, 4.78, 4.79
			\todo{Content}
		% end

		\subsection{Beispiele} % N/A
			\todo{Content}

			\subsubsection{Optimale Steuerung eines 6-gelenkigen Industrieroboters} % 4.71, 4.72
				\todo{Content}
			% end

			\subsubsection{PKW-Fahrt} % 4.73, 4.74
				\todo{Content}
			% end
		% end
	% end
% end

\chapter{Berechnung von Ableitungen} % 5.1, 5.2, 5.3, 5.4
	\todo{Content}

	\section{Finite-Differenzen-Approximation (numerische Differentiation)} % 5.5
		\todo{Content}

		\subsection{VorwÃ¤rtsdifferenzen-Approximation} % 5.6, 5.13
			\todo{Content}

			\subsubsection{Fehler} % 5.8, 5.12
				\todo{Content}

				\paragraph{Approximationsfehler} % 5.9
					\todo{Content}
				% end

				\paragraph{Funktionsgenauigkeit} % 5.10
					\todo{Content}
				% end

				\paragraph{Rundungsfehler} % 5.11
					\todo{Content}
				% end
			% end

			\subsubsection{Wahl der Schrittweite} % 5.14, 5.15, 5.16, 5.17
				\todo{Content}
			% end
		% end

		\subsection{Zentrale-Differenzen-Approximation} % 5.18, 5.19, 5.21, 5.22
			\todo{Content}
		% end
	% end

	\section{Numerische Differentiation von Simulationsmodellen} % 5.23, 5.24, 5.25
		\todo{Content}

		\subsection{Ableitung von ODE-Simulationsmodellen} % 5.27, 5.28
			\todo{Content}
		% end

		\subsection{Externe numerische Differentiation} % N/A
			\todo{Content}

			\subsubsection{Naiver Ansatz} % 5.29, 5.30, 5.31
				\todo{Content}
			% end

			\subsubsection{Gekoppelte VorwÃ¤rtsdifferenzen-Approximation} % 5.32
				\todo{Content}
			% end
		% end

		\subsection{Interne numerische Differentiation} % 5.33, 5.34
			\todo{Content}
		% end
	% end

	\section{Symbolische Differentiation} % 5.35, 5.36
		\todo{Content}
	% end

	\section{Automatisches Differenzieren} % 5.37, 5.38, 5.39, 5.40
		\todo{Content}

		\paragraph{Beispiel} % 5.41, 5.42, 5.43
			\todo{Content}
		% end
	% end
% end

\chapter{Minimierung von Abweichungen} % 6.1, 6.2, 6.3
	\todo{Content}

	\section{ParameterschÃ¤tzung bei ODE-Systemen} % 6.5, 6.7
		\todo{Content}
	% end

	\section{GÃ¼tekriterien zur Minimierung von Abweichungen} % 6.8, 6.9
		\todo{Content}
	% end

	\section{Lineare Ausgleichsrechnung} % 6.10, 6.11
		\todo{Content}
	% end

	\section{OptimalitÃ¤tsbedingungen und Spezialverfahren} % 6.12, 6.13, 6.17
		\todo{Content}

		\subsection{Quasi-Newton} % 6.14
			\todo{Content}
		% end

		\subsection{GauÃ-Newton Verfahren} % 6.15
			\todo{Content}
		% end

		\subsection{Levenberg-Marquardt Verfahren} % 6.16
			\todo{Content}
		% end

		\subsection{SQP-Verfahren} % 6.18
			\todo{Content}
		% end
	% end

	\section{Normalen-Gleichungen} % 6.22, 6.23
		\todo{Content}

		\paragraph{Beispiel} % 6.24, 6.25, 6.26, 6.27, 6.28
			\todo{Content}
		% end
	% end

	\section{Interpretation von Berechnungsergebnissen} % 6.29
		\todo{Content}

		\subsection{MÃ¶gliche Ursachen fÃ¼r Schwierigkeiten} % 6.30, 6.31
			\todo{Content}
		% end
	% end

	\section{Die Varianz-Kovarianz-Matrix} % 6.32, 6.33, 6.34
		\todo{Content}
	% end

	\section{Optimale Experimentgestaltung} % 6.35, 6.36, 6.37
		\todo{Content}
	% end

	\section{Beispiele} % 6.39
		\todo{Content}

		\subsection{ParameterabhÃ¤ngige Gesamtfahrzeugdynamik} % 6.39, 6.40
			\todo{Content}

			\subsubsection{Simulierte Messwerte} % 6.41, 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Echte Messwerte} % 6.44
				\todo{Content}
			% end

			\subsubsection{Vergleich der Verfahren} % 6.45
				\todo{Content}
			% end
		% end

		\subsection{ParameterschÃ¤tzung fÃ¼r "BioBiped"} % 6.47, 6.48, 6.49
			\todo{Content}
		% end
	% end
% end
