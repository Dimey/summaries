% !TeX spellcheck = en_US

\chapter{EinfÃ¼hrung} % 1.5
	\todo{Content}

	\section{Beispiele} % 1.6, 1.7, 1.8, 1.10, 1.11
		\todo{Content}
	% end

	\section{Fragestellungen} % 1.12
		\todo{Content}
	% end

	\section{Allgemeine Formulierung eines Optimierungsproblems} % 1.13, 1.14
		\todo{Content}
	% end

	\section{Statische vs. Dynamische Optimierung} % 1.15, 1.16, 1.17, 1.18
		\todo{Content}
	% end

	\section{Klassifizierung von Optimierungsverfahren} % 1.23
		\todo{Content}
	% end

	\section{Typische Struktur} % 1.24
		\todo{Content}
	% end
% end

\chapter{Gradient-Based Optimization without Constraints}
	\section{Solution Characterization}
		This section covers the theoretical results for solving a nonlinear optimization problem using calculus.
	
		\subsection{One-Dimensional Optimization}
			For a one-dimensional function \( \varphi(p) : \R \to \R \) the first-order necessary condition for a minimum is that the derivative of \( \varphi(p) \) w.r.t. the parameter \(p\) vanishes:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} \! = 0
			\end{align*}
			Where \( p^\ast \) denotes the optimal solution, i.e. the minimum.
			
			All solutions that fulfill this condition are \emph{candidates} for a minimum. If \( \varphi \) is twice continuous differentiable, the sufficient condition for a minimum is that the second-order derivative is positive:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} > 0
			\end{align*}
			Then \(p^\ast\) is called a \emph{strict minimum}. This condition is sufficient, but not necessary! The second-order necessary condition for a minimum is that the second-order derivative is non-negative, i.e. \( \dv{\varphi(p^\ast)}{p} \geq 0 \).
			
			\subsubsection{Possibilities for a Minimum}
				There are three cases for a minimum:
				\begin{itemize}
					\item \(\varphi(p)\) is twice continuously differentiable everywhere
					\item \(\varphi'(p)\) is not continuous everywhere but at \(p^\ast\)
					\item \(\varphi'(p)\) is not continuous everywhere, not even at \(p^\ast\)
				\end{itemize}
				While the latter case is common, it is problematic as the solution can typically not be determined analytically (if a function is not continuous at one point, it is rarely invertible).
			% end
		% end

		\subsection{Multi-Dimensional Optimization}
			\label{subsec:multiDimensionalOptimalityConditions}
		
			For multi-dimensional objective functions \( \varphi : \R^{n_p} \to \R \), where \(n_p\) is the dimensionality of the parameters, the first-order necessary condition is that the gradient vanishes:
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) =
					\begin{bmatrix}
						\pdv{\varphi}{p_1} \\
						\vdots \\
						\pdv{\varphi}{p_{n_p}}
					\end{bmatrix}
				=
					\begin{bmatrix}
						0 \\
						\vdots \\
						0
					\end{bmatrix}
			\end{align*}
			
			If \(\varphi(\vec{p})\) is twice continuously differentiable, the second-order sufficient condition is that the Hessian of \(\varphi(\vec{p})\) is positive definite. Analogous to the one-dimensional case, the second-order necessary condition is that the Hessian is positive semi-definite, i.e.:
			\begin{align*}
				\mat{H}_\varphi(\vec{p}^\ast) =
					\begin{bmatrix}
						\pdv[2]{\varphi}{p_1}        & \cdots & \pdv{\varphi^2}{p_{n_p} p_1} \\
						\vdots                       & \ddots & \vdots                       \\
						\pdv{\varphi^2}{p_1 p_{n_p}} & \cdots & \pdv[2]{\varphi}{p_{n_p}}
					\end{bmatrix}
				> 0
				\quad\text{or respectively}\quad
				\mat{H}_\varphi(\vec{p}^\ast) \geq 0
			\end{align*}

			\paragraph{Example} % 2.7, 2.8, 2.9
				\todo{Content}
			% end
		% end
	% end

	\section{Numerical Gradient-Based Methods}
		\subsection{Starting Point}
			\subsubsection{Structure of Gradient-Based Methods}
				Given a initial approximation \( \vec{p}^{(0)} \), an approximation of the minimum \( \vec{p}^\ast \) is wanted. Gradient-based methods are iteration methods based on the iteration rule
				\begin{align*}
					\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)},\quad k = 0, 1, 2, \cdots
				\end{align*}
				where
				\begin{itemize}
					\item \(\vec{d}^{(k)}\) is the search direction found as the solution of a linear sub problem and
					\item \(\alpha^{(k)}\) is the step size found by a one-dimensional \emph{line search}.
				\end{itemize}
				The iteration terminates once \( \vec{p}^{(k + 1)} \) is "close to" \(\vec{p}^\ast\), e.g. when the gradient nearly vanishes.
			% end

			\subsubsection{Descent Direction}
				Gradient-based methods have to ensure the the local search direction \(\vec{d}^{(k)}\) really is a descent direction (the algorithm shall not "run up the hill"). This property is ensured iff the angle \( \delta \) between the search direction and the gradient \( \grad{\varphi}\big(\vec{}^{(k)}\big) \) greater than \SI{90}{\degree}, i.e.
				\begin{align}
					\cos \delta = \frac{\big( \vec{d}^{(k)} \big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)}{\big\lVert \vec{d}^{(k)} \big\rVert \cdot \big\lVert \grad{\varphi}\big(\vec{p}^{(k)}\big) \big\rVert} < 0 \quad\iff\quad \big( \vec{d}^{(k)} \big)^T \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big) < 0  \label{eq:descentDirection}
				\end{align}
				This is called the "necessary descent condition".
			% end

			\subsubsection{Algorithmic Structure}
				The \autoref{alg:gradientBasedAlgorithmStructure} shows the basic structure of any gradient-based optimization algorithm.
			
				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
					\While{not converged}{
						Determine new search direction: \tabto{6cm} \( \vec{d}^{(k)} \in \R^{n_p} \) \;
						Determine new step size: \tabto{6cm} \( \alpha^{(k)} \in \R^+ \) \;
						Update the approximation: \tabto{6cm} \( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( k \gets k + 1 \) \;
					}
				
					\caption{Algorithmic structure of a gradient-based optimization algorithms.}
					\label{alg:gradientBasedAlgorithmStructure}
				\end{algorithm}
			% end
		% end

		\subsection{Steepest Descent}
			Steepest descent is the straightforward way for getting a search direction. The search direction is just set to the negative of the gradient:
			\begin{align*}
				\vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Often quickly reaches areas around the local minimum.
						\item No second derivatives needed.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Very slow in areas around the local minimum compared to (Quasi-) Newton Methods.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Conjugate Gradient}
			Basic approach for conjugate gradient:
			\begin{align*}
				\vec{d}^{(0)} &= -\grad{\varphi}\big(\vec{p}^{(0)}\big) \\
				\vec{d}^{(k)} &= \text{Component of } -\grad{\varphi}\big(\vec{p}^{(k)}\big) \text{ that is conjugate to } \vec{d}^{(0)}, \vec{d}^{(1)}, \cdots, \vec{d}^{(k - 1)}
			\end{align*}

			For a quadratic objective function
			\begin{align*}
				\varphi(\vec{p}) = \frac{1}{2} \vec{p}^T \mat{A} \vec{p} - \vec{b}^T \vec{p}
			\end{align*}
			with a positive semi-definite matrix \(\mat{A}\) and constant \(\mat{A}\), \(\vec{b}\), the search direction is given as the solution of:
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(j)} = 0,\quad j = 1, \cdots, k - 1
			\end{align*}
			
			With an optimal step size \( \alpha^{(k)} \), i.e.
			\begin{align*}
				\alpha^{(k)} = \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big)
					\quad\implies\quad \alpha^{(k)} = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(k)}} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \big(\vec{d}^{(k)}\big)
			\end{align*}
			the minimum of \(\varphi\) is reached in \(n_p\) steps.
			
			The extension for nonlinear objective functions is given in~\autoref{alg:conjugateGradient}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \vec{d}^{(0)} \gets -\grad{\varphi}\big(\vec{p}^{(0)}\big) \) and \( k \gets 0 \) \;
				\While{not converged}{
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( \beta^{(k + 1)} \gets \frac{\big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)}{\big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)} \) \;
					\( \vec{d}^{(k + 1)} \gets -\grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \beta^{(k + 1)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}
				
				\caption{Conjugate Gradient for nonlinear Objective Function.}
				\label{alg:conjugateGradient}
			\end{algorithm}
		
			\begin{itemize}
				\item Exact line search necessary.
				\item Different variants of GC-algorithms mainly distinguish in the choice of of \( \beta^{(k)} \).
				\item Advantages:
					\begin{itemize}
						\item Faster then steepest descent.
						\item No explicit storing of the Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \) necessary.
						\item No explicit matrix-vector multiplication.
						\item Useful even for extreme high dimensions \( n_p \).
						\item Exact for quadratic objectives \( \varphi(\vec{p}) \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item A lot slower then (Quasi-) Newton Methods.
						\item In general not useful for optimizing simulation models.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Newton Method}
			Assuming the approximation of each iteration, \( \vec{p}^{(k)} \), is close to the minimum \( \vec{p}^\ast \), the gradient \( \grad{\varphi}(\vec{p}^\ast) \) can be taylor-expanded around \( \vec{p}^{(k)} \):
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) \overset{T\big(\!\vec{p}^{(k)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \big( \vec{p}^\ast - \vec{p}^{(k)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			By leaving our the higher order terms the search direction \( \vec{d}^{(k)} \coloneqq \vec{p}^\ast - \vec{p}^{(k)} \) is given by the solution of the system of linear equations
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The realization is shown in~\autoref{alg:newtonMethod}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
				\While{not converged}{
					Solve \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}
				
				\caption{Newton Method}
				\label{alg:newtonMethod}
			\end{algorithm}
		
			When plugging the search direction into the necessary descent condition~\eqref{eq:descentDirection}
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
					= -\Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)^T\Big) \Big(\!\mat{H}_\varphi\big(\vec{p}^{(k)}\big)\!\Big)^{-1} \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
					< 0
			\end{align*}
			it is clear that this is only fulfilled iff the Hessian is positive definite. But this is only the case in a region around the minimum! If the approximation is far away from the minimum, the search direction might also be an ascent direction causing the Newton method to diverge. There are two main solutions to this problem:
			\begin{enumerate}
				\item If the Hessian is not positive definite, replace it by an identity matrix. That is, set the search direction to the steepest descent.
				\item Regularize the equation system with a weight \( \nu > 0 \) such that the new matrix is positive definite (this "rotates" the matrix in the direction of the steepest descent such that the new search direction always fulfills the descent condition):
			\end{enumerate}
			\begin{align*}
				\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \nu \mat{I} \Big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Near to strong local minima of twice continuous differentiable objective, the Newton method is quadratic convergent.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Computationally expensive as a linear system has to be solved in every iteration.
						\item Not only first, but also second-order derivatives have to be available. This is a big disadvantage:
							\begin{itemize}
								\item In practice, the first derivative is rarely and the second derivative is never available.
								\item Even a single wrong component in the gradient or the Hessian destroys the quadratic convergence.
							\end{itemize}
					\end{itemize}
			\end{itemize}

			\subsubsection{Availability of Second-Order Derivatives}
				The obvious idea is to approximate the Hessian using finite differences. The approximated Hessian is then given as
				\begin{align*}
					\mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T )  \label{eq:approxHessianSymm}
				\end{align*}
				where \(\tilde{\mat{H}}\) is given by
				\begin{align*}
					\tilde{\mat{H}}_i = \pdv{p_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big) \approx \frac{1}{h_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)} + h_i \vec{e}_i\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big)
				\end{align*}
				where \( \tilde{\mat{H}}_i \) is the \(i\)-th column of \(\tilde{\mat{H}}\). The equation~\eqref{eq:approxHessianSymm} is used to force the Hessian to be symmetric.
				
				Problems:
				\begin{itemize}
					\item The Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T ) \) is not necessarily positive definite.
					\item In every iteration the Gradient has to be evaluated \(n_p\) times more.
					\item The linear system still needs to be solved.
					\item Only useful for high-dimensional problems with sparse gradients!
				\end{itemize}
				Another possibility are \emph{Quasi-Newton Methods}.
			% end
		% end

		\subsection{Quasi-Newton Methods}
			Quasi-Newton methods are equivalent to the Newton method, however, the Hessian (or its inverse) is approximated by a positive definite matrix
			\begin{align*}
				\hat{\mat{H}}^{(k)} \approx \mat{H}_\varphi\big(\vec{p}^{(k)}\big)
			\end{align*}
			that is updated in every iteration. This yields a lot of advantages over the classic Newton method:
			\begin{itemize}
				\item Only first-order derivatives needed.
				\item As \( \hat{\mat{H}} \) constructed positive definite, the descent condition is fulfilled anytime.
				\item If the inverse Hessian is directly approximated, only \( \mathcal{O}(n_p^2) \) multiplications instead of \( \mathcal{O}(n_p^3) \) for solving the linear system.
			\end{itemize}
		
			But how to do the Hessian update? By Taylor-expanding the gradient \( \grad{\varphi}\big(\vec{p}^{(k)}\big) \) around \( \vec{p}^{(k + 1)} \)
			\begin{align*}
				\grad{\varphi}\big(\vec{p}^{(k)}\big) \overset{T\big(\!\vec{p}^{(k + 1)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \big( \vec{p}^{(k)} - \vec{p}^{(k + 1)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			and cutting off the higher-order terms, the following approximation holds:
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \vec{d}^{(k)} \approx \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The approximation of the Hessian must therefore fulfill the \emph{secant condition}
			\begin{align*}
				\tilde{\mat{H}}^{(k + 1)} \vec{d}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			There exist a lot of different approaches for doing the Hessian updates \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \mat{U}{(k)} \) for rank-1 or rank-2 matrices \(\mat{U}^{(k)}\):
			\begin{itemize}
				\item Approach for rank-1 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T \)
				\item Approach for rank-2 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T + \beta_2 \vec{v} \vec{v}^T \)
			\end{itemize}
			The vectors \( \vec{u}, \vec{v} \in \R^{n_p} \) and scalars \( \beta_1, \beta_2 \in \R \) must have to be chosen such that \( \tilde{\mat{H}}^{(k + 1)} \) is
			\begin{itemize}
				\item positive definite,
				\item symmetric,
				\item fulfills the secant condition and
				\item adding up the matrices is efficient and robust.
			\end{itemize}

			\subsubsection{BFGS-Update}
				The most known rank-2 update for the Hessian is the \emph{BFGS-Update}\footnote{"BFGS" stands for the authors Broyden, Fletcher, Goldfarb and Shanno.}
				\begin{align*}
					\vec{u} & = \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} &   \beta_1 & = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \\
					\vec{v} & = \vec{g}^{(k)}                       &   \beta_2 & = \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}}
				\end{align*}
				where \( \vec{g}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \). Plugging that into the general approach for rank-2 updates yields the update rule for BFGS-approximations:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)}
							- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
							+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T
				\end{align*}
				\begin{itemize}
					\item The direct approximation of the Hessian inverse is not really robust (e.g. for a non-optimal step size rule).
					\item A better alternative is to directly approximate a useful factorization, e.g. the Cholesky decomposition. This is more robust and equally efficient (\( \mathcal{O}(n_p^2) \)).
				\end{itemize}
			
				The pseudo code for the BFGS update is shown in~\autoref{alg:bfgs}.
				
				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \tilde{\mat{H}}^{(0)} = \mat{I} \) and \( k \gets 0 \) \;
					\While{not converged}{
						Solve \( \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
						\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
						\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( \tilde{\mat{H}}^{(k + 1)} \gets \tilde{\mat{H}}^{(k)} - \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T + \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T \) \;
						\( k \gets k + 1 \) \;
					}
					
					\caption{Quasi-Newton Method with BFGS Update.}
					\label{alg:bfgs}
				\end{algorithm}
			% end
		% end

		\subsection{Comparison} % 2.29, 2.30, 2.31, 2.32, 2.33, 2.34, 2.35, 2.36
			\todo{Content}
			
			\begin{itemize}
				\item 
			\end{itemize}
		% end
		
		\subsection{Notes and Discussion}
			\begin{itemize}
				\item The convergence of gradient-based methods can be shown under weak preconditions.
				\item As the search direction is only a local descent direction, gradient-based algorithms only yields local minima.
				\item There is no algorithm that can guarantee to find the global minimum!
				\item Some approaches for determining a global minimum:
					\begin{itemize}
						\item Choose the initialization well, i.e. close to the global minimum.
						\item Execute the algorithm multiple times with different starting points.
						\item Validate the solution against properties of the original problem.
						\item Execute direct search methods beforehand to find promising regions for the local minimum search.
					\end{itemize}
			\end{itemize}
		
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item If gradient-based algorithms converge, they converge utterly fast.
						\item Efficient also for high-dimensional problems, i.e. a large \( n_p \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Only applicable for functions that are differentiable almost everywhere.
						\item Require gradient information exact up to four to eight decimal points.
						\item Convergence to a local minimum near the initialization \( \vec{p}^{(0)} \).
						\item Require some expert knowledge.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Step Size Rules, Line Search}
		\label{sec:gradientUnconsStepSize}
	
		In every iteration of gradient-based algorithms, the step size has to be determined by minimizing the one-dimensional function:
		\begin{align*}
			\psi(\alpha) = \varphi\big(\vec{p}^{(k)} + \alpha \vec{d}^{(k)}\big)
		\end{align*}
		As the necessary first-order condition for a minimum, the derivative w.r.t. \(\alpha\) has to vanish:
		\begin{align*}
			\dv{\psi\big(\alpha^{(k)}\big)}{\alpha} = \dv{\alpha} \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) = \Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \vec{d}^{(k)} \overset{!}{=} 0
		\end{align*}
		Thus the gradient of \(\varphi\) at the minimum \(\alpha^{(k)}\) as to be orthogonal to the search direction \(\vec{d}^{(k)}\). Intuitively, the optimal step size has to be chosen such that the iteration step cannot go any further without ascending again ("hitting an ascending contour line").
		
		Goal of the line search is to reach the minimum of \(\psi\) with least invocations of \(\psi\) as possible. Most of the existing search methods can be classified into
		\begin{itemize}
			\item \emph{Polynomial approximation}, e.g. quadratic or cubic interpolation
			\item \emph{Direct search methods}, e.g. Fibonacci-search, golden ratio search
			\item \emph{Optimal vs. non-optimal search methods}, e.g. by finding an improvement but not the minimum
			\item Usage of the gradient information \( \psi' \) or not.
		\end{itemize}
	
		Requirements:
		\begin{itemize}
			\item Finding the \(\alpha^{(k)}\) with a minimal value of \(\psi\).
			\item Do not waste too much computation time on the line search.
		\end{itemize}
		In general, an exact line search requires lots of \(\psi\)-evaluations. But how far does \(\psi\) need to be reduced in order to guarantee convergence? In general, the condition \( \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) < \varphi\big(\vec{p}^{(k)}\big) \) is not enough!
		
		\subsection{Inexact Line Search}
			Procedure: Generate and inspect a series of candidates for \(\alpha^{(k)}\) and terminate once one of the candidates fulfills specific criteria, e.g. the Armijo rule or Wolfe conditions.
			
			\subsubsection{Armijo Rule}
				The \emph{Armijo rule} guarantees a sufficient reduction in \(\varphi\):
				\begin{align*}
					\varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) \leq \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \psi'(0)
				\end{align*}
				Where \( 0 < c_1 < 1 \) is any constant, e.g. \( c_1 = 10^{-4} \).
				
				Hence, the minimal reduction has to be proportional to \(\alpha^{(k)}\) and the derivative \( \psi'(0) \).
			% end
			
			\subsubsection{Curvature Condition}
				But a sufficient descent condition is not enough as the step sizes must not be too small (otherwise progress would stop). Thus a second condition has to be employed, the \emph{curvature condition} that requires a minimum curvature on \(\psi\):
				\begin{align*}
					\Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \geq c_2 \cdot \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = c_2 \psi'(0) \quad\iff\quad \psi'\big(\vec{p}^{(k)}\big) \geq c_2 \psi'(0)
				\end{align*}
				Where \( c_1 < c_2 < 1 \) is any constant, e.g. \( c_2 = 0.9 \).
			% end
			
			\subsubsection{Wolfe and Goldstein Conditions}
				Combining the Armijo rule and the curvature condition yields the Wolfe conditions that guarantee both a minimal reduction and a minimal curvature. They are especially useful for Quasi-Newton methods as the Wolfe conditions are scale invariant, i.e. independent of
				\begin{itemize}
					\item multiplying \(\varphi\) with any constant and
					\item affine transformations of \(\vec{p}\).
				\end{itemize}
			
				There are other possibilities are, e.g. the \emph{Goldstein conditions}
				\begin{align*}
					\varphi\big(\vec{p}^{(k)}\big) + (1 - c) \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
						\leq \varphi\big( \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \big)
						\leq \varphi\big( \vec{p}^{(k)} \big) + c \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
				\end{align*}
				with any \( 0 < c < 1/2 \), that are useful for Newton, but not for Quasi-Newton methods.
			% end
		% end
		
		\subsection{Notes}
			\begin{itemize}
				\item For gradient-based methods a step size \( \alpha^{(k)} > 1 \) is in general not useful because:
					\begin{itemize}
						\item The search direction \( \vec{d}^{(k)} \) is determined using a linear or quadratic Taylor approximation.
						\item The Taylor approximation is only valid in a small region around the current approximation \( \vec{p}^{(k)} \), i.e. for \( 0 < \alpha^{(k)} \leq 1 \).
					\end{itemize}
				\item The local quadratic or super-linear convergence of Newton-type methods is visible in practice as the last step can be executed with full step size \( \alpha^{(k)} = 1 \).
			\end{itemize}
		% end
	% end

	\section{Trust Region Methods}
		Gradient-based methods with line search determine a fixed search direction and adjust the step size \( \alpha^{(k)} \) according to that search direction to reach global convergence.
		
		Another approach is to determine both the length and direction of \( \vec{d}^{(k)} \). The iteration step then becomes
		\begin{align*}
			\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \vec{d}^{(k)}
		\end{align*}
		without any explicit step size. The length and direction of \( \vec{d}^{(k)} \) are then determined as a solution of the quadratic sub-problem
		\begin{align*}
			\min_{\vec{d} \in \R^{n_p}} &\, \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d} + \frac{1}{2} \vec{d}^T \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d} \\
			\mathrm{subject~to}\quad &\,
				\begin{alignedat}[t]{2}
					\lVert \vec{d} \rVert_2 & \leq \delta
				\end{alignedat}
		\end{align*}
		where \(\delta\) describes the area around the current approximation \( \vec{p}^{(k)} \) where the quadratic approximation of \( \varphi\big(\vec{p^{(k)}} + \vec{d}\big) \) makes sense, i.e. the \emph{trust region}.
		
		The value of \(\delta\) is extremely important for the efficiency of the method.
		\begin{itemize}
			\item If \(\delta\) is too small, opportunities for large steps are missed.
			\item If \(\delta\) is too large, the minimum of the quadratic approximation might be far off the minimum of the objective if the Hessian is indefinite or negative definite.
		\end{itemize}
		It is possible to add a regularization parameter \(\beta \geq 0\) to the quadratic approximation
		\begin{align*}
			\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \beta \mat{I} \Big) \vec{d} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
		\end{align*}
		such that the matrix is positive semidefinite. The solution of this regularized problem also solves the trust region problem if either \( \beta = 0 \), \( \lVert \vec{d} \rVert \leq \delta \) or \( \beta \geq 0 \), \( \lVert \vec{d} \rVert = \delta \).
	% end

	\section{Rate of Convergence}
		A common criteria to measure the performance of a gradient method are \emph{rates of convergence}. These provide information on how fast an algorithm converges, i.e. how fast \( \vec{p}^{(k)} \to \vec{p}^\ast \) or \( \big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert \to 0 \).
		
		\textbf{Definition:}
		Let \( \big(\{\, \vec{p}^{(k)} \,\big\} \) be the series of approximations produced by an optimization method. Then this series has rate of convergence \(r\) if \(r\) is the greatest positive number such that the limit
		\begin{align*}
			0 \leq \liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert^r} = \gamma < \infty
		\end{align*}
		converges (where \( \vec{p}^\ast = \liminfty[k] \vec{p}^{(k)} \)). If \( r = 1 \), then \( \gamma < 1 \) has to hold for the method to converge.
		
		A sequence is said to converge superlinearly if
		\begin{align*}
			\liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert} = 0
		\end{align*}
		holds. Even though technically this condition holds for \( r > 1 \), in practice only methods with \( 1 < r < 2 \) are said to converge superlinearly (e.g. for \(r = 2\), the sequence is called to converge quadratically).

		\paragraph{Examples} % 2.53
			\todo{Content}
		% end
		
		\subsection{Gradient-Based Methods}
			Under ideal conditions (i.e. \(\varphi\) is twice continuously differentiable and \( \mat{H}_\varphi(\vec{p}^\ast) \) is positive definite), all of the following hold:
			\begin{itemize}
				\item Steepest descent is (locally) linearly convergent (with exact line search).
				\item The Newton method is (locally) quadratically convergent.
				\item Quasi-Newton methods with BFGS-update are (locally) superlinearly convergent (for inexact line search using the Wolfe conditions).
			\end{itemize}
			But: Even a single wrong component in the Hessian reduced the quadratic convergence of the Newton method to linear convergence!
		% end
	% end
% end

\chapter{Gradient-Free Optimization without Constraints}
	This chapter covers different types of sampling methods (direct search methods):
	\begin{enumerate}
		\item Metaheuristics (random search),
		\item Deterministic Sampling Methods (pattern search) and
		\item Surrogate optimization.
	\end{enumerate}
	These optimization methods only use evaluations of the objective \(\varphi\) and do not user gradient information (neither analytically nor using numerical differentiation). That is, \(\varphi\) is used as a "Black Box" for function evaluations.

	Even for \(\varphi\) that are not differentiable and have lots of local minima, gradient-free algorithms are remarkably robust, but can also fail fast.

	\section{Introduction}
		The objective \(\varphi\) that is to be optimized often has suboptimal properties, e.g.:
		\begin{itemize}
			\item the evaluation is noisy
			\item not differentiable
			\item high computation time for evaluation (e.g. when a simulation has to be run for each evaluation)
		\end{itemize}
	
		But gradient-free techniques have a wide range of applications, e.g. in automobile, aerospace industry, robotics, financial, etc. The goal is to reduce the objective function as much as possible and find regions in which the objective is differently sensitive w.r.t. changes in the optimization variables.

		\subsection{Simulation-Based Optimization}
			In a simulation-based setting, the objective is evaluated by running a simulation (e.g. by solving a set of differential equations or running a real experiment). This yields a suboptimal setting for optimization, yet a common one:
			\begin{itemize}
				\item Only function values are computable and not gradient information is available.
				\item Source code of the optimization is often not available. Hence, no information about how the simulation is computed.
				\item Discontinuous/non-differentiable systems and model properties, e.g. due to collisions.
				\item Non-differentiable structure inside the simulation (e.g. if-else).
				\item Discontinuities caused by subprograms, heuristics, table data, \dots
				\item Function evaluations are computationally expensive.
				\item Numerical "noise" overlay the actual system properties.
				\item Non-deterministic simulations (e.g. due to complex friction).
			\end{itemize}
			Hence, simulation-based optimization is a black box problem and can be solved (or approximated) using gradient-free methods!
		% end

		\subsection{Black-Box Optimization}
			Naturally, gradient-based optimization methods are not well-suited for problems with "low" differentiability and computationally expensive function evaluations. These problems may be solved using gradient-free optimization methods. But\dots
			\begin{itemize}
				\item Gradient-based techniques are really slow in comparison to gradient-based ones for differentiable optimization problems.
				\item They need a lot of function evaluations for high-dimensional problems and are thus practically only applicable for problems with dimensions \( n_p < 100 \), better \( n_p < 20 \).
				\item Have lots of problems with nonlinear equality constraints!
				\item The theory of gradient-free methods is not as mature as the theory for gradient-based methods.
			\end{itemize}
		% end
	% end

	\section{Metaheuristics}
		\subsection{Evolutionary Algorithm (EA)}
			A \emph{evolutionary algorithm} mimics the biological evolutionary strategy with random search methods.
			
			\begin{enumerate}
				\item Initialization: \tabto{2.5cm} Choose one "Parent" \( \vec{p}^{(0)} \) and a number of descendants \(\ell\).
				\item Iteration: \tabto{2.5cm} Create \(\ell\) descendants via "mutation"
			\end{enumerate}
			\begin{align*}
				\vec{p}^{(k, i)} = \vec{p}^{(k)} + \alpha_i^{(k)} \vec{d}_i,\quad i = 1, \cdots, \ell
			\end{align*}
			\begin{itemize}
				\item[] where \( \vec{d}_i \in \R^{n_p} \) are vectors of Gaussian distributed variables and \( \alpha_i \in \R \) art suitable "mutation step sizes" Then select the descent with the lower \( \varphi \) value and repeat.
			\end{itemize}
		% end

		\subsection{Genetic Algorithms (GA)}
			\emph{Genetic algorithms} are inspired by biological evolutionary strategies the positive properties caused by mutation are kept through natural selection. GAs are applicable for both real and discrete optimization variables \(\vec{p}\).
			\begin{enumerate}
				\item Initialization: \tabto{3cm} Choose a suitable set of different "individuals" (first generation).
				\item Evaluation: \tabto{3cm} Determine the "fitness" of each candidate using the objective/fitness function.
				\item Selection: \tabto{3cm} Randomly select candidates of the current generation (the higher the fitness, the higher the probability to be chosen).
				\item Recombination: \tabto{3cm} Combine values (genomes) of the selected individuals and create new individuals.
				\item Mutation: \tabto{3cm} Randomly change the genomes.
				\item New Generation: \tabto{3cm} Select new individuals as the new generation and continue with step 2.
			\end{enumerate}

			\paragraph{Example} % 3.13, 3.14
				\todo{Content}
			% end
		% end

		\subsection{Further Metaheuristics}
			Further metaheuristics based on real representations of \(\vec{p}\) like in evolutionary algorithms:
			\begin{itemize}
				\item Particle Swam: Population method, uses idea of combining local and swarm knowledge, direction and speed for particles are adjusted
				\item \dots
			\end{itemize}
		
			Further metaheuristics based on binary representations of \(\vec{p}\) like in genetic algorithms:
			\begin{itemize}
				\item Tabu Search: A list of possible manipulations is given, another lists dynamically the inverses of them, these cannot be applied any more
				\item \dots
			\end{itemize}
		% end
	% end

	\section{Deterministic Sampling Methods (Pattern Search Methods)}
		Deterministic sampling methods can be further categorized into
		\begin{itemize}
			\item Qualitative methods: Only ranking w.r.t. to the function value.
				\begin{itemize}
					\item Simplex Methods
					\item Coordinate- or compass-search
					\item Multidirectional search
					\item Pattern search methods
					\item \dots
				\end{itemize}
			\item Quantitative methods: Consideration of the real function values.
				\begin{itemize}
					\item Implicit filtering (based on the simplex method)
					\item DIRECT (dividing rectangles)
					\item \dots
				\end{itemize}
		\end{itemize}

		\subsection{Nelder-Mead Simplex Method}
			A \emph{simplex} is a simple object that consists of \( n_p + 1 \) points \( \vec{p}^{(i)} \) in the parameter space (which is \(n_p\)-dimensional). In a 2D space, the three points form a triangle. In the iteration phase the values of \(\varphi\) at the corners are compared and the simplex is transformed according to specific rules (see~\autoref{subsec:nelderMeadIteration}). The algorithm terminates once the simplex contracts onto a single point.

			\subsubsection{Iteration Phase}
				\label{subsec:nelderMeadIteration}
			
				The iteration phase starts by sorting the edges according to its \(\varphi\)-values:
				\begin{align*}
					\varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p^{(2)}}\big) \leq \cdots \leq \varphi\big(\vec{p}^{(n_p + 1)}\big)
				\end{align*}
				where \( \vec{p}^{(1)} \) is called the \emph{best} point and \( \vec{p}^{(n_p + 1)} \) is called the \emph{worst}. The algorithm now tried to replace the worst point \( \vec{p}^{(n_p + 1)} \) with another point of the form
				\begin{align*}
					\vec{p}(\mu) = (1 + \mu) \bar{\vec{p}} - \mu \vec{p}^{(n_p + 1)}
				\end{align*}
				Where \(\bar{\vec{p}}\) is the centroid of the of all points \emph{except the worst}, i.e.:
				\begin{align*}
					\bar{\vec{p}} = \frac{1}{n_p} \sum_{i = 1}^{n_p} \vec{p}^{(i)}
				\end{align*}
				This corresponds to a reflection of the worst point over the centroid with a weight \(\mu\) that specifies "how far the point point gets pushed out", i.e. the ratio of the original distance of the worst point to the centroid that is preserved while reflecting. If \(\mu = 1\), the point is mirrored.
				
				In every iteration, the value \(\mu\) is chosen of a set of four values
				\begin{align*}
					-1 < \mu_{ic} < 0 < \mu_{oc} < \mu_{r} < \mu_{e}
				\end{align*}
				for example \( (\mu_{ic}, \mu_{oc}, \mu_r, \mu_\mathit{e}) = (-0.5, 0.5, 1, 2) \).
			% end

			\subsubsection{Algorithm}
				Some termination criteria are for example:
				\begin{itemize}
					\item Exactness in the objective: \( \varphi\big( \vec{p}^{(n_p + 1)} \big) - \varphi\big(\vec{p}^{(1)}\big) \leq \varepsilon \)
					\item Maximum number of function evaluations: \( k = k_\mathrm{max} \)
					\item Sufficient small distance on the simplex corners.
				\end{itemize}
			
				\begin{itemize}
					\item Initialization: Choose a start simplex, evaluate the objective at the corners, sort them and set \( k = n_p + 1 \).
					\item Iteration: While \( \varphi\big( \vec{p}^{(n_p + 1)} \big) - \varphi\big(\vec{p}^{(1)}\big) > \varepsilon \) and \( k < k_\mathrm{max} \), do:
						\begin{enumerate}[label = (\alph*)]
							\item Calculate the centroid \( \bar{\vec{p}} = \frac{1}{n_p} \sum_{i = 1}^{n_p} \vec{p}^{(i)} \).
							\item \emph{Reflection:} If \( \varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(n_p)}\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_r) \); go to (g). \\
									"Use the reflected point if it is better than the second worst, but not better than the best."
							\item \emph{Expansion:} If \( \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(1)}\big) \), then: \\
									"If the reflected point is better than the best, \dots"
								\begin{itemize}
									\item If \( \varphi\big( \vec{p}(\mu_e) \big) < \varphi\big( \vec{p}(\mu_r) \big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_e) \); go to (g). \\
											"\dots and the expanded point is better than the reflected point, use the expanded point."
									\item If \( \varphi\big( \vec{p}(\mu_r) \big) < \varphi\big( \vec{p}(\mu_e) \big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_r) \); go to (g). \\
											"\dots and the expanded point is worst than the reflected point, use the reflected point."
								\end{itemize}
							\item \emph{Outer Contraction:} If \( \varphi\big(\vec{p}^{(n_p)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(n_p + 1)}\big) \), then: \\
									"If the reflected point is better than the worst, but worst than second worst, \dots"
								\begin{itemize}
									\item If \( \varphi\big(\vec{p}(\mu_{oc})\big) < \varphi\big(\vec{p}(\mu_r)\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_{oc}) \); go to (g). \\
											"\dots and the outer contraction point is better than the reflected point, use the outer contraction point."
									\item Else, go to (f).
								\end{itemize}
							\item \emph{Inner Contraction:} If \( \varphi\big(\vec{p}^{(n_p + 1)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) \), then: \\
									"If the reflected point is worst than the worst point, \dots"
								\begin{itemize}
									\item If \( \varphi\big(\vec{p}(\mu_{ic})\big) < \varphi\big(\vec{p}^{(n_p + 1)}\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_{ic}) \); go to (g). \\
											"\dots and the inner contraction point is better than the worst, use the inner contraction point."
									\item Else, go to (f).
								\end{itemize}
							\item \emph{Shrink:} For all \( 2 \leq i \leq n_p + 1 \), set \( \vec{p}^{(i)} = \vec{p}^{(1)} - \frac{1}{2} \big( \vec{p}^{(i)} - \vec{p}^{(1)} \big) \).
							\item \emph{Sort:} Sorting the current simplex corners such that \( \varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p^{(2)}}\big) \leq \cdots \leq \varphi\big(\vec{p}^{(n_p + 1)}\big) \) holds again. Repeat.
						\end{enumerate}
				\end{itemize}
			
				The Nelder-Mead method therefore always tries to create a big simplex and only shrink if every other action would yield a worst corner/simplex.
			% end
			
			\subsubsection{Notes}
				\begin{itemize}
					\item The method is not guaranteed to converge. But in practice, it yields good results.
					\item Can get stuck on a suboptimal point such that the algorithm has to be restarted with other initial simplex corners.
				\end{itemize}
			
				\paragraph{Simplex-Gradient}
					It is possible to detect stagnation using a \emph{Simplex-Gradient} \( \vec{D}^{(k)} \in \R^{n_p} \), \( \vec{D}^{(k)} = \big( \mat{V}^{(k)} \big)^{-T} \vec{\delta}^{(k)} \) where \( \mat{V}^{(k)} \) is the matrix of the simplex directions
					\begin{align*}
						\mat{V}^{(k)} =
							\begin{bmatrix}
								\vec{p}^{(2)} - \vec{p}^{(1)} & \vec{p}^{(3)} - \vec{p}^{(1)} & \cdots & \vec{p}^{(n_p + 1)} - \vec{p}^{(1)}
							\end{bmatrix}
						\eqqcolon
							\begin{bmatrix}
								\vec{v}^{(1)} & \vec{v}^{(2)} & \cdots & \vec{v}^{(n_p)}
							\end{bmatrix}
						\in \R^{n_p \times n_p}
					\end{align*}
					and \( \vec{\delta}^{(k)} \) is the vector of the objective differences:
					\begin{align*}
						\vec{\delta}^{(k)} =
							\begin{bmatrix}
								\varphi\big(\vec{p}^{(2)}\big) - \varphi\big(\vec{p}^{(1)}\big) \\
								\varphi\big(\vec{p}^{(3)}\big) - \varphi\big(\vec{p}^{(1)}\big) \\
								\vdots \\
								\varphi\big(\vec{p}^{(n_p + 1)}\big) - \varphi\big(\vec{p}^{(1)}\big)
							\end{bmatrix}
						\in \R^{n_p}
					\end{align*}
					Analogous to a gradient-based method, this yields a condition for \emph{minimum progress}
					\begin{align*}
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} < -\alpha \big\lVert \vec{D}^{(k)} \big\rVert^2,\qquad
						\hat{\varphi} = \frac{1}{n_p + 1} \sum_{i = 1}^{n_p + 1} \varphi\big(\vec{p}^{(i)}\big)
					\end{align*}
					with a small \( \alpha > 0 \). One approach for a condition on when to restart is to restart if both
					\begin{align*}
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} > -\alpha \big\lVert \vec{D}^{(k)} \big\rVert^2
						\quad\text{and}\quad
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} < 0
					\end{align*}
					hold.
				% end
			% end
		% end

		\subsection{Multidirectional Search Methods}
			\begin{itemize}
				\item In the Nelder-Mead method a bad conditioning of the simplex, i.e. the matrix \( \mat{V}^{(k)} \), leads to problems that cannot be avoided.
				\item In multidirectional search methods this problem is avoided by making every simplex congruent to its predecessors.
				\item The algorithm uses similar steps for reflection, expansion and contraction, but possibly needs a lot more function evaluations.
			\end{itemize}
		% end

		\subsection{Asynchronous Parallel Pattern Search (APPS)}
			\begin{itemize}
				\item \emph{Asynchronous Parallel Pattern Search} is a pattern-based search method on a grid.
				\item The direction of the pattern determines the descent direction.
				\item Patterns can be varied while maintaining their mathematical properties.
				\item It is "naturally" parallelizable.
			\end{itemize}
		% end

		\subsection{Implicit Filtering}
			\emph{Implicit filtering} is a descent method using "smooth" approximations of the gradients. It uses a central approximation of the simplex gradient
			\begin{align*}
				\vec{D}_C^{(k)} = \frac{1}{2} \big( \vec{D}^{(k)} + \vec{D}_R^{(k)} \big)
			\end{align*}
			where \( \vec{D}_R^{(k)} \) is the gradient of the simplex that is reflected around \( \vec{p}^{(k)} \).
			
			The structure of implicit filtering is sketched in\autoref{alg:implicitFiltering}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose \( \alpha, \beta \in (0, 1) \) and set \( \mat{H} = \mat{I} \) \;
				\While{not converged}{
					Calculate \( \varphi\big(\vec{p}^{(k)}\big) \), \( \vec{D}_C^{(k)} \) and the search direction \( \vec{d}^{(k)} = -\mat{H}^{-1} \vec{D}_C^{(k)} \) \;
					Inexact line search for \( j = 1, \cdots, j_\mathrm{max} \), \( \lambda \coloneqq \beta^j \) until the following holds:
						\begin{align*}
							\varphi(\vec{p} + \lambda \vec{d}) - \varphi(\vec{p}) \leq \alpha \lambda \grad_{\Delta \vec{p}_k} \varphi^T(\vec{p}) \vec{d}^{(k)}
						\end{align*} \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \lambda \vec{d}^{(k)} \) \;
					\If{line search successful}{
						Quasi-Newton update of the Hessian \(\mat{H}\) with \( \vec{p}^{(k + 1)} - \vec{p}^{(k)} \) and \( \vec{D}_C^{(k + 1)} - \vec{D}_C^{(k)} \) \;
					} \Else{
						\( \mat{H} \gets \mat{I} \) \;
						Shrink the simplex \;
					}
				}
				
				\caption{Implicit Filtering.}
				\label{alg:implicitFiltering}
			\end{algorithm}
		% end
	% end

	\section{Surrogate Optimization}
		In \emph{surrogate optimization methods}, the (complex) objective is replaced with a simpler approximation that maintains the key properties of the objective (e.g. a noisy measurement might be replaced by a simpler regression model). This surrogate function is then minimized and adjusted in order to find a good approximation of the solution of the original problem (this can also be applied for constraint functions).
		
		Requirements for the surrogate function \( \hat{\varphi} : \R^{n_p} \to \R \): For all function evaluation (or "sampling") points \( \Big( \vec{p}^{(i)}, \varphi\big(\vec{p}^{(i)}\big) \Big) \), \( i = 1, \cdots, m \) it must hold that
		\begin{align*}
			\varphi\big( \vec{p}^{(i)} \big) = \hat{\varphi}\big( \vec{p}^{(i)} \big) + \epsilon
		\end{align*}
		where \( \epsilon \in \R \) is some "slack" constant that determines how exact the surrogate function shall be.
		\begin{itemize}
			\item For \(\epsilon = 0\), the problem is the same as interpolation.
			\item For \(\epsilon > 0\), the surrogate function does not perfectly reproduce the objective, but might be smoother.
		\end{itemize}
		Further requirements are that \( \hat{\varphi} \) should be fast to compute and the gradients of \(\hat{\varphi}\) should be available in closed form.
		
		This raises some questions:
		\begin{enumerate}
			\item \(\varphi\) might be too complex for a simple approximation \(\implies\) which approximation method should be used?
			\item How to generate the data basis of the function evaluations? Generating all in one point will not yields good results\dots
			\item Which method is feasible to minimize \(\hat{\varphi}\)?
		\end{enumerate}

		\subsection{Approximation Methods}
			\subsubsection{Response Surface Methods (RSMs)}
				\emph{Response surface methods} use simple polynomials of a low degree as the model function \(\hat{\varphi}\), e.g.:
				\begin{itemize}
					\item Degree one (linear): \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \beta_1^T \vec{p} \)
					\item Degree one with mixed terms: \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1^T \vec{p} + \sum_{i} \sum_{\substack{j \\ j \neq i}} \beta_2^{i, j} p_i p_j \)
					\item Degree two (quadratic): \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1 \vec{p} + \vec{p}^T \mat{\beta}_2 \vec{p} \)
					\item Higher degree: \dots
				\end{itemize}
				The unknown parameters \( \beta_1 \in \R \), \( \vec{\beta}_2 \in \R^{n_p} \) and \( \mat{\beta}_2 \in \R^{n_p \times n_p} \) can be approximated using least squares:
				\begin{align*}
					\min_{\beta_1, \vec{\beta}_2, \mat{\beta}_2} \sum_i \Big( \varphi\big(\vec{p}^{(i)}\big) - \hat{\varphi}\big(\vec{p}^{(i)}\big) \Big)
				\end{align*}
				
				\begin{itemize}
					\item Advantage: Simple and the approximations are easy to compute.
					\item Disadvantage: The RSMs cover only the global behavior and not local accuracy.
				\end{itemize}
			% end

			\subsubsection{Radial Basis Functions (RBFs)}
				Now the surrogate function \( \hat{\varphi} \) uses a linear combination of \emph{radial basis functions}:
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \sum_{i = 1}^{m} \gamma_i h\Big( \big\lVert \vec{p} - \vec{p}^{(i)} \big\rVert \Big)
				\end{align*}
				with basis function \( h(\cdot) \) based only on the euclidean distance from the interpolation point, e.g.
				\begin{itemize}
					\item Linear: \tabto{2cm} \( h(r_i) = r_i \)
					\item Cubic: \tabto{2cm} \( h(r_i) = r_i^3 \)
					\item Thin-Plate: \tabto{2cm} \( h(r_i) = r_i^2 \log r \)
				\end{itemize}
				where \( r_i = \big\lVert \vec{p} - \vec{p}^{(i)} \big\rVert \).
				
				A suitable combination of RSM and RBF yield cubic spline-approximation:
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1 \vec{p} + \sum_i \gamma_i h(\vec{p}),\quad \beta_1 \in \R, \beta_2 \in \R^{n_p}, \gamma_i \in \R
				\end{align*}
				\begin{itemize}
					\item Univariate: \tabto{2cm} \( h(\vec{p}) = \frac{1}{12} \sum_i r_i^3 \)
					\item Bivariate: \tabto{2cm} \( h(\vec{p}) = \frac{1}{16 \pi} \sum_i r_i^2 \log r_i \)
				\end{itemize}
			% end

			\subsubsection{Design and Analysis of Computer Experiments (DACE)}
				Assuming the model function is a realization of a stochastic process
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \vec{v}^T(\vec{p}) \vec{\beta} + Z(\vec{p})
				\end{align*}
				where \( \vec{v}(\vec{p}) \) is a vector of basis functions (e.g. RSM, RBF) and \( Z(\vec{p}) \) is a stationary random variable that is Gaussian distributed with zero mean. The covariance between two points \( \vec{p}^{(l)} \) and \( \vec{p}^{(k)} \) is given as
				\begin{align*}
					\Cov\Big[ Z\big(\vec{p}^{(l)}\big), Z\big(\vec{p}^{(k)}\big) \Big] = \sigma^2 R\big(\vec{p}^{(l)}, \vec{p}^{(k)}\big)
					\quad\text{with}\quad
					R\big(\vec{p}^{(l)}, \vec{p}^{(k)}\big) = \prod_{i = 1}^{n_p} e^{-\theta_i d_i^2},\quad d_i = \big\lVert \vec{p}_i^{(l)} - \vec{p}_i^{(k)} \big\rVert_2
				\end{align*}
				Die unknown parameters \(\vec{\beta}\), \(\vec{\theta}\) and \(\sigma^2\) are then estimated using statistical estimators, e.g. maximum likelihood.
			% end
		% end

		\subsection{Select of the Sampling Points}
			\subsubsection{Design of Experiments (DoE)}
				The classical strategy for selection sampling points, \emph{design of experiments} is mainly designed for physical experiments, not for deterministic ones! Typical approach:
				\begin{itemize}
					\item Classical selection
						\begin{itemize}
							\item orthogonal arrays
							\item latin hypercubes
							\item combinations
						\end{itemize}
					\item Metric-bases methods
						\begin{itemize}
							\item MiniMax: minimizing the maximal distance between the sampling points
							\item MaxiMin: maximizing the minimal distance
						\end{itemize}
					\item Stochastic selection for Gaussian processes
						\begin{itemize}
							\item Entropy Design or D-Opt: maximizing the determinant of the covariance matrix
							\item A-Opt: depends on the trace of the covariance matrix
							\item G-Opt: minimize the maximum mean squared error
						\end{itemize}
				\end{itemize}
			% end
		% end

		\subsection{Minimizing the Surrogate Function}
			To successfully minimize the original objective function, the data basis of the surrogate function has to be expanded sequentially. The following sections describe two methods for this, the Strawman and the Shoemaker method.

			\subsubsection{Strawman}
				\begin{enumerate}
					\item Calculate the current minimum of the surrogate function (e.g. using gradient descent).
					\item Add the minimum of the surrogate function as a sampling point.
					\item Calculate the new surrogate function. Repeat.
				\end{enumerate}
			% end

			\subsubsection{Shoemaker}
				\begin{enumerate}
					\item Calculate the minimum with a minimal distance to all sampling points.
					\item Extend the sampling points by this point.
					\item Determine the new surrogate function. Repeat.
				\end{enumerate}
			% end

			\subsubsection{DACE-Based, Sequential Update Strategy}
				\begin{itemize}
					\item The the expected mean error as a criteria for the quality of the surrogate function.
					\item Weigh small function values and uncertainties in the approximations.
					\item There exist different strategies following this basic idea.
					\item Termination criteria:
						\begin{itemize}
							\item Number of function evaluations
							\item No more improvements in the objective function
							\item \(\varphi(\vec{p})\) close to the minimal possible function value
						\end{itemize}
					\item Sequential methods are better on normal computers, for parallel computers a special scheme should be used.
				\end{itemize}
			% end
		% end

		\subsection{Discussion}
			\begin{itemize}
				\item Independent of the approximation method, the surrogate function has to be minimized one or more times per iteration (depending on the actual method).
				\item But the effort for the minimization is negligible as one simulation run for the evaluation of \(\varphi\) often takes a lot longer.
				\item Every method of~\autoref{c:gradientOptimization} can be used for minimizing \(\hat{\varphi}\) as the gradients are available by design.
				\item Advantages:
					\begin{itemize}
						\item \(\hat{\varphi}\) is given in closed form as well as the gradientd.
						\item Easy to compute, Newton-type methods applicable!
						\item "Smooth" surrogate function.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Approximation accuracy is limited.
						\item The number of sample points rises a lot for high-dimensional problems (curse of dimensionality).
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Comparison}
		\subsection{Magnetic Bearing Design} % 3.56, 3.57, 3.58, 3.59, 3.60, 3.61
			\todo{Content}
		% end

		\subsection{Walking Optimization of a Humanoid Robot} % 3.62, 3.63
			\todo{Content}
		% end
	% end

	\section{Discussion}
		\begin{itemize}
			\item Advantages:
				\begin{itemize}
					\item Application is easy, no or little prior knowledge required.
					\item Robust toward discontinuities of \(\varphi\) or \(\grad{\varphi}\).
					\item No calculation of gradients necessary.
					\item No need to start "close to" a solution.
					\item Some methods (e.g. evolutionary algorithms) are highly parallelizable, some are not (e.g. Nelder-Mead).
				\end{itemize}
			\item Disadvantages:
				\begin{itemize}
					\item Slow convergence, high number of steps needed and high computation time due to many \(\varphi\)-evaluations.
					\item Inefficient for large \( n_p \).
					\item Major difficulties for nonlinear constraints in \(\vec{p}\).
				\end{itemize}
		\end{itemize}
	% end
% end

\chapter{Gradient-Based Optimization with Constraints}
	This chapter covers the optimization problems with nonlinear equality- and inequality-constraints:
	\begin{align*}
		\min_{\vec{p} \in \R^{n_p}} &\, \varphi(\vec{p}) \\
		\mathrm{subject~to}\quad &\,
			\begin{alignedat}[t]{3}
				\vec{a}(\vec{p}) &= \vec{0},\quad & \vec{a} : \R^{n_p} \to \R^{n_a} \\
				\vec{b}(\vec{p}) &\geq \vec{0},\quad & \vec{b} : \R^{n_p} \to \R^{n_b}
			\end{alignedat}
	\end{align*}
	Such an optimization problem is called a \emph{nonlinear programming problem} (NLP).
	
	\begin{itemize}
		\item A point \( \vec{p} \in \R^{n_p} \) that fulfills the constraints is called a \emph{feasible point}.
		\item The set of all feasibly points is called the \emph{feasible region}.
		\item For a local minimum \( \vec{p}^\ast \) it holds that \( \varphi(\vec{p}^\ast) \leq \varphi(\vec{p}) \) for all feasible points \(\vec{p}\) in a neighborhood of \(\vec{p}^\ast\).
		\item All constraints that are \emph{active} at a point \(\vec{p}\) make up the \emph{active set}. This set includes all equality constraints and the active inequality constraints \( A(\vec{p}) \coloneqq \{\, j \in \N : 1 \leq j \leq n_b,\, b_j(\vec{p}) = 0 \,\} \).
	\end{itemize}

	\section{Solution Characterization}
		By a geometric view it is clear that the gradient of the active constraints must be parallel to the gradient of the objective function, i.e. there has to be a constant \( \mu_i^\ast \) for each active constraint \( a_i \) such that
		\begin{align*}
			\grad{\varphi}(\vec{p}^\ast) = \mu_i^\ast \grad{a_i}(\vec{p}^\ast)
			\quad\iff\quad
			\grad{\varphi}(\vec{p}^\ast) - \mu_i^\ast \grad{a_i}(\vec{p}^\ast) = \vec{0}
		\end{align*}
		
		This leads directly leads to the definition of the Lagrangian
		\begin{align*}
			L(\vec{p}, \vec{\mu}, \vec{\sigma}) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p}) - \vec{\sigma}^T \vec{b}(\vec{p})
		\end{align*}
		with the Lagrange multiplier \( \vec{\mu} \), \( \vec{\sigma} \) which encodes the insight above.
		
		For formulating the \emph{first-order optimality conditions} analogous to the ones for unconstrained optimization (see~\autoref{subsec:multiDimensionalOptimalityConditions}), a \emph{constraint qualification} must hold: The gradients
		\begin{align*}
			\grad{a_1}(\vec{p}^\ast), \cdots, \grad{a_{n_p}}(\vec{p}^\ast) \quad\text{and}\quad \grad{b_j}(\vec{p}^\ast), j \in A(\vec{p}^\ast)
		\end{align*}
		of the active constraints have to linearly independent.

		\subsection{First-Order Necessary Optimality Conditions (Karush-Kuhn-Tucker Conditions, KKT)}
			Let \( \varphi : \R^{n_p} \to \R \), \( \vec{a} : \R^{n_p} \to \R^{n_a} \) and \( \vec{b} : \R^{n_p} \to \R^{n_b} \) be continuously differentiable and let the constraint qualification be fulfilled. If \( \vec{p} \) is a feasible local minimum of the NLP, then there exist Lagrange multiplier \( \vec{\mu} \in \R^{n_a} \), \( \vec{\sigma} \in \R^{n_b} \) such that the \emph{Karush-Kuhn-Tucker} conditions hold:
			\begin{align}
				\grad_{\vec{p}}L(\vec{p}, \vec{\mu}, \vec{\sigma}) & = \vec{0}    \tag{KKT.i}   \\
				\vec{\mu}^T \vec{a}(\vec{p})                       & = \vec{0}    \tag{KKT.iia} \\
				\vec{\sigma}^T \vec{b}(\vec{p})                    & = \vec{0}    \tag{KKT.iib} \\
				\vec{\sigma}                                       & \geq \vec{0} \tag{KKT.iii} \label{eq:kkt-iii}
			\end{align}
			Here, \( L(\vec{p}, \vec{\mu}, \vec{\sigma}) \) is the Lagrangian
			\begin{align*}
				L(\vec{p}, \vec{\mu}, \vec{\sigma}) \coloneqq \varphi(\vec{p}) + \vec{\mu}^T \vec{a}(\vec{p}) + \vec{\sigma}^T \vec{b}(\vec{p})
			\end{align*}
			and the inequality in~\eqref{eq:kkt-iii} is element-wise.
		% end

		\subsection{Second-Order Necessary Optimality Conditions}
			The second-order necessary optimality condition is fulfilled if the first-order condition is fulfilled and the Hessian of the Lagrangian has a positive curvature \emph{along the feasible directions}:
			\begin{gather*}
				%\forall \vec{z} \in \R^{n_p} \setminus \{\vec{0}\},\, \vec{z}^T \cdot \grad{a_i}(\vec{p}) = \vec{0},\, \vec{z}^T \cdot \grad{b_j}(\vec{p}) : 
				\vec{z}^T \mat{H}_L^{\vec{p}}(\vec{p}, \vec{\mu}, \vec{\sigma}) \vec{z} \geq 0 \\
				\quad\text{for all }
				\vec{z} \in \R^{n_p} \setminus \{\vec{0}\}
				\text{ with }
				\big( \vec{z}^T \cdot \grad{a_i}(\vec{p}) = \vec{0} \big)_{i = 1, \cdots, n_a},
				\text{ and }
				\big( \vec{z}^T \cdot \grad{b_j}(\vec{p}) = \vec{0} \big)_{j \in A(\vec{p})}
			\end{gather*}
		% end

		\subsection{Example} % 4.11, 4.13
			\todo{Content}
		% end
	% end

	\section{Simple Bounds, Box Constraints}
		The most common type of inequality-constraints are simple lower and upper bounds on the optimization variables:
		\begin{align*}
			\min_{\vec{p} \in \R^{n_p}} &\, \varphi(\vec{p}) \\
			\mathrm{subject~to}\quad &\,
				\vec{p}_{i, \mathrm{min}} \leq \vec{p}_i \leq \vec{p}_{i, \mathrm{max}}
		\end{align*}
		In practice, these are not only used in nearly all constrained, but also in unconstrained optimization problems as they might increase the efficiency by constraining the search space. They also increase robustness of optimization techniques that ensure the fulfilling of the constrains in every iteration.
		
		Box constraints can also be used to prohibit "insecure values", e.g. negative variables when the objective function takes square roots.
		
		For gradient-free methods, the constraints can be ensured by clipping the new iteration value to the bounds:
		\begin{align*}
			p_i^{(k + 1)} = \max\big\{\, p_{i, \mathrm{min}},\, \min\big\{\, p_{i, \mathrm{max}},\, p_i^{(k + 1)} \,\big\} \big\}
		\end{align*}
		For gradient-based methods, box constraints can be considered like every other linear and nonlinear constraint (more on that later).
	% end

	\section{Penalty Function}
		The approach of \emph{penalty functions} is to transform the constrained problem to an unconstrained problem by punishing violations of the constraints. As this arises big problems with lots of and highly nonlinear constraints, penalty functions are rarely used in practice anymore in favor of sequential quadratic programming (see~\autoref{sec:sqp}).
		
		As of today, penalty functions are mainly used for step size determination for nonlinear programs and in the application of robust, gradient-free methods in the unconstrained optimization for solving NLPs.

		\subsection{Exterior Penalty Functions}
			The original nonlinear problem is replaced by an unconstrained problem with a penalty function \(\Phi\)
			\begin{align*}
				\min_{\vec{p} \in \R^{n_p}} \Phi(\vec{p}, \rho),\quad \Phi(\vec{p}, \rho) = \varphi(\vec{p}) + \rho \sum_j \pi_j(\vec{p}),\quad \rho \in \R^+
			\end{align*}
			with a function \( \pi_j \) per constraint that is positive if the constraint is violated and zero otherwise.
			
			Often a series of unconstrained optimization problems \( \Phi(\vec{p}, \rho) \) is solved with an increasing \(\rho\), such that the solution \( \vec{p}^\ast(\rho) \) gets pushed into the feasible region of the NLP step by step in in the hope of
			\begin{align*}
				\liminfty[\rho] \vec{p}^\ast(\rho) = \vec{p}^\ast
			\end{align*}
			where \( \vec{p}^\ast \) is the real minimum.
			
			This approach is called \emph{exterior penalty functions} as the penalty term is only relevant if \( \vec{p} \) violates the according constraints.
			
			\subsubsection{Quadratic Penalty Function}
				The \emph{quadratic penalty function} is the most common exterior penalty function:
				\begin{align*}
					\Phi_Q(\vec{p}, \rho) = \varphi(\vec{p}) + \rho \cdot \frac{1}{2} \Bigg( \sum_{k = 1}^{n_a} \big(a_k(\vec{p})\big)^2 + \sum_{jk = 1}^{n_b} \big(\hat{b}_k(\vec{p})\big)^2 \Bigg)
				\end{align*}
				Here, \( \hat{b}_j(\vec{p}) \) denotes the violation of the inequality constraints:
				\begin{align*}
					\hat{b}_j(\vec{p}) =
						\begin{cases*}
							0             & iff \( b_j(\vec{p}) \geq 0 \) \\
							-b_j(\vec{p}) & iff \( b_j(\vec{p}) < 0 \)
						\end{cases*}
					= \big\lvert\! \min\{ 0,\, b_j(\vec{p}) \} \big\rvert
				\end{align*}
				
				But in the case of \( \rho \to \infty \), the Hessian of \( \Phi_C \) gets more and more ill-condition and may even be singular in the limit.
			% end

			\paragraph{Example} % 4.22, 4.23
				\todo{Content}
			% end
		% end

		\subsection{Interior Penalty Functions}
			When using exterior penalty functions, it is not guaranteed that every solution of the iterating unconstrained problems fulfills the constraints of the original NLP. Therefore, exterior penalty functions are not applicable if fulfilling the constraints in every iteration is required. \emph{Interior penalty functions} guarantee exactly that: Every subproblem yields a feasible solution.
			
			Given a NLP with only inequality constraints, interior penalty functions use \emph{barrier functions} that have the following properties:
			\begin{itemize}
				\item Value of infinity everywhere except inside the feasible region.
				\item Continuously differentiable inside the feasible region.
				\item The values go to \( +\infty \) as \(\vec{p}\) gets closer to the edge of the feasible region.
			\end{itemize}
		
			\subsubsection{Logarithmic Barrier Function}
				The \emph{logarithmic barrier function} is the most used interior penalty function:
				\begin{align*}
					\Phi_B(\vec{p}, r) = \varphi(\vec{p}) - r \sum_{k = 1}^{n_b} \ln b_i(\vec{p})
				\end{align*}
				The barrier parameter \( r > 0 \) will be decreased step by step and the solution should converge to the minimum as \( r \to 0 \).
				
				But in the case of \( r \to 0 \), the Hessian of \( \Phi_B \) gets more and more ill-condition and may even be singular in the limit.
			% end

			\paragraph{Example} % 4.26
				\todo{Content}
			% end
		% end

		\subsection{Exact Penalty Functions}
			Neither the quadratic penalty function nor the logarithmic barrier function are "exact" penalty function so that, with an appropriate \(\rho\) or respectively \(r\), the solution of the unconstrained NLP yields the exact solution.
			
			One important penalty function of this class is the \emph{exact \(\ell_1\)-penalty function}:
			\begin{align*}
				\Phi_{\ell_1}(\vec{p}, \rho) = \varphi(\vec{p}) + \rho \sum_{k = 1}^{n_a} \big\lvert a_k(\vec{p}) \big\rvert + \rho \sum_{k = 1}^{n_b} \big\lvert\! \min\{ 0,\, b_j(\vec{p}) \} \big\rvert
			\end{align*}
			However, the \(\ell_1\)-penalty function is not differentiable! This make the numerical solution difficult. But the minimization yields, for adequate big \(\rho\), the minimum of the NLP!

			\paragraph{Example 1} % 4.28
				\todo{Content}
			% end

			\paragraph{Example 2} % 4.29
				\todo{Content}
			% end
		% end

		\subsection{Augmented Lagrangian}
			The downsides of exterior, interior and the \(\ell_1\)-penalty function can be avoided by not using the objective directly, but by using the Lagrangian:
			\begin{align*}
				L_Q(\vec{p}, \vec{\mu}, \vec{\sigma}, \rho) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p}) - \vec{\sigma}^T \vec{b}(\vec{p}) + \rho \cdot \frac{1}{2} \Bigg( \sum_{k = 1}^{n_a} \big(a_k(\vec{p})\big)^2 + \sum_{jk = 1}^{n_b} \big( \min\{ 0,\, b_j(\vec{p}) \} \big)^2 \Bigg)
			\end{align*}
			But this requires a good approximation of the Lagrange multiplier \( \vec{\mu} \) and \( \vec{\sigma} \).

			\paragraph{Example 1} % 4.31, 4.32
				\todo{Content}
			% end

			\paragraph{Example 2} % 4.33
				\todo{Content}
			% end

			\subsubsection{Notes}
				\begin{itemize}
					\item In practice, the augmented Lagrangian \( L_Q \) is used in the following fashion:
						\begin{enumerate}
							\item Choose Lagrange multipliers \( \vec{\mu} \), \( \vec{\sigma} \) and the parameter \( \rho > 0 \).
							\item Calculate a local minimum \( \vec{p}^\ast(\rho) \) of \( L_Q \) using methods of the unconstrained optimization.
							\item Update the Lagrange multipliers and \(\rho\). Repeat with 2.
						\end{enumerate}
					\item The update of the Lagrange multipliers according to \( \vec{\mu}(\rho) = -\rho \vec{a}\big(\vec{p}^\ast(\rho)\big) \) is based on the convergence properties of the quadratic penalty function.
					\item If
						\begin{itemize}
							\item \(\vec{p}^\ast\) is a minimum of the NLP,
							\item the constraint qualification hold and
							\item the KKT-conditions and the second-order sufficient optimality condition is fulfilled for \( \vec{\mu}^\ast \), \( \vec{\sigma}^\ast \),
						\end{itemize}
						then
						\begin{itemize}
							\item exists a threshold \(\hat{\rho}\) for which it holds that for all \( \rho \geq \hat{\rho} \) it holds that
							\item \(\vec{p}^\ast\) is a strict local minimum of the (quadratic) augmented Lagrangian \( L_Q(\vec{p}, \vec{\mu}^\ast, \vec{\sigma}^\ast, \rho) \).
						\end{itemize}
				\end{itemize}
			% end
		% end
	% end

	\section{Constraint Elimination}
		For NLPs with active constraints it is often tempting to transform the NLP to remove some of the constraints and to reduce the number of optimization variables (the degrees of freedom). But this might yields wrong results! It has to be assured that:
		\begin{itemize}
			\item The original minimum is not eliminated.
			\item The nonlinearity of the problem is not increased too much (causing problems in finite difference approximations of derivatives).
			\item No singularities arise in the transformed objective.
			\item No new discontinuities and non-differentiabilities are added to the objective.
			\item The Hessian of the surrogate problem is not singular or ill-conditioned near the minimum.
			\item The transformed problem does not have additional local minima or stationary points.
			\item And a lot more.
		\end{itemize}
		In general: Keep more constraints and keep the function as linear as possible instead of applying transformations that behave badly.

		\paragraph{Example 1} % 4.38
			\todo{Content}
		% end

		\paragraph{Example 2} % 4.39
			\todo{Content}
		% end

		\paragraph{Example 3} % 4.40
			\todo{Content}
		% end
	% end

	\section{Sequential Quadratic Programming (SQP)}
		\label{sec:sqp}
	
		To take care of highly nonlinear equality and inequality constraints, information about the trajectory of these have to be considered, i.e. gradient and Hessian information. Assuming that the constraints that are active on the solution are known, the optimization problem is given as:
		\begin{align*}
			\min_{\vec{p} \in \R^{n_p}} &\, \varphi(\vec{p}) \\
			\mathrm{subject~to}\quad &\, \vec{a}(\vec{p}) = \vec{0},\quad \vec{a} : \R^{n_p} \to \R^{n_a}
		\end{align*}
		The KKT-conditions are then equivalent to the simpler formulation
		\begin{align*}
			\grad{L}(\vec{p}, \vec{\mu}) \coloneqq
				\begin{bmatrix}
					\grad_{\vec{p}}L(\vec{p}, \vec{\mu}) \\
					\grad_{\vec{\mu}}L(\vec{p}, \vec{\mu})
				\end{bmatrix}
			=
				\begin{bmatrix}
					\grad{\varphi}(\vec{p}) - \sum_{k = 1}^{n_a} \mu_k \cdot \grad{a_k}(\vec{p}) \\
					-\vec{a}(\vec{p})
				\end{bmatrix}
			= \vec{0}
		\end{align*}
		with the Lagrangian
		\begin{align*}
			L(\vec{p}, \vec{\mu}) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p})
		\end{align*}
		This yields a system of \( n_p + n_a \) nonlinear equations for \( n_p + n_a \) unknowns \( \vec{p} \), \( \vec{\mu} \).
		
		Taylor-expanding the gradient of the Lagrangian around \( \big( \vec{p}^{(k)}, \vec{\mu}^{(k)} \big) \) yields
		\begin{align*}
			\grad{L}(\vec{p}^\ast, \vec{\mu}^\ast) \overset{T\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big)}{=} \grad{L}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) + \mat{H}_L\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \begin{bmatrix} \vec{d}_p^{(k)} \\ \vec{d}_\mu^{(k)} \end{bmatrix} + \cdots \overset{!}{=} 0
		\end{align*}
		with \( \vec{d}_p^{(k)} \coloneqq \vec{p}^\ast - \vec{p}^{(k)} \) and \( \vec{d}_\mu^{(k)} \coloneqq \vec{\mu}^\ast - \vec{\mu}^{(k)} \). Cutting off the higher-order terms yields the \emph{Lagrange-Newton method} where the search direction \( \vec{d}_p^{(k)} \), \( \vec{d}_\mu^{(k)} \) is given as the solution of
		\begin{align}
			\mat{H}_L\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \begin{bmatrix} \vec{d}_p^{(k)} \\ \vec{d}_\mu^{(k)} \end{bmatrix} = -\grad{L}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \label{eq:sqpEqSystem}
		\end{align}
		The Iteration equation \( k \to k + 1 \) is analogous to the Newton method:
		\begin{align*}
			\begin{bmatrix}
				\vec{p}^{(k + 1)} \\
				\vec{\mu}^{(k + 1)}
			\end{bmatrix}
			=
			\begin{bmatrix}
				\vec{p}^{(k)} \\
				\vec{\mu}^{(k)}
			\end{bmatrix}
			+
			\begin{bmatrix}
				\vec{d}_p^{(k)} \\
				\vec{d}_\mu^{(k)}
			\end{bmatrix}
		\end{align*}
		But there is one catch: The set of active constraints at the minimum is generally not known and can change in every iteration.

		\subsection{Finding the Search Direction}
			Further analysis of the linear system~\eqref{eq:sqpEqSystem} with the Lagrangian
			\begin{align*}
				L(\vec{p}, \vec{\mu}) = \varphi(\vec{p}) - \vec{\mu}^T \vec{a}(\vec{p})
			\end{align*}
			yields that the linear system has the following structure:
			\begin{align*}
				\begin{bmatrix}
					\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) & -\mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \\
					-\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) & \mat{0}
				\end{bmatrix}
				\begin{bmatrix}
					\vec{d}_p^{(k)} \\
					\vec{d}_\mu^{(k)}
				\end{bmatrix}
				=
				\begin{bmatrix}
					-\grad{\varphi}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \vec{\mu}^{(k)} \\
					\vec{a}\big(\vec{p}^{(k)}\big)
				\end{bmatrix}
			\end{align*}
			Where \( \mat{H}_L^p\big(\vec{p}^{(k)}\big) \) is the Hessian of the Lagrangian w.r.t. \(\vec{p}\) and \( \mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \) is the Jacobian
			\begin{align*}
				\mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) =
					\begin{bmatrix}
						\pdv{a_1}{p_1}     & \cdots & \pdv{a_{n_a}}{p_1}     \\
						\vdots             & \ddots & \vdots                 \\
						\pdv{a_1}{p_{n_p}} & \cdots & \pdv{a_{n_a}}{p_{n_p}}
					\end{bmatrix}
			\end{align*}
			\textbf{Note that this definition of the Jacobian differs from the usual definition! This one has the gradients of the function has columns!}
			
			This linear system can be transformed to
			\begin{align*}
				\begin{bmatrix}
					\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) & -\mat{J}_{\vec{a}}\big(\vec{p}^{(k)}\big) \\
					-\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) & \mat{0}
				\end{bmatrix}
				\begin{bmatrix}
					\vec{d}_p^{(k)} \\
					\underbrace{\vec{d}_\mu^{(k)} + \vec{\mu}^{(k)}}_{\vec{\mu}^{(k + 1)}}
				\end{bmatrix}
				=
				\begin{bmatrix}
					-\grad{\varphi}\big(\vec{p}^{(k)}\big) \\
					\vec{a}\big(\vec{p}^{(k)}\big)
				\end{bmatrix}
			\end{align*}
			where \( \vec{d}_p^{(k)} \) can be viewed as the solution of a quadratic minimization problem!

			\subsubsection{Quadratic Problem (QP)}
				The said quadratic problem is given as:
				\begin{align*}
					\min_{\vec{d}_p \in \R^{n_p}} &\, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) \vec{d}_p \\
					\mathrm{subject~to}\quad &\, \vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_a^T\big(\vec{p}^{(k)}\big) \vec{d}_p = \vec{0}
				\end{align*}
				Where the quadratic objective function of the QP consists of a quadratic Taylor-approximation of the NLP objective plus a weighted curvature condition via the Hessian of the activate conditions:
				\begin{align*}
					\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}\big) = \mat{H}_\varphi\big(\vec{p}^{(k)}\big) - \sum_{i = 1}^{n_a} \mu_i^T \mat{H}_{a_i}\big(\vec{p}^{(k)}\big)
				\end{align*}
				The linear constraints of the QP also consist of Taylor-approximations of the active NLP constraints.
				
				Solving this quadratic optimization problem is more robust and possibly faster than solving the linear system of equations. There exist special algorithms for solving QPs. But even though they are not active, the inequality constraints must be fulfilled. Therefore, the QP is extended to also determine the Lagrange multipliers \( \vec{\mu}^{(k)} \), \( \vec{\sigma}^{(k)} \) along with the search direction \( \vec{d}_p^{(k)} \):
				\begin{align}
					\min_{\vec{d}_p \in \R^{n_p}} &\, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) \vec{d}_p \\
					\mathrm{subject~to}\quad &\,
						\begin{alignedat}[t]{2}
							\vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p &= \vec{0} \\
							\vec{b}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{b}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p & \geq \vec{0}
						\end{alignedat}  \label{eq:qp}
				\end{align}
				Iteration step \( k \to k + 1 \): \( \vec{p}^{(k + 1)} = \vec{p}^{(k)} + \vec{d}_p^{(k)} \)
			% end
			
			\subsubsection{Notes}
				\begin{itemize}
					\item Similar to the Newton method, it can be shown under some assumptions that the SQP method converges to a local minimum.
					\item Algorithms for solving general QPs can be used to determine the active constraints in each iteration by solving the QP. \\ Even though it is not needed, it is useful for efficiency to use as much information as possible from the last iteration ("hot start").
					\item Other approaches determine the active constraints (working set) outside of the QP solution to only solve QPs with equality constraints which is especially efficient.
				\end{itemize}
			% end
		% end

		\subsection{Step Size Rules}
			If the initialization \(\vec{p}^{(0)}\) is "far" away from the minimum (or the QP is a bad, local approximation of the NLP), the convergence can be improved by determining the optimal step size for
			\begin{align*}
				\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \alpha^{(k)} \vec{d}_p^{(k)}
				\quad\text{or respectively}\quad
				\begin{bmatrix}
					\vec{p}^{(k + 1)} \\
					\vec{\mu}^{(k + 1)}
				\end{bmatrix}
				=
				\begin{bmatrix}
					\vec{p}^{(k)} \\
					\vec{\mu}^{(k)}
				\end{bmatrix}
				+
				\alpha^{(k)}
				\begin{bmatrix}
					\vec{d}_p^{(k)} \\
					\vec{\mu}_{QP}^{(k + 1)} - \vec{\mu}^{(k)}
				\end{bmatrix}
			\end{align*}
			Common methods for determining the step size are
			\begin{itemize}
				\item the (quadratic) augmented Lagrangian \( L_Q \) or
				\item the exact \(\ell_1\)-penalty function \( \Phi_{\ell_1} \)
			\end{itemize}
			with step size rules like the Armijo rule similar as in the unconstrained optimization (see~\autoref{sec:gradientUnconsStepSize}).
		% end

		\subsection{Approximation of the Lagrange Multipliers}
			If the approximation \( \vec{p}^{(k)} \) is far away from the NLP solution, it is not useful to use the Lagrange multipliers of the QP for the NLP (as the linearization is only valid locally). Another method is to approximate the Lagrange multipliers after calculating \( \vec{p}^{(k + 1)} \) (i.e. solving the QP) using minimum least squares:
			\begin{align*}
				\min_{\mu \in \R^{n_a}} &\,
						\bigg\lVert
							\grad{\varphi}\big(\vec{p}^{(k + 1)}\big)
							- \sum_{i = 1}^{n_a} \mu_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
						\bigg\rVert_2^2 \\
				\intertext{or equivalently model the active inequality constraints explicitly:}
				\min_{\mu \in \R^{n_a}} &\,
						\bigg\lVert
							\grad{\varphi}\big(\vec{p}^{(k + 1)}\big)
							- \sum_{i = 1}^{n_a} \mu_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
							- \sum_{i \in A\big(\vec{p}^{(k + 1)}\big)} \sigma_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
						\bigg\rVert_2^2
			\end{align*}
			Solving least squares problem will be discussed in detail in~\autoref{c:leastSquares}.
		% end

		\subsection{Termination Criteria}
			An obvious termination criteria would be to check whether the necessary KKT-conditions are sufficiently fulfilled, i.a. \( \grad{L}(\vec{p}, \vec{\mu}) \approx \vec{0} \).
			
			The commonly used SQP method \emph{NPSOL} terminates if all of the following criteria are fulfilled:
			\begin{enumerate}
				\item Old and new approximation do not change any more:
			\end{enumerate}
			\begin{align*}
				\big\lVert \vec{p}^{(k + 1)} - \vec{p}^{(k)} \big\rVert = \alpha^{(k)} \cdot \big\lVert \vec{d}_p^{(k)} \big\rVert_2 \leq \sqrt{\varepsilon_\mathrm{opt}} \Big( 1 + \big\lVert \vec{p}^{(k + 1)} \big\rVert_2 \Big)
			\end{align*}
			\begin{enumerate}
				\setcounter{enumi}{1}
				\item Gradient of the objective function that is projected onto the active constraints vanishes:
			\end{enumerate}
			\begin{align*}
				\Big\lVert \vec{Z}^T \cdot \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \Big\rVert_2 \leq \sqrt{\varepsilon_\mathrm{opt}} \Big( 1 + \max\Big\{ 1 + \big\lvert \varphi\big(\vec{p}^{(k + 1)}\big) \big\rvert,\, \big\lVert \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \big\rVert_2 \} \Big)
			\end{align*}
			\begin{enumerate}
				\setcounter{enumi}{2}
				\item Constraints are sufficiently fulfilled:
			\end{enumerate}
			\begin{align*}
				\big\lvert a_i\big(\vec{p}^{(k + 1)}\big) \big\rvert & \leq \varepsilon_\mathrm{ft},\quad i = 1, \cdots, n_a  \\
				b_j\big(\vec{p}^{(k + 1)}\big)                       & \geq -\varepsilon_\mathrm{ft},\quad i = 1, \cdots, n_b
			\end{align*}
			Where the constraints \( \varepsilon_\mathrm{opt} \), \( \varepsilon_\mathrm{ft} \) have to be chosen by the user.
			
			In the more modern method \emph{SNOPT}, all of the following two criteria have to be fulfilled:
			\begin{gather*}
				\frac{
					\max_{i = 1, \cdots, n_p} \bigg\{ \bigg\lvert \pdv{\varphi\big(\vec{p}^{(k)}\big)}{p_i} - \sum_{j = 1}^{n_a} \mu_j^{(k)} \cdot \pdv{a_j\big(\vec{p}^{(k)}\big)}{p_i} - \sum_{l \in A\big(\vec{p}^{(k)}\big)} \sigma_j^{(k)} \cdot \pdv{b_l\big(\vec{p}^{(k)}\big)}{p_i} \bigg\rvert \bigg\}
				}{
					\sqrt{\sum_{j = 1}^{n_a} \big( \mu_j^{(k)} \big)^2 + \sum_{l \in A\big(\vec{p}^{(k)}\big)} \big(\sigma_j^{(k)}\big)^2}
				} \leq \varepsilon_\mathrm{opt} \\
				\frac{\max\Big\{ \big\lvert a_j\big(\vec{p}^{(k)}\big) \big\rvert : i = 1, \cdots, n_p \Big\} \cup \Big\{ \big\lvert \min\big\{ 0,\, b_l\big(\vec{p}^{(k)}\big) \big\} \big\rvert : l \in A\big(\vec{p}^{(k)}\big) \Big\}}{\big\lVert \vec{p}^{(k)} \big\rVert_2} \leq \varepsilon_\mathrm{ft}
			\end{gather*}
			
			A common criteria to detect failures is a maximum number of iterations \( k_\mathrm{max} \).
		% end

		\subsection{Hessian Approximation}
			The quadratic problem\eqref{eq:qp} needs the \( (n_p \times n_p) \)-dimensional Hessian of the Lagrangian
			\begin{align*}
				\mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) = \mat{H}_\varphi\big(\vec{p}^{(k)}\big) - \sum_{i = 1}^{n_a} \mu_i^{(k)} \mat{H}_{a_i}\big(\vec{p}^{(k)}\big) - \sum_{i \in A\big(\vec{p}^{(k)}\big)} \sigma_i^{(k)} \mat{H}_{b_i}\big(\vec{p}^{(k)}\big)
			\end{align*}
			for (theoretical) quadratic convergence of the SQP method. But in practice, the second-order derivatives (the Hessian) is often not available! Additionally it is assumed that the Hessian has a positive curvature (i.e. is positive definite) along all feasible directions, which is fulfilled near a strict minimum. However, this cannot be assumed in every iteration.
			
			Hence, approximations/modifications of the Hessian are required.

			\subsubsection{Na{\"i}ve Approach: BFGS Approximation}
				It is tempting to use the BFGS update for the Hessian that is really successful in the unconstrained optimization. The update rule is given as:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} &= \tilde{\mat{H}}^{(k)}
						- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
						+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T \\
					\vec{g}^{(k)} &= \grad_{\vec{p}}L\big(\vec{p}^{(k + 1)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big) - \grad_{\vec{p}}L\big(\vec{p}^{(k)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big)
				\end{align*}
				But it is not necessary that the full Hessian of the Lagrangian is positive definite! Additionally, it is very inefficient to calculate the full Hessian if the NLP has lots of active constraints. Hence, the approximation has to be modified in order to be useful for SQP methods.
			% end

			\subsubsection{Reduced Hessian} % 4.61, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68
				Every active constraints reduces the degrees of freedom by one Hence the degrees of freedom are the number of optimization variables \(n_p\) minus the number of active and linearly independent constraints. In all of the following it is assumed that it is known which constraints are active at the solution, yielding the following, simpler, view of the NLP:
				\begin{align}
					\min_{\vec{d}_p \in \R^{n_p}} &\, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) \vec{d}_p \\
					\mathrm{subject~to}\quad &\,
						\begin{alignedat}[t]{2}
							\vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p &= \vec{0}
						\end{alignedat}  \label{eq:activeQp}
				\end{align}
				where the last Jacobian \( \mat{J}_{\vec{a}}^T \) might also contain active inequality constraints which are left out here for brevity. But they are handled the same as regular equality constraints and can be considered to be part of it knowing which are active (as assumed). The degrees of freedom of this NLP are therefore \( n_p - n_a \).
				
				Assuming the current approximation \( \vec{p}^{(k)} \) fulfills the constraints, \( \vec{a}\big(\vec{p}^{(k)}\big) = \vec{0} \), the next approximation also has to fulfill the constraints:
				\begin{align*}
					\vec{a}\big(\vec{p}^{(k)} + \vec{d}_p\big) = \vec{0}
				\end{align*}
				By Taylor-expanding this equation around \( \vec{p}^{(k)} \)
				\begin{align*}
					\vec{a}\big(\vec{p}^{(k)} + \vec{d}_p\big) \overset{T\big(\vec{p}^{(k)}\big)}{=} \vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p + \cdots = 0
				\end{align*}
				a linear system for the search direction \( \vec{d}_p \) can be found such that the new iteration is also feasible:
				\begin{align*}
					\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p = \vec{0}
				\end{align*}
				Thus, \( \vec{d}_p \) has to lie in the kernel of \( \mat{J}_{\vec{a}}^T \) and the dimensionality of the kernel is \( n_p - n_a \).
				
				Hence, the kernel is spanned by \( n_p - n_a \) basis vectors (which are not uniquely determined). Let \( \mat{Z}^{(k)} \) be the matrix that contains these basis vectors as columns, then
				\begin{align*}
					\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Z}^{(k)} = \mat{0}
				\end{align*}
				holds and the search direction \( \vec{d}_p \in \R^{n_p} \) can be represented as
				\begin{align*}
					\vec{d}_p = \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z
				\end{align*}
				where \( \mat{Z}^{(k)} \in \R^{n_p \times (n_p - n_a)} \) are the basis vectors of the kernel and \( \mat{Y}^{(k)} \in \R^{n_p \times n_a} \) are the basis vectors of the image space of \( \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \). The vectors \( \vec{d}_p^y \in \R^{n_a} \) and \( \vec{d}_p^z \in \R^{n_p - n_a} \) are unknown and have to be computed to solve the QP. Plugging this formulation of \( \vec{d}_p \) into the constraints of~\eqref{eq:activeQp}:
				\begin{align}
					&& \vec{0} &= \vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p &  \nonumber \\
					\iff && -\vec{a}\big(\vec{p}^{(k)}\big) &= \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p &  \nonumber \\
					     &&  &= \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big) &  \nonumber \\
					     &&  &= \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Y}^{(k)} \vec{d}_p^y + \underbrace{\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Z}^{(k)}}_{=\, \mat{0}} \vec{d}_p^z &  \nonumber \\
					     &&  &= \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Y}^{(k)} \vec{d}_p^y &  \label{eq:qpDYlinear}
				\end{align}
				The vector \( \vec{d}_p^y \) is therefore determined by the active constraints via the linear system~\eqref{eq:qpDYlinear}. Now there remain \( n_p - n_a \) degrees of freedom for the actual optimization.
				
				Plugging the formulation of \( \vec{d}_p \) into the objective of the QP\footnote{Note that the parameters of the Hessian are kept implicitly for brevity.}
				\begin{align*}
					&\, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}} \vec{d}_p \\
					=&\, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big) + \frac{1}{2} \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big)^T \mat{H}_L^{\vec{p}} \big( \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \big)
				\end{align*}
				yields the following objective when leaving out all constant parts w.r.t. \( \vec{d}_p^z \), as that is the optimization variable:
				\begin{align*}
					\varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \mat{Z}^{(k)} \vec{d}_p^z + \big(\vec{d}_p^y\big)^T \big(\mat{Z}^{(k)})^T \mat{H}_L^{\vec{p}} \big( \mat{Z}^{(k)} \vec{d}_p^z \big) + \frac{1}{2} \big(\vec{d}_p^z\big)^T \big(\mat{Z}^{(k)})^T \mat{H}_L^{\vec{p}} \mat{Z}^{(k)} \vec{d}_p^z
				\end{align*}
				The solution of this optimization problem can be computed by solving the following linear system (if the reduced Hessian is positive definite, which it is close to a strict local minimum):
				\begin{align*}
					\Big( \big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Z}^{(k)} \Big) \vec{d}_p^z = -\big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Y}^{(k)} \vec{d}_p^y - \big(\mat{Z}^{(k)}\big)^T \cdot \grad{\varphi}\big(\vec{p}^{(k)}\big)
				\end{align*}
				
				\paragraph{Summary}
					Using the representation \( \vec{d}_p = \mat{Y}^{(k)} \vec{d}_p^y + \mat{Z}^{(k)} \vec{d}_p^z \), the search direction as the solution of the QP~\eqref{eq:activeQp} can be computed by solving two staggered linear systems:
					\begin{align*}
						\mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \mat{Y}^{(k)} \vec{d}_p^y &= -\vec{a}\big(\vec{p}^{(k)}\big) \\
						\Big( \big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Z}^{(k)} \Big) \vec{d}_p^z &= -\big(\mat{Z}^{(k)}\big)^T \mat{H}_L^{(k)} \mat{Y}^{(k)} \vec{d}_p^y - \big(\mat{Z}^{(k)}\big)^T \cdot \grad{\varphi}\big(\vec{p}^{(k)}\big)
					\end{align*}
					
					\begin{itemize}
						\item If \( \vec{a}\big(\vec{p}^{(k)}\big) = \vec{0} \), i.e. the constraints are linear, \( \vec{d}_p^y = \vec{0} \).
						\item If additionally \( \big(\mat{Z}^{(k)}\big)^T \cdot \grad{\varphi}\big(\vec{p}^{(k)}\big) = \vec{0} \), then \( \vec{d}_p^z = \vec{0} \).
					\end{itemize}
				% end
				
				\paragraph{First- and Second-Order Conditions}
					Using the matrix \( \mat{Z} \) some of the necessary and sufficient conditions can be formulated equivalent:
					\begin{itemize}
						\item First-order necessary condition:
					\end{itemize}
					\begin{align*}
						\grad{\varphi}(\vec{p}^\ast) - \sum_{i = 1}^{n_a} \mu_i \cdot \grad{a_i}(\vec{p}^\ast) - \sum_{i \in A(\vec{p}^\ast)} = \vec{0}
						\quad\iff\quad
						\mat{Z}^T(\vec{p}^\ast) \cdot \grad{\varphi}(\vec{p}^\ast) = \vec{0}
					\end{align*}
					\begin{itemize}
						\item Second-order necessary condition: \\ The reduced Hessian of the Lagrangian \( \mat{Z}^T(\vec{p}^\ast) \cdot \mat{H}_L^{\vec{p}}(\vec{p}^\ast, \vec{\mu}^\ast, \vec{\sigma}^\ast) \cdot \mat{Z}(\vec{p}^\ast) \) is positive semidefinite.
						\item Second-order sufficient condition: \\ The reduced Hessian of the Lagrangian is positive definite.
					\end{itemize}
				% end

				\paragraph{Example} % 4.62
					\todo{Content}
				% end
			% end

			\subsubsection{Approximation of the Reduced Hessian}
				The reduced Hessian contains all information that is needed to compute the QP solution! Hence, SQP methods can be built on this reduced Hessian. A Quasi-Newton approximation, e.g. BFGS, can be used to update the reduced Hessian:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} &= \tilde{\mat{H}}^{(k)}
							- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
							+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T \\
					\vec{d}^{(k)} &= \vec{d}_p^z \\
					\vec{g}^{(k)} &= \big(\mat{Z}^{(k + 1)}\big)^T \cdot \grad_{\vec{p}}L\big(\vec{p}^{(k + 1)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big) - \big(\mat{Z}^{(k)}\big)^T \cdot \grad_{\vec{p}}L\big(\vec{p}^{(k)}, \vec{\mu}^{(k + 1)}, \vec{\sigma}^{(k + 1)}\big)
				\end{align*}
				
				Analogous to unconstrained optimization, it is helpful to replace the rank-2 update with a rank-1 update and directly approximate a Cholesky decomposition.
			% end
		% end

		\subsection{SQP Method (Algorithm)}
			A sketch of the implementation of an SQP method is given in~\autoref{alg:sqp}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
				\While{not converged}{
					Calculate the Lagrange multipliers \(\vec{\mu}^{(k)}\), \(\vec{\sigma}^{(k)}\) using least squares:
					\begin{align*}
						\min_{\mu \in \R^{n_a}} &\,
							\bigg\lVert
								\grad{\varphi}\big(\vec{p}^{(k + 1)}\big)
								- \sum_{i = 1}^{n_a} \mu_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
								- \sum_{i \in A\big(\vec{p}^{(k + 1)}\big)} \sigma_i^{(k + 1)} \cdot \grad{a_i}\big(\vec{p}^{(k + 1)}\big)
							\bigg\rVert_2^2
					\end{align*} \;
					\If{termination criteria fulfilled}{
						\Return \;
					}
					Calculate new search direction \( \vec{d}_p^{(k)} \) by solving the quadratic problem:
					\begin{align*}
						\min_{\vec{d}_p \in \R^{n_p}} &\, \varphi\big(\vec{p}^{(k)}\big) + \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)^T \vec{d}_p + \frac{1}{2} \vec{d}_p^T \mat{H}_L^{\vec{p}}\big(\vec{p}^{(k)}, \vec{\mu}^{(k)}, \vec{\sigma}^{(k)}\big) \vec{d}_p \\
						\mathrm{subject~to}\quad &\,
							\begin{alignedat}[t]{2}
								\vec{a}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{a}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p &= \vec{0} \\
								\vec{b}\big(\vec{p}^{(k)}\big) + \mat{J}_{\vec{b}}^T\big(\vec{p}^{(k)}\big) \vec{d}_p & \geq \vec{0}
							\end{alignedat}
					\end{align*} \;
					Calculate the step size \( \alpha^{(k)} \) using by minimizing a merit function, e.g. the augmented Lagrangian
					\begin{align*}
						\min_{\alpha \in \R^+} \, L_Q\big( \vec{p}^{(k)} + \alpha \vec{d}_p^{(k)},\, \vec{\mu}^{(k)} + \alpha \vec{d}_\mu^{(k)},\, \vec{\sigma}^{(k)} + \alpha \vec{d}_\sigma^{(k)},\, \rho^{(k)} \big)
					\end{align*}
					or the exact \(\ell_1\)-penalty function \( \min_{\alpha \in \R^+} \, \Phi_{\ell_1} \big( \vec{p}^{(k)} + \alpha \vec{d}_p^{(k)},\, \rho^{(k)} \big) \) \;
					Calculate the new solution approximation:
					\begin{align*}
						\vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}_p^{(k)}
					\end{align*} \;
				}
				
				\caption{Sequential Quadratic Programming}
				\label{alg:sqp}
			\end{algorithm}
		% end

		\subsection{Notes}
			\begin{itemize}
				\item When using the reduced Hessian, the matrix \( \mat{Z}^{(k)} \) has to be updated in every iteration.
				\item Especially for high-dimensional problems, the gradients and Jacobians are sparse, i.e. only a few entries are nonzero. This behavior can be exploited in order to optimize implementations a lot.
				\item To rise efficiency and robustness further, sophisticated implementations of SQP methods (e.g. NPSOL, SNOPT) differentiate between constraints like
					\begin{itemize}
						\item upper and lower bounds
						\item linear constraints
						\item nonlinear constraints
					\end{itemize}
			\end{itemize}
		% end
		
		\subsection{Examples} % N/A
			\todo{Content}
			
			\subsubsection{Optimal Control of a 6-DoF Industry Robot} % 4.71, 4.72
				\todo{Content}
			% end
			
			\subsubsection{Car Drive} % 4.73, 4.74
				\todo{Content}
			% end
		% end
		
		\subsection{Wrap-Up}
			\begin{itemize}
				\item Motivation:
					\begin{itemize}
						\item The Newton method is applied to determine a root ("zero point") of the gradient of the Lagrangian.
						\item This yields a linear equation system to determine a search direction.
						\item Requires first- and second-order derivatives of the objective and the constraints.
						\item Preliminaries for the derivative where differentiability and knowing which constraints are active.
					\end{itemize}
				\item Basic structure:
					\begin{itemize}
						\item The linear system derived as a first step is equivalent to the solution of a quadratic problem (QP).
						\item Solving the QP is faster and more robust than solving the linear system directly.
						\item This yields a sequence of quadratic problems, thus \emph{sequential quadratic programming} (SQP).
					\end{itemize}
				\item Improvement of the basic structure:
					\begin{itemize}
						\item For globalizing the method, a step size determination was introduced using line search on an appropriate test function (penalty function).
						\item As the second-order derivatives are commonly not available, Quasi-Newton approximations of the Lagrangian are used.
					\end{itemize}
				\item SQP for high-dimensional NLPs:
					\begin{itemize}
						\item If the NLP is high-dimensional, the reduced Hessian shall be used.
						\item The QP can be solved faster and the Hessian of the Lagrangian can be approximation using Quasi-Newton approaches!
						\item High-dimensional NLPs often have sparse gradients as Jacobians which can be used for further performance improvements.
					\end{itemize}
				\item Outlook:
					\begin{itemize}
						\item There are lots of SQP methods, e.g. trust-region SQP that can work with indefinite or negative definite Hessians or methods that allow only feasible approximations in every iteration (feasible SQP methods), while the "classic" SQP only fulfills the constraints at the end (infeasible SQP method).
						\item Other numerical methods for solving nonlinear, constrained optimization problems exist, e.g. inner point methods.
					\end{itemize}
			\end{itemize}
		% end
	% end
% end

\chapter{Calculation of Derivatives}
	To use efficient gradient-based algorithms, the first-order derivatives
	\begin{align*}
		\pdv{\varphi}{p_i} && \pdv{a_k}{p_i} && \pdv{b_l}{p_i}
	\end{align*}
	of the objective and the constraints are needed. But these are typically not available directly! And even one wrong derivative could destroy the fast convergence properties\dots

	\section{Finite Difference Approximation (numerical Differentiation)}
		\subsection{Forward Difference Approximation}
			The most known approximation of the first derivative  is the \emph{forward difference approximation}
			\begin{align*}
				\pdv{\varphi(\vec{p})}{p_i} \approx D_{V, i} \varphi(\vec{p}) = \frac{1}{\delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) \big)
			\end{align*}
			where \( \vec{e}_i \in \R^{n_p} \) is the \(i\)-th unit vector and \(\delta_i\) is an appropriate step size. The complete error is:

			\subsubsection{Error}
				The complete error of the approximation is composed of the following:
				\begin{itemize}
					\item Approximation error (theoretical error)
					\item Function precision
					\item Rounding error
				\end{itemize}

				\paragraph{Approximation Error}
					The (theoretical) approximation error is given as the neglected terms of the Taylor approximation (i.e. the Lagrangian remainder):
					\begin{align*}
						&& \varphi(\vec{p} + \vec{e}_i \delta_i) &\overset{\mathclap{\,T(\vec{p})\,}}{=} \varphi(\vec{p}) + \pdv{\varphi(\vec{p})}{p_i} \delta_i + \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i^2,\quad \tilde{\vec{p}} \in [\vec{p}, \vec{p} + \vec{e}_i \delta_i] & \\
						\iff && \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) - \pdv{\varphi(\vec{p})}{p_i} \delta_i &= \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i^2 & \\
						\iff && \underbrace{\frac{1}{\delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) \big)}_{=\, D_{V, i}\varphi(\vec{p})} - \pdv{\varphi(\vec{p})}{p_i} &= \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i & \\
						\iff && D_{V, i}\varphi(\vec{p}) - \pdv{\varphi(\vec{p})}{p_i} &= \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i \eqqcolon T_{V, i}(\varphi; \delta_i) &
					\end{align*}
					In theory, the error should decrease with the step size. But today's computers have finite arithmetic! Other error factors have a serious role demolishing this theoretical result.
				% end

				\paragraph{Function Precision}
					The function precision takes into account that the target function \( \varphi \) cannot be calculated with machine precision, e.g. because the evaluation depends on other methods or because rounding errors have summed up due to cancellation or ill-conditioning.
					
					This can be taken into account with the absolute errors \( \varepsilon \), \( \varepsilon_{\delta_i} \) of the function evaluations:
					\begin{align*}
						\hat{\varphi}(\vec{p}) = \varphi(\vec{p}) + \varepsilon && \hat{\varphi}(\vec{p} + \vec{e}_i \delta_i) = \varphi(\vec{p} + \vec{e}_i \delta_i) + \varepsilon_{\delta_i}
					\end{align*}
					Where the absolute error can be expressed in terms of a relative error \( \varepsilon_R = 10^{-n_d} \) as \( \varepsilon = \varepsilon_R \varphi(\vec{p}) \) where \(n_d\) are the number of decimal places that are correct. Plugging \( \hat{\varphi} \) into the forward approximation yields the function precision error \( C(D_{V, i}\varphi; \delta_i) \):
					\begin{align*}
						D_{V, i}\hat{\varphi}(\vec{p})
							= \frac{1}{\delta_i} \big( \hat{\varphi}(\vec{p} + \vec{e}_i \delta_i) - \hat{\varphi}(\vec{p}) \big)
							= \frac{1}{\delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p}) \big) + \frac{\varepsilon_{\delta_i} - \varepsilon}{\delta_i}
							\eqqcolon D_{V, i} \varphi(\vec{p}) + C(D_{V, i} \varphi; \delta_i)
					\end{align*}
				% end

				\paragraph{Rounding Error}
					Additionally to the function precision and the theoretical error, rounding errors are produced by the subtraction and the division. But if \(\delta_i\) does not get "too small", these are negligible compared to the approximation error and the function precision.
				% end
				
				\paragraph{Total Error}
					Hence, the total error is given as:
					\begin{align}
						T_{V, i}(\varphi; \delta_i) + C(D_{V, i} \varphi; \delta_i) = \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i + \frac{\varepsilon_{\delta_i} - \varepsilon}{\delta_i}  \label{eq:forwardError}
					\end{align}
				% end
			% end

			\subsubsection{Choosing the Step Size}
				Ideally, the step size should be chooses such that the error is minimal. As the error term~\eqref{eq:forwardError} contains second-order derivatives that are not available, an upper bound has to be drawn on the error that more or less independent of the derivatives:
				\begin{align*}
					\Bigg\lvert \frac{1}{2} \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \delta_i + \frac{\varepsilon_{\delta_i} - \varepsilon}{\delta_i} \Bigg\rvert
						&\leq \frac{1}{2} \delta_i \Bigg\lvert \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \Bigg\rvert + \frac{1}{\delta_i} \big\lvert \varepsilon_{\delta_i} - \varepsilon \big\rvert
						\leq \frac{1}{2} \delta_i L_{\varphi\prime\prime, i} + \frac{2}{\delta_i} \varepsilon_R L_\varphi \\
					L_{\varphi\prime\prime, i} &\coloneqq \max\Bigg\{\, \Bigg\lvert \pdv[2]{\varphi(\tilde{\vec{p}})}{p_i} \Bigg\rvert \,:\, \tilde{\vec{p}} \in [\vec{p}, \vec{p} + \vec{e}_i \delta_i] \,\Bigg\} \\
					L_\varphi &\coloneqq \max\Big\{\, \big\lvert \varphi(\vec{p}) \big\rvert,\, \big\lvert \varphi(\vec{p} + \vec{e}_i \delta_i) \big\rvert \,\Big\}
				\end{align*}
				Minimizing this w.r.r. to the step size yields:
				\begin{gather*}
					\min_{\delta_i \in \R^+} \, \Phi(\delta_i),\quad \Phi(\delta_i) = \frac{1}{2} \delta_i L_{\varphi'', i} + \frac{2}{\delta_i} \varepsilon_R L_\varphi \\
					\implies\quad \Phi'(\delta_i) = \frac{1}{2} L_{\varphi\prime\prime, i} - \frac{2}{\delta_i^2} \varepsilon L_\varphi \overset{!}{=} 0
					\quad\overset{L_{\varphi\prime\prime, i} \,\neq\, 0}{\iff}\quad \delta_i = \sqrt{\frac{4 \varepsilon_R L_\varphi}{L_{\varphi'', i}}}
				\end{gather*}
				If \( L_\varphi / L_{\varphi\prime\prime, i} \approx \), then \( \delta_i \approx 2 \sqrt{\varepsilon_R} \). If additionally it is possible to evaluate the function with machine precision, i.e. \( \varepsilon_R = \varepsilon_\mathrm{mach} \), the optimal step size is simply
				\begin{align*}
					\delta_i \approx 2 \sqrt{\varepsilon_\mathrm{mach}}
				\end{align*}
				This step size often is a good choice and thus set as the default in most implementations. It also explains the rule of thumb that forward-differences can approximately evaluate half of decimal places correctly.
			% end
			
			\subsubsection{Notes}
				\begin{itemize}
					\item For evaluating the gradient \( \grad{\varphi}(\vec{p}) \) it is necessary to
						\begin{itemize}
							\item Determine \(n_p\) step sized \(\delta_i\) and to evaluate \(\varphi\) at least \( 2n_p \) times to approximate \( L_{\varphi\prime\prime, i} \).
							\item Every iteration of a gradient-based method needs the gradients causing high computation times.
							\item It is better to use a one-time approximation of \emph{relative step sizes} \( \varepsilon_i \) with \( \delta_i = \varepsilon_i \big(1 + \lvert p_i \rvert\big) \) at the initialization \(\vec{p}^{(0)}\).
							\item The value \( \varepsilon_i = 5 \sqrt{\varepsilon_\mathrm{mach}} \) can be used as an initial relative step size.
							\item If the optimization fails, restart with a new initialization and re-calculate the step sizes.
						\end{itemize}
				\end{itemize}
			% end
		% end

		\subsection{Central-Difference Approximation}
			For forward difference approximation often yields results that are good enough, except the gradients are too small. Additionally, the forward approximation is not sufficient if the optimization step size \( \alpha^{(k)} \) such that the changes in \( \vec{p} \) are less than the step size \(\delta\) or the differences in the function values are "too small" relative to \(\delta\).
			
			A potentially more exact approximation are \emph{central differences}:
			\begin{align*}
				\pdv{\varphi(\vec{p})}{p_i} \approx D_{Z, i}\varphi(\vec{p}) = \frac{1}{2 \delta_i} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - \varphi(\vec{p} - \vec{e}_i \delta_i) \big)
			\end{align*}
			Analogous to the forward differences, Taylor-expand this formula yields the insight that the order of the approximation error is \( \mathcal{O}(\delta_i^2) \) while the order of the forward differences is \( \mathcal{O}(\delta_i) \). Analogous to the forward differences, the optimal step size \( \delta_i^Z \) can be calculated by minimizing an upper bound on the total error, yielding the optimal step size
			\begin{align*}
				\delta_i^Z = \sqrt[3]{\frac{3 \varepsilon_R L_\varphi}{L_{\varphi\prime\prime\prime, i}}},\quad L_{\varphi\prime\prime\prime, i} = \max\Bigg\{\, \Bigg\lvert \pdv[3]{\varphi(\tilde{\vec{p}})}{p_i} \Bigg\rvert \,:\, \tilde{\vec{p}} \in \big[ \vec{p} - \vec{e}_i \delta_i^Z, \vec{p} + \vec{e}_i \delta_i^Z \big] \,\Bigg\}
			\end{align*}
			For functions with \( L_\varphi / L_{\varphi\prime\prime\prime, i} \approx 1 \) it follows \( \delta_i^Z \approx \sqrt[3]{3\varepsilon_R} \approx \delta_i^{2/3} \). If additionally \( L_\varphi \approx 1 \) and \( L_{\varphi\prime\prime\prime, i} \approx 1 \), it follows:
			\begin{align*}
				\Bigg\lvert D_{Z, i}\hat{\varphi}(\vec{p}) - \pdv{\varphi(\vec{p})}{p_i} \Bigg\rvert \leq \varepsilon_R^{2/3}
			\end{align*}
			Hence, the rule of thumb that central difference can approximately evaluate two third of decimal places correctly.
			
			But central differences produce much more computational overhead compared to forward/backward differences! Thus they should only be used if needed (by switching from forward to central differences).
			
			To approximate the second derivative, the following schema can be used
			\begin{align*}
				\pdv[2]{\varphi(\vec{p})}{p_i} \approx \frac{1}{\delta_i^2} \big( \varphi(\vec{p} + \vec{e}_i \delta_i) - 2\varphi(\vec{p}) + \varphi(\vec{p} - \vec{e}_i \delta_i) \big)
			\end{align*}
			that is a combination of forward and backward differences to approximate the second-order derivative. This directly gives the \(i\)-th diagonal entry of the Hessian of \(\varphi\), which can reduce the number of iterations needed.
		% end
	% end

	\section{Numerical Differentiation of Simulation Models}
		An important class in optimization is simulation-based optimization where the system state \( \vec{x}(t) \) is given as the numerical solution of ODEs or PDEs. In this setting, the objective \(\varphi\) and the constraints \(\vec{a}\), \(\vec{b}\) are dependent on the state variables \(\vec{x}\) of an ODE/PDE system. Hence, the calculation of \( \varphi\big(\vec{x}(\vec{p})\big) \) requires solving the ODE/PDE numerically:
		\begin{itemize}
			\item Every calculation of \( \varphi\big(\vec{x}(\vec{p})\big) \), \(\vec{a}\) and \(\vec{b}\) is computationally expensive.
			\item The calculation is only possible with simulation errors (i.e. approximation error in the ODE/PDE solver and accumulated rounding errors).
			\item The gradients (see below) are generally not available and have to me approximated.
		\end{itemize}
		\begin{align*}
			\grad{\varphi}\big(\vec{x}(\vec{p})\big) = \pdv{\varphi\big(\vec{x}(\vec{p})\big)}{\vec{p}} = \pdv{\varphi}{\vec{x}} \pdv{\vec{x}}{\vec{p}}
		\end{align*}

		\subsection{Derivative of ODE-Simulation Models}
			\label{subsec:derivativeOde}
		
			Given an IVP (initial value problem) \( \dot{\vec{x}} = \vec{f}(t, \vec{x}; \vec{p}) \), \( \vec{x}(0) = \vec{x}_0 \), \( 0 \leq t \leq t_f \), the derivatives of the (numerical) solution \( \vec{x}(t; \vec{p}) \) w.r.t. to the parameters \( \vec{p} \) are required. Formally, the parameters \(\vec{p}\) can be transformed to initial values \(\vec{x}_0\) and thus the derivatives w.r.t. the parameters to derivatives w.r.t. the initial values. The derivative w.r.t. the initial values is called the \emph{sensitivity matrix}:
			\begin{align*}
				\pdv{\vec{x}(t; \vec{x}_0)}{\vec{x}_0}
			\end{align*}
			
			The IVP with the parameters \(\vec{p}\) transformed to initial values is given as
			\begin{align*}
				\begin{bmatrix}
					\dot{x}_1 \\
					\vdots \\
					\dot{x}_{n_x}
				\end{bmatrix}
				= \dot{\vec{x}} = \vec{f}(t, \vec{x}, x_{n_x + 1}, \cdots, x_{n_x + n_p}) \\
				\begin{bmatrix}
					\dot{x}_{n_x + 1} \\
					\vdots \\
					\dot{x}_{n_x + n_p}
				\end{bmatrix}
				=
				\begin{bmatrix}
					0 \\
					\vdots \\
					0
				\end{bmatrix}
			\end{align*}
			with \( x_{n_x + 1} \coloneqq p_1 \), \( \cdots \), \( x_{n_x + n_p} \coloneqq p_{n_p} \) and the initial values
			\begin{align*}
				\vec{x}(0) = \vec{x}_0 \qquad
				\begin{bmatrix}
					x_{n_x + 1}(0) \\
					\vdots \\
					x_{n_x + n_p}(0)
				\end{bmatrix}
				=
				\begin{bmatrix}
					p_1 \\
					\vdots \\
					p_{n_p}
				\end{bmatrix}
			\end{align*}
			That is, the "parameter states" are added as time invariant states always equaling the parameters.
		% end

		\subsection{External Numerical Differentiation}
			\subsubsection{Na{\"i}ve Approach} % 5.29, 5.30, 5.31
				Just use forward differences:
				\begin{align*}
					\pdv{\vec{x}(t; \vec{p})}{p_i} \approx \frac{1}{\delta_i} \big( \vec{x}(t; \vec{p} + \vec{e}_i \delta_i) - \vec{x}(t; \vec{p}) \big)
				\end{align*}
				\begin{itemize}
					\item This approach requires solving \(n_p\) additional IVP solutions for calculating the tweaked evaluations.
					\item To solve the IVP as best as possible, it may use variable step sizes.
					\item This causes gradient-based algorithms to have extremely big problems in finding the minimum!
					\item But this is not only caused by the optimization method\dots
				\end{itemize}
			
				\paragraph{Runge-Kutta Methods}
					When using Runge-Kutta methods of order \(m\), the numerical solutions depends on the integration tolerance and the internal step width of the integration. By just looking at the initial values, the error of RK methods of order \(m\) is
					\begin{align*}
						\tilde{\vec{x}}(t; \vec{x}_0, h_1) &= \vec{x}(t; \vec{x}_0) + \sum_{j = m}^{N} \tilde{c}_j(t, \vec{x}_0) h_1^j + \mathcal{O}\big(h_1^{N + 1}\big) \\
						\tilde{\vec{x}}(t; \vec{x}_0 + \vec{e}_i \delta_i, h_2) &= \vec{x}(t; \vec{x}_0 + \vec{e}_i \delta_i) + \sum_{j = m}^{N} \tilde{c}_j(t, \vec{x}_0 + \vec{e}_i \delta_i) h_2^j + \mathcal{O}\big(h_2^{N + 1}\big)
					\end{align*}
					with differentiable functions \( \tilde{c}_j(t, \vec{x}_0) \). Plugging this into the forward difference scheme yields:
					\begin{align*}
						  &\, \frac{1}{\delta_i} \big( \tilde{\vec{x}}(t; \vec{x}_0 + \vec{e}_i \delta_i, h_2) - \tilde{\vec{x}}(t; \vec{x}_0, h_1) \big) \\
						= &\, \pdv{\vec{x}(t; \vec{x}_0)}{x_{0, i}} + \mathcal{O}(\delta_i) + \sum_{j = m}^{N} \tilde{c}_j(t, \vec{x}_0 + \vec{e}_i \delta_i) \cdot \underbrace{\frac{h_2^j - h_1^j}{\delta_i}}_{\to\,\infty} + \sum_{j = m}^{N} \Bigg( \pdv{\tilde{c}_j(t, \vec{x}_0)}{x_{0, i}} + \mathcal{O}(\delta_i) \!\Bigg) h_1^j + \mathcal{O}\Bigg( \frac{h_1^{N + 1}}{\delta_i} \Bigg) + \mathcal{O}\Bigg( \frac{h_2^{N + 1}}{\delta_i} \Bigg)
					\end{align*}
					This is the problem! If the integration steps \( h_1 \neq h_2 \) are not equal, the error term becomes dominant as \( \delta_i \) usually is "small".
					
					A "solution" would be to set \( h_1 = h_2 \), causing bad integration results.
				% end
			% end

			\subsubsection{Coupled Forward Differences Approximation}
				It is possible to simultaneously integrate an \( (n_p + 1) \)-times big IVP for each tweaked value guaranteeing \( h_1 = h_2 \):
				\begin{align*}
					\dot{\vec{x}} &= \vec{f}(t, \vec{x}),\quad \vec{x}(0) = \vec{x}_0 \\
					\dot{\vec{x}} &= \vec{f}(t, \vec{x}) \\
					&\,\,\,\vdots \\
					\dot{\vec{x}} &= \vec{f}(t, \vec{x})
				\end{align*}
			% end
		% end

		\subsection{Internal Numerical Differentiation}
			The external numerical differentiation costs a lot of time! One insight: The sensitivity matrix can be expresses as the solution of a matrix-ODE:
			\begin{align*}
				\dv{t} \pdv{\vec{x}(t; \vec{x}_0)}{\vec{x}_0} = \pdv{\vec{f}\big(t, \vec{x}(t; \vec{x}_0)\big)}{\vec{x}} \pdv{\vec{x}(t; \vec{x}_0)}{\vec{x}_0},\quad \pdv{\vec{x}(0; \vec{x}_0)}{\vec{x}_0} = \mat{I}
			\end{align*}
			
			\begin{itemize}
				\item Variant 1: Simultaneously integrate the ODE and the matrix-ODE with a standard integrator.
				\item Variant 2 (better): Differentiated integrate method calculates \( \vec{x}(t; \vec{p}) \), \( \pdv{\vec{x}(t; \vec{p})}{\vec{p}} \), \( \pdv{\dot{\vec{x}}(t; \vec{p})}{\vec{p}} \)
					\begin{itemize}
						\item Advantage: Really efficient.
						\item Disadvantage: Implementation complexity; especially complication for switching points.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Symbol Differentiation}
		If the functions \( \varphi \), \( \vec{a} \), \( \vec{b} \) are given as explicit formulas, the derivatives \( \grad{\varphi}(\vec{p}) \), \( \mat{J}_{\vec{a}}(\vec{p}) \), \( \mat{J}_{\vec{b}}(\vec{p}) \) could be evaluated using a computer algebra system (e.g. Maple, Mathematica, SymPy, \dots) in closed form. This is based on a systematic application of chain, product, \dots rules and often requires a special input format.
		\begin{itemize}
			\item Advantage: No approximation error, only rounding errors.
			\item Disadvantages: Can lead to complex functions that are computationally expensive to evaluate.
		\end{itemize}
	% end

	\section{Automatic Differentiation}
		\emph{Automatic differentiation} refers to a technique to generate first and possible second-order derivatives of an existing program that calculates \(\varphi\), \(\vec{a}\), \(\vec{b}\). This is based on the insight that every every so complex function can be composed of as a sequence of elementary functions with one or two arguments:
		\begin{itemize}
			\item 1-argument functions: Sine, Cosine, Exponential, Logarithm, \dots
			\item 2-argument functions: Addition, Subtraction, Multiplication, Division, Exponentiation
		\end{itemize}
		AD-methods are based on an analysis of the evaluation sequence as a \emph{computation graph} of elementary functions.
		
		There are two main design decisions:
		\begin{itemize}
			\item Pre-compile a program to get the derivative function to be able to evaluation the function and the derivatives simultaneously (forward mode).
			\item Build the computation graph after the function has been evaluated and evaluate the derivative afterwards (backward mode).
		\end{itemize}
		The computational complexity of the gradient in forward mode on a scalar function with multiply variables \(\varphi(\vec{p})\) can be as high as for symbolic differentiation: \( \mathcal{O}(n_p \). But for the backward mode, a complexity of \( \mathcal{O}(5) \) can be reached independent of \( n_p \)! But the memory complexity can rise a lot\dots
		
		\begin{itemize}
			\item Advantages:
				\begin{itemize}
					\item No approximation error, only rounding errors.
					\item AD is continuously improving and even today's algorithms are capable of a lot of calculations.
				\end{itemize}
			\item Disadvantages:
				\begin{itemize}
					\item It is problematic to handle piecewise functions (if-then-else), approximation of tabular data, approximations of Sine, Cosine, \dots by rationale functions. \\ But this is problematic even for analytical derivatives\dots
				\end{itemize}
		\end{itemize}
		Even ODE- and PDE-simulations can be handled using AD!
		
		Popular implementations, commonly used in machine learning, are libraries like TensorFlow (static computation graph) and PyTorch (dynamic computation graph).
	% end
% end

\chapter{Parameter Estimation}
	\label{c:leastSquares}
	
	This chapter covers a special type of objective function: the sum of squares, called \emph{least squares} optimization problems:
	\begin{align*}
		\varphi(\vec{p}) = \frac{1}{2} \sum_{i = 1}^{n_r} r_i^2(\vec{p}) = \frac{1}{2} \lVert \vec{r}(\vec{p}) \rVert_2^2
	\end{align*}
	This objective function arises in a lot of optimization problems, e.g.: Given some detected corner points \( (x_i, y_i)_{i = 1, \cdots, n_r} \) of a ball, what is the radius \(R_K\) and the position \( (x_K, y_K) \) if the ball? The residual (error) function \(\vec{r}\) can be determined from the circle equation:
	\begin{align*}
		R_K^2 = (x_i - x_K)^2 + (y_i - y_K)^2 \quad\implies\quad r_i(x_K, y_K, R_K) = \sqrt{(x_i - x_K)^2 + (y_i - y_K)^2} - R_K
	\end{align*}
	
	If, in practice, some parametric model is used, this almost always leads to a parameter fitting problem to measure/minimize the differences between the model and measurements. By minimizing the differences, the model with that most correspond to the measurements can be found (e.g. the parameters of a friction model or inertia of a robot).

	\section{Objective Functions}
		There are various different objective functions that could be used:
		\begin{itemize}
			\item Absolute sum: \tabto{4cm} \( \varphi_1(\vec{p}) = \lVert \vec{r}(\vec{p}) \rVert_1 = \sum_{i = 1}^{n_r} \lvert r_i(\vec{p}) \rvert \)
			\item Sum of squares: \tabto{4cm} \( \varphi_2(\vec{p}) = \frac{1}{2} \lVert \vec{r}(\vec{p}) \rVert_2^2 = \frac{1}{2} \sum_{i = 1}^{n_r} r_i^2(\vec{p}) \)
			\item Maximum difference: \tabto{4cm} \( \varphi_\infty(\vec{p}) = \lVert \vec{r}(\vec{p}) \rVert_\infty = \max\big\{ \lvert r_i(\vec{p}) \rvert : i = \dotsrange{1}{n_r} \big\} \)
		\end{itemize}
		But even if \(\vec{r}\) is differentiable, \(\varphi_1\) and \(\varphi_\infty\) are generally not. But \(\varphi_\infty\) can be transformed into a NLP with differentiable functions:
		\begin{align*}
			\min_{\vec{p}, p_{n_r + 1}} &\, p_{n_r + 1} \\
			\mathrm{subject~to} &\,
				-p_{n_r + 1} \leq r_i(\vec{p}) \leq p_{n_r + 1},\quad i = \dotsrange{1}{n_r}
		\end{align*}
		Anyway, \(\varphi_\infty\) is extremely sensitive towards outliers. On the other hand, \(\varphi_2\) is less sensitive towards outliers and is differentiable (if \(\vec{r}\) is differentiable)!
		
		Hence, in most cases the least squares objective \(\varphi_2\) is used. Besides the differentiability, it is also statistically appealing: If the measurement noise \(\varepsilon_{ij}\) are statistically independent and Gaussian distributed with mean \(0\) and variance \(\sigma^2\), the solution of \( \varphi_2 \to \mathrm{min} \) is a maximum likelihood estimator!
		
		Additionally, by exploiting the structure of \(\varphi_2\), the efficiency of gradient-based algorithms can be increased (e.g. due to sparse Jacobians).
	% end

	\section{Linear Least Squares}
		In the special case of a linear residual function \( \vec{r}(\vec{p}) = \mat{J}^T \vec{p} + \vec{f} \) and the least squares objective
		\begin{align*}
			\varphi(\vec{p}) = \varphi_2(\vec{p}) = \frac{1}{2} \lVert \vec{r}(\vec{p}) \rVert_2^2
		\end{align*}
		the optimization problem is quadratic in \(\vec{p}\):
		\begin{align*}
			\varphi(\vec{p}) = \frac{1}{2} \big\lVert \mat{J}^T \vec{p} + \vec{f}_r \big\rVert = \frac{1}{2} \big( \vec{p}^T \mat{J} + \vec{f}_r^T \big) \big( \mat{J}^T \vec{p} + \vec{f}_r \big)
		\end{align*}
		Zeroing the gradient \( \grad{\varphi}(\vec{p}) \overset{!}{=} \vec{0} \) yields the \emph{normal equations}
		\begin{align*}
			\mat{J} \mat{J}^T \vec{p}^\ast = -\mat{J}\vec{f}_r
		\end{align*}
		that a solution must fulfill (where \( \mat{J} \mat{J}^T \) is symmetric and positive definite). This linear system should not be solved as a normal linear system as it is ill-conditioned! Better use special methods like orthogonalization or single value decomposition.
	% end

	\section{Optimality Conditions and Special Methods}
		Let \( \varphi(\vec{p}) = \varphi_2(\vec{p}) \) be two times continuously differentiable. Then the following first- and second-order necessary conditions can be formulated:
		\begin{enumerate}
			\item The gradient has to vanish:
		\end{enumerate}
		\begin{align*}
			\grad{\varphi_2}(\vec{p}^\ast) = \mat{J}_{\vec{r}}(\vec{p}^\ast) \cdot \vec{r}(\vec{p}^\ast) = \vec{0}
		\end{align*}
		\begin{enumerate}
			\setcounter{enumi}{1}
			\item The Hessian has to be positive semidefinite:
		\end{enumerate}
		\begin{align}
			\mat{H}_{\varphi_2}(\vec{p}^\ast) = \mat{J}_{\vec{r}}(\vec{p}^\ast) \, \mat{J}_{\vec{r}}^T(\vec{p}^\ast) + \sum_{i = 1}^{n_r} \big( r_i(\vec{p}^\ast) \cdot \mat{H}_{r_i}(\vec{p}^\ast) \big)  \label{eq:leastSquaresHessian}
		\end{align}

		\subsection{Gauss-Quasi-Newton Method}
			In the Quasi-Newton method, the search direction is determined as a solution of the linear system
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			where the Hessian is approximated, e.g. using a BFGS update. But in the case of a least squares optimization problem, the only part of the Hessian~\eqref{eq:leastSquaresHessian} depending on second-order derivatives is:
			\begin{align*}
				\sum_{i = 1}^{n_r} \big( r_i(\vec{p}^\ast) \cdot \mat{H}_{r_i}(\vec{p}^\ast) \big)
			\end{align*}
			But as the objective gets smaller and smaller, this term also vanishes. Hence, the Hessian can be approximated using first-order derivatives only
			\begin{align*}
				\mat{H}_{\varphi_2}(\vec{p}^\ast) \approx \mat{J}_{\vec{r}}(\vec{p}^\ast) \, \mat{J}_{\vec{r}}^T(\vec{p}^\ast)
			\end{align*}
			if the residuals are "small". This leads to the \emph{Gauss-Newton Method} where the search direction is given as the solution of the normal equations
			\begin{align*}
				\mat{J}_r\big(\vec{p}^{(k)}\big) \mat{J}_r^T\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\mat{J}_r\big(\vec{p}^{(k)}\big) \cdot \vec{r}\big(\vec{p}^{(k)}\big)
			\end{align*}
			or as the solution of a (better conditioned) linear least squares problem:
			\begin{align*}
				\min_{\vec{d} \in \R^{n_p}} &\, \frac{1}{2} \Big\lVert \mat{J}_r^T\big(\vec{p}^{(k)}\big) \vec{d} + \vec{r}\big(\vec{p}^{(k)}\big) \Big\rVert_2^2
			\end{align*}
			
			If residuals are big or the problem is ill-conditioned, it can be modified with a suitable matrix \( \mat{B}^{(k)} \):
			\begin{align*}
				\Big( \mat{J}_{\vec{r}}\big(\vec{p}^{(k)}\big) \mat{J}_{\vec{r}}^T\big(\vec{p}^{(k)}\big) + \mat{B}^{(k)} \Big) \vec{d}^{(k)} = -\mat{J}_r\big(\vec{p}^{(k)}\big) \cdot \vec{r}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			Additionally a step size rule should be used. This method is implemented, e.g. in \emph{NLSCON} which also allows additional nonlinear inequality constraints.
		% end

		\subsection{Levenberg-Marquardt Methods}
			Instead of a Newton-approach, trust region methods can be used to determine the search direction. In the \emph{Levenberg-Marquardt Method}, the search direction \(\vec{d}^{(k)}\) is given as the solution of:
			\begin{align*}
				\Big( \mat{J}_{\vec{r}}\big(\vec{p}^{(k)}\big) \mat{J}_{\vec{r}}^T\big(\vec{p}^{(k)}\big) + \gamma^{(k)} \mat{I} \Big) \vec{d}^{(k)} = -\mat{J}_{\vec{r}}\big(\vec{p}^{(k)}\big) \cdot \vec{r}\big(\vec{p}^{(k)}\big),\quad \gamma^{(k)} \geq 0
			\end{align*}
			That is, the search direction is a mixture of the Gauss-Newton direction and steepest descent. The same search direction can be received by solving a constrained linear least squares problem:
			\begin{align*}
				\min_{\vec{d} \in \R^{n_p}} &\, \frac{1}{2} \Big\lVert \mat{J}_{\vec{r}}^T\big(\vec{p}^{(k)}\big) \vec{d} + \vec{r}\big(\vec{p}^{(k)}\big) \Big\rVert_2^2 \\
				\mathrm{subject~to} &\,
					\lvert \vec{d} \rVert_2 \leq \delta^{(k)}
			\end{align*}
			Where \(\delta\) and \(\gamma\) have a connection.
			
			This is implemented, e.g. in \emph{LMDER}, \emph{LMJAC} of \emph{MINPACK}.
		% end
		
		\subsection{Notes}
			\begin{itemize}
				\item The Gauss-Newton method needs an appropriate test function that also exploits the structure of the objective function to determine the step size to ensure global convergence.
				\item In the Levenberg-Marquardt method, globalization happens by choosing an appropriate trust region.
				\item Levenberg-Marquardt and Gauss-Newton methods are in general a lot more efficient (faster and more precise) than solving least squares problems using general purpose optimization techniques (e.g. Quasi-Newton or SQP).
				\item When optimization by simulation, the sensitivity matrix \( \pdv{\vec{x}(t; \vec{p})}{\vec{p}} \) is needed (see\autoref{subsec:derivativeOde}).
			\end{itemize}
		% end
	% end

	\section{Conditioning of Normal Equations}
		Often, the Jacobian of the objective function of linear least squares \( \mat{J} \) is ill-conditioned, i.e. \( \cond \mat{J} \) is high, causing round error to be increased. In extreme cases, \( \mat{J} \) does not has full rank, i.e. \( \cond \mat{J} = \infty \). In this case, the solution \( \vec{d} \) of the normal equations
		\begin{align*}
			\mat{J} \mat{J}^T \vec{d} = -\mat{J}\vec{r}
		\end{align*}
		is not unique! Common reasons for ill-conditioning are:
		\begin{itemize}
			\item "too few" measurements \\ E.g. the measurements are not "dense enough".
			\item not the "correct" measurements \\ E.g. the measurements to not depend or weakly depend on the optimization variables.
			\item System model does not fit to the measurements (it is incompatible). Then either the measurements or the model is wrong.
		\end{itemize}
	% end

	\section{Result Interpretation}
		\subsection{Common Problems}
			Common problems if the residuals are still high after the optimization terminates:
			\begin{itemize}
				\item Derivatives are not precise enough.
				\item The optimization method is not suitable for the problem, e.g. if the method cannot handle inexact function evaluations or has problems with local minima of \(\varphi_2\).
				\item In parameter estimation settings,
					\begin{itemize}
						\item the system model and measurements might be incompatible (wrong measurements) or
						\item the system model and the physical might be incompatible (wrong model).
					\end{itemize}
			\end{itemize}
		
			Common problems for small residuals:
			\begin{itemize}
				\item Some optimization variables might not be uniquely determined.
				\item Too less measurements, variables are not unique in general (e.g. linearly dependent in the model).
			\end{itemize}
		
			Practical aspects:
			\begin{itemize}
				\item Scaling/balancing of the variables: By the transformation \( p_i \to \hat{p}_i = s_i p_i \), \( s_i = \const > 0 \), the derivative changes to \( \pdv{\varphi}{\hat{p}_i} = \pdv{\varphi}{p_i} \frac{1}{s_i} \).
				\item Scaling of the residuals by weights \( w_i > 0 \): \( \varphi(\vec{p}) = \frac{1}{2} \sum_{i = 1}^{n_r} w_i r_i^2(\vec{p}) \)
			\end{itemize}
		% end
		
		\subsection{The Covariance Matrix}
			Assuming the measurement errors \( \varepsilon_i \) (in the measurements \(r_i\)) are normally distributed with zero mean and constant variance \(\sigma^2\). Then the solution of the linear least squares problem is a maximum likelihood estimator for the parameters! The covariance matrix \(\mat{V}\) is then given by \( \mat{V} = \sigma^2 \big(\mat{J} \mat{J}^T\big)^{-1} \) where the variance can be approximated by
			\begin{align*}
				\sigma^2 \approx \frac{\lVert \vec{r}(\vec{p}^\ast) \rVert^2}{n_r - n_p}
			\end{align*}
			and the mean of the residual squares is given as
			\begin{align*}
				\E\big[ \lVert \vec{r}(\vec{p}^\ast) \rVert^2 \big] = (n_r - n_p) \sigma^2
			\end{align*}
			Hence, a bad conditioning of \(\mat{J}\) implies high variance!
		% end
	% end

	\section{Optimal Experimental Design}
		Goal of \emph{optimal experimental design} is a good conditioning of the parameter identification problem, i.e. the optimal experimental parameters \( \vec{s}^\ast \) is a solution of the optimization problem
		\begin{align*}
			\min_{\vec{s}} &\, \phi_\mathrm{exp}\big(\mat{V}(\vec{s})\big)
		\end{align*}
		where \(\mat{V}\) is the covariance matrix. Some objective \(\phi_\mathrm{exp}\) are:
		\begin{enumerate}
			\item Determinant of \(\mat{V}\).
			\item Trace mean, i.e. \( \tr \mat{V} / n_p \)
			\item Biggest eigenvalue of \(\mat{V}\)
			\item Absolute length of the biggest confidence interval.
			\item Conditional number.
		\end{enumerate}
	% end

	\section{Examples} % 6.39
		\todo{Content}

		\subsection{Parameter-Dependent Vehicle Dynamics} % 6.39, 6.40
			\todo{Content}

			\subsubsection{Simulated Measurements} % 6.41, 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Real Measurements} % 6.44
				\todo{Content}
			% end

			\subsubsection{Comparison} % 6.45
				\todo{Content}
			% end
		% end

		\subsection{Parameter Estimation for "BioBiped"} % 6.47, 6.48, 6.49
			\todo{Content}
		% end
	% end
% end

\chapter{Minimization of Functionals}
	In the setting of \emph{variational problems}, the unknown \(\vec{x}\) is a function of \(t\) (where \(t\) is the independent variable) and the objective is a functional \( J[\vec{x}] \) of integral type:
	\begin{align*}
		J[\vec{x}] = \int\! L\big( \vec{x}(t), \dot{\vec{x}}(t), t \big) \dd{t}
	\end{align*}
	the solution \( \vec{x}^\ast \) must fulfill given constraints, e.g.:
	\begin{itemize}
		\item Initial and end conditions (boundary conditions)
		\item Inequality constraints
		\item Integral-type constraints
		\item Differential equations
	\end{itemize}
	If differential equations are given, the problem is called an \emph{optimal control problem}.

	\section{Euler-Lagrange Equation} % 7.5, 7.6, 7.9
		Given an optimization problem
		\begin{align*}
			\min_{\vec{x}} &\, J[\vec{x}],\quad J[\vec{x}] = \int_a^b\! L\big(\vec{x}(t), \dot{\vec{x}}, t\big) \dd{t} \\
			\mathrm{subject~to} &\,
				\begin{alignedat}{2}
					\vec{x}(a) &= \vec{x}_a \\
					\vec{x}(b) &= \vec{x}_b
				\end{alignedat}
		\end{align*}
		where \( \vec{x} : \R \to \R^{n_x} \), \( L : \R^{n_x} \times \R^{n_x} \times R \to R \) are two times continuously differentiable.
		
		A \emph{stationary} solution \( \vec{x}^\ast \), i.e. a solution that is a minimum candidate, has to fulfill the \emph{Euler-Lagrange Equation}:
		\begin{align*}
			\pdv{L}{x_i} - \dv{t} \pdv{L}{\dot{x}_i} = 0,\quad i = \dotsrange{1}{n_x}
		\end{align*}
		Additionally, the boundary conditions \( \vec{x}(a) = \vec{x}_a \), \( \vec{x}(b) = \vec{x}_b \) must be fulfilled, yielding a second-order ordinary boundary value problem.

		\subsection{Example} % 7.10
			As of the \emph{Hamilton's principle} \(\vec{x}(t)\) between \(t_1\) and \(t_2\) is a stationary point of the action functional
			\begin{align*}
				\int_{t_1}^{t_2} \! L\big(\vec{x}(t), \dot{\vec{x}}(t), t\big) \dd{t}
			\end{align*}
			where \(L\) is the Lagrangian of the system, i.e. the difference of kinetic and potential energy:
			\begin{align*}
				L = T - V
			\end{align*}
			
			For a ball with mass \(m\) and gravity acceleration \(g\) that is thrown into the air in a straight line (i.e. one-dimensional), the kinetic and potential energy are given as:
			\begin{align*}
				T = \frac{m}{2} \dot{x}^2 \qquad\qquad V = mgx
			\end{align*}
			The Lagrangian then is \( L = T - V = \frac{m}{2} \dot{x}^2 - mgx \). Plugging that in the Euler-Lagrange equations yields a second-order differential equation for the movement of the ball:
			\begin{align*}
				\pdv{L}{x} = -mg \qquad \pdv{L}{\dot{x}} = mx \qquad \dv{t} \pdv{L}{\dot{x}} = m\ddot{x} \quad\implies\quad -mg - m\ddot{x} = 0 \quad\iff\quad \ddot{x} = -g
			\end{align*}
			Solving this differential equation with the initial values \( x(0) = 0 \), \( \dot{x}(0) = \dot{x}_0 \) yields the equation of movement of the ball:
			\begin{align*}
				x(t) = -\frac{g}{2} t^2 + \dot{x}_0 t
			\end{align*}
			As expected, this equation is a perfect parabola w.r.t. time! The same can be done in two dimensions.
		% end

		\subsection{Notes}
			\begin{itemize}
				\item The Euler-Lagrange equations are in general not tractable. Hence, numerical methods have to be used.
				\item The equations can be extended to solutions with non-differentiabilities, (in-) equality constraints, integral-type constraints, \dots
				\item Also, second-order necessary conditions can be formulated.
			\end{itemize}
		% end

		\subsection{Derivation} % 7.6, 7.7, 7.8, 7.9
			\todo{Content}
		% end
	% end
% end

\chapter{Optimale steuerung dynamischer Systeme} % 8.1
	\todo{Content}

	\subsection{Einleitung} % N/A
		\subsection{Zeitkontinuierliche Systeme} % 8.2, 8.3
			\todo{Content}
		% end

		\subsection{Allgemeine Problemstellung} % 8.4, 8.5
			\todo{Content}
		% end
	% end

	\section{Notwendige Bedingungen fÃ¼r ein Minimum des Basisproblems} % 8.6, 8.7, 8.8
		\todo{Content}

		\subsection{Notwendige Bedingungen erster Ordnung} % 8.20, 8.21, 8.22, 8.23, 8.27, 8.31
			\todo{Content}
		% end

		\subsection{Notwendige Bedingung zweiter Ordnung (Legendre-Clebsch Bedingung)} % 8.28, 8.31
			\todo{Content}
		% end

		\subsection{Beispiel} % 8.12
			\todo{Content}
		% end

		\subsection{Anwendung: Optimale Robotersteuerung} % 8.13
			\todo{Content}

			\subsubsection{Die Roboterdynamik} % 8.14, 8.15
				\todo{Content}
			% end

			\subsubsection{Optimale Steuerung} % 8.16, 8.17
				\todo{Content}
			% end

			\subsubsection{GÃ¼tekriterien} % 8.18
				\todo{Content}
			% end

			\subsubsection{Notwendige Bedingungen} % 8.24, 8.25, 8.26, 8.29, 8.30
				\todo{Content}
			% end
		% end
	% end

	\section{Bang-Bang und SingulÃ¤re Steuerung} % 8.32, 8.33, 8.34
		\todo{Content}

		\subsection{SingulÃ¤re Steuerung} % 8.35, 8.36
			\todo{Content}

			\subsubsection{Notwendige Bedingungen fÃ¼r singulÃ¤re Steuerung} % 8.37, 8.38
				\todo{Content}
			% end
		% end

		\subsection{Anwendung: Zeitminimale Robotersteuerung} % 8.39, 8.40, 8.41
			\todo{Content}

			\subsubsection{Eigenschaften} % 8.42
				\todo{Content}
			% end
		% end

		\subsection{Bemerkungen} % 8.43
			\todo{Content}
		% end
	% end

	\section{Die Wertefunktion} % 8.44, 8.45, 8.47, 8.51
		\todo{Content}

		\subsection{Hamilton-Jacobi-Bellman Gleichung} % 8.50
			\todo{Content}

			\subsubsection{Herleitung} % 8.46, 8.47, 8.48, 8.49, 8.50
				\todo{Content}
			% end
		% end

		\subsection{Bemerkungen} % 8.52
			\todo{Content}
		% end
	% end

	\section{ZustandsbeschrÃ¤nkungen} % 8.53, 8.54, 8.55
		\todo{Content}

		\subsection{Gemischte UngleichungsbeschrÃ¤nkungen} % 8.56, 8.57
			\todo{Content}

			\subsubsection{Notwendige Bedingungen} % 8.58, 8.59, 8.60, 8.61
				\todo{Content}
			% end
		% end

		\subsection{Reine ZustandsungleichungsbeschrÃ¤nkungen} % 8.62, 8.63, 8.65
			\todo{Content}

			\subsubsection{Ordnung einer ZustandsbeschrÃ¤nkung} % 8.66
				\todo{Content}
			% end

			\subsubsection{Erweiterte Hamiltonfunktion} % 8.67, 8.68
				\todo{Content}
			% end

			\subsubsection{Maximumprinzip} % 8.69, 8.70, 8.71, 8.72
				\todo{Content}
			% end

			\subsubsection{Bemerkungen} % 8.73, 8.74
				\todo{Content}
			% end
		% end

		\subsection{Beispiele} % 8.75
			\todo{Content}

			\subsubsection{Optimale Robotersteuerung} % 8.76
				\todo{Content}
			% end

			\subsubsection{Energieminimierungsproblem} % 8.77, 8.78, 8.79, 8.80, 8.81, 8.82, 8.83, 8.84, 8.85, 8.86
				\todo{Content}
			% end
		% end

		\subsection{Zusammenfassung} % 8.87, 8.88
			\todo{Content}
		% end
	% end
% end

\chapter{Berechnung optimaler Trajektorien} % 9.2
	\todo{Content}

	\section{Erste Berechnungsverfahren} % 9.2
		\todo{Content}

		\subsection{Dynamische Programmierung} % 9.3
			\todo{Content}
		% end

		\subsection{Gradientenverfahren} % 9.4
			\todo{Content}
		% end
	% end

	\section{Indirekte Verfahren} % 9.5, 9.6, 9.7
		\todo{Content}
	% end

	\section{Direkte Verfahren} % 9.8, 9.9, 9.10
		\todo{Content}

		\subsection{Direkte Kollokationsverfahren} % 9.11, 9.25
			\todo{Content}

			\subsubsection{Erste Diskretisierung} % 9.12, 9.13, 9.14
				\todo{Content}

				\paragraph{Modellalgorithmus} % 9.15
					\todo{Content}
				% end

				\paragraph{Bemerkungen} % 9.16
					\todo{Content}
				% end
			% end

			\subsubsection{Zweite Diskretisierung} % 9.17, 9.18
				\todo{Content}

				\paragraph{Gauss- und Lobatto-Punkte} % 9.19, 9.20, 9.21
					\todo{Content}
				% end

				\paragraph{Resultierendes NLP} % 9.21
					\todo{Content}
				% end
			% end

			\subsubsection{Diskretisierung von UngleichungsbeschrÃ¤nkungen} % 9.22
				\todo{Content}
			% end

			\subsubsection{Konvergenzeigenschaften} % 9.30, 9.31, 9.32
				\todo{Content}
			% end

			\subsubsection{Validierung der berechneten LÃ¶sung} % 9.33
				\todo{Content}
			% end

			\subsubsection{Wahl der Gitterpunkte} % 9.36, 9.37
				\todo{Content}

				\paragraph{Approximative, segmentweise FehlerschÃ¤tzung} % 9.38
					\todo{Content}
				% end

				\paragraph{Segmentweise FehlerschÃ¤tzung bei zeitkontinuierlichen Nebenbedingungen} % 9.39
					\todo{Content}
				% end

				\paragraph{Segmentweise SchÃ¤tzung des OptimalitÃ¤tsfehlers} % 9.40
					\todo{Content}
				% end
			% end

			\subsubsection{Vorgehensweise} % 9.41
				\todo{Content}
			% end

			\subsubsection{Bemerkungen} % 9.43
				\todo{Content}
			% end

			\subsubsection{Anwendung: Optimale Robotersteuerung} % 9.23, 9.24, 9.26, 9.27, 9.28
				\todo{Content}
			% end

			\subsubsection{Beispiel: Zeitminimale Robotersteuerung} % 9.34, 9.35
				\todo{Content}
			% end
		% end

		\subsection{Direkte SchieÃverfahren} % 9.44, 9.45, 9.46, 9.47
			\todo{Content}
		% end
	% end

	\section{Bemerkungen} % 9.48, 9.49
		\todo{Content}
	% end
% end

\chapter{Optimale RÃ¼ckkopplungssteuerung} % 10.1, 10.2, 10.3
	\todo{Content}

	\section{Feedforward Control} % 10.4, 10.5
		\todo{Content}
	% end

	\section{Sollwerttrajektorien-Folgeregelung} % 10.6, 10.7, 10.8, 10.9, 10.10
		\todo{Content}
	% end

	\section{Optimal Feedback Control} % 10.11, 10.12
		\todo{Content}
	% end

	\section{Linear-Quadratische Optimalsteuerung} % 10.13, 10.14
		\todo{Content}

		\subsection{Notwendige Bedingungen} % 10.15, 10.16, 10.17
			\todo{Content}
		% end

		\subsection{Zeitinvariante Dynamik} % 10.18, 10.19
			\todo{Content}
		% end
	% end

	\section{Benachbarte Extremalen} % 10.20, 10.21, 10.22
		\todo{Content}

		\subsection{Indirekte Verfahren} % 10.23, 10.24, 10.25, 10.26
			\todo{Content}
		% end

		\subsection{Direkte Verfahren} % 10.27
			\todo{Content}
		% end

		\subsection{ModellgestÃ¼tzte, prÃ¤diktive Regelung} % 10.28, 10.29
			\todo{Content}
		% end
	% end

	\section{Numerische Synthese der nichtlinearen RÃ¼ckkopplungssteuerung} % 10.30, 10.31, 10.32, 10.33
		\todo{Content}
	% end
% end

\chapter{Weitere Themen der Optimalen Steuerung} % 11.1
	\todo{Content}

	\section{Inverse Optimalsteuerung} % 11.2, 11.3
		\todo{Content}
	% end

	\section{Differentialspiele} % 11.4, 11.5
		\todo{Content}

		\subsection{Nichtkooperative Nullsummen-Differentialspiele} % 11.6, 11.7
			\todo{Content}

			\subsubsection{Beispiel} % 11.7
				\todo{Content}
			% end
		% end

		\subsection{Notwendige Bedingungen} % 11.8
			\todo{Content}
		% end
	% end

	\section{Lernverfahren und Optimierung} % 11.9
		\todo{Content}

		\subsection{Einleitung} % 11.10, 11.15, 11.17, 11.18
			\todo{Content}
		% end

		\subsection{Grundlagen} % 11.19, 11.20, 11.21, 11.22
			\todo{Content}

			\subsubsection{Formales Schema} % 11.23
				\todo{Content}
			% end

			\subsubsection{Gradien Descent} % 11.24
				\todo{Content}
			% end

			\subsubsection{(Tiefe) Neuronale Netze} % 11.25
				\todo{Content}
			% end

			\subsubsection{Grundlegende Fragen} % 11.26
				\todo{Content}
			% end
		% end

		\subsection{BezÃ¼ge zur (optimalen) Steuerung und Regelung} % 11.27, 11.28, 11.29, 11.30
			\todo{Content}

			\subsubsection{Bewertung der Aktionen} % 11.31
				\todo{Content}
			% end

			\subsubsection{Wertefunktion} % 11.32
				\todo{Content}
			% end

			\subsubsection{Erster Ansatz} % 11.33
				\todo{Content}
			% end

			\subsubsection{Lernen der Wertefunktion} % 11.34, 11.35
				\todo{Content}
			% end

			\subsubsection{Kombination aus Wertefunktion und optimaler Steuerung} % 11.36
				\todo{Content}
			% end

			\subsubsection{Bemerkungen} % 11.37
				\todo{Content}
			% end
		% end
	% end
% end
