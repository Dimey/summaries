% !TeX spellcheck = en_US

\chapter{EinfÃ¼hrung} % 1.5
	\todo{Content}

	\section{Beispiele} % 1.6, 1.7, 1.8, 1.10, 1.11
		\todo{Content}
	% end

	\section{Fragestellungen} % 1.12
		\todo{Content}
	% end

	\section{Allgemeine Formulierung eines Optimierungsproblems} % 1.13, 1.14
		\todo{Content}
	% end

	\section{Statische vs. Dynamische Optimierung} % 1.15, 1.16, 1.17, 1.18
		\todo{Content}
	% end

	\section{Klassifizierung von Optimierungsverfahren} % 1.23
		\todo{Content}
	% end

	\section{Typische Struktur} % 1.24
		\todo{Content}
	% end
% end

\chapter{Gradient-Based Optimization without Constraints}
	\section{Solution Characterization}
		This section covers the theoretical results for solving a nonlinear optimization problem using calculus.
	
		\subsection{One-Dimensional Optimization}
			For a one-dimensional function \( \varphi(p) : \R \to \R \) the first-order necessary condition for a minimum is that the derivative of \( \varphi(p) \) w.r.t. the parameter \(p\) vanishes:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} \! = 0
			\end{align*}
			Where \( p^\ast \) denotes the optimal solution, i.e. the minimum.
			
			All solutions that fulfill this condition are \emph{candidates} for a minimum. If \( \varphi \) is twice continuous differentiable, the sufficient condition for a minimum is that the second-order derivative is positive:
			\begin{align*}
				\dv{\varphi(p^\ast)}{p} > 0
			\end{align*}
			Then \(p^\ast\) is called a \emph{strict minimum}. This condition is sufficient, but not necessary! The second-order necessary condition for a minimum is that the second-order derivative is non-negative, i.e. \( \dv{\varphi(p^\ast)}{p} \geq 0 \).
			
			\subsubsection{Possibilities for a Minimum}
				There are three cases for a minimum:
				\begin{itemize}
					\item \(\varphi(p)\) is twice continuously differentiable everywhere
					\item \(\varphi'(p)\) is not continuous everywhere but at \(p^\ast\)
					\item \(\varphi'(p)\) is not continuous everywhere, not even at \(p^\ast\)
				\end{itemize}
				While the latter case is common, it is problematic as the solution can typically not be determined analytically (if a function is not continuous at one point, it is rarely invertible).
			% end
		% end

		\subsection{Multi-Dimensional Optimization}
			For multi-dimensional objective functions \( \varphi : \R^{n_p} \to \R \), where \(n_p\) is the dimensionality of the parameters, the first-order necessary condition is that the gradient vanishes:
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) =
					\begin{bmatrix}
						\pdv{\varphi}{p_1} \\
						\vdots \\
						\pdv{\varphi}{p_{n_p}}
					\end{bmatrix}
				=
					\begin{bmatrix}
						0 \\
						\vdots \\
						0
					\end{bmatrix}
			\end{align*}
			
			If \(\varphi(\vec{p})\) is twice continuously differentiable, the second-order sufficient condition is that the Hessian of \(\varphi(\vec{p})\) is positive definite. Analogous to the one-dimensional case, the second-order necessary condition is that the Hessian is positive semi-definite, i.e.:
			\begin{align*}
				\mat{H}_\varphi(\vec{p}^\ast) =
					\begin{bmatrix}
						\pdv[2]{\varphi}{p_1}        & \cdots & \pdv{\varphi^2}{p_{n_p} p_1} \\
						\vdots                       & \ddots & \vdots                       \\
						\pdv{\varphi^2}{p_1 p_{n_p}} & \cdots & \pdv[2]{\varphi}{p_{n_p}}
					\end{bmatrix}
				> 0
				\quad\text{or respectively}\quad
				\mat{H}_\varphi(\vec{p}^\ast) \geq 0
			\end{align*}

			\paragraph{Example} % 2.7, 2.8, 2.9
				\todo{Content}
			% end
		% end
	% end

	\section{Numerical Gradient-Based Methods}
		\subsection{Starting Point}
			\subsubsection{Structure of Gradient-Based Methods}
				Given a initial approximation \( \vec{p}^{(0)} \), an approximation of the minimum \( \vec{p}^\ast \) is wanted. Gradient-based methods are iteration methods based on the iteration rule
				\begin{align*}
					\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)},\quad k = 0, 1, 2, \cdots
				\end{align*}
				where
				\begin{itemize}
					\item \(\vec{d}^{(k)}\) is the search direction found as the solution of a linear sub problem and
					\item \(\alpha^{(k)}\) is the step size found by a one-dimensional \emph{line search}.
				\end{itemize}
				The iteration terminates once \( \vec{p}^{(k + 1)} \) is "close to" \(\vec{p}^\ast\), e.g. when the gradient nearly vanishes.
			% end

			\subsubsection{Descent Direction}
				Gradient-based methods have to ensure the the local search direction \(\vec{d}^{(k)}\) really is a descent direction (the algorithm shall not "run up the hill"). This property is ensured iff the angle \( \delta \) between the search direction and the gradient \( \grad{\varphi}\big(\vec{}^{(k)}\big) \) greater than \SI{90}{\degree}, i.e.
				\begin{align}
					\cos \delta = \frac{\big( \vec{d}^{(k)} \big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)}{\big\lVert \vec{d}^{(k)} \big\rVert \cdot \big\lVert \grad{\varphi}\big(\vec{p}^{(k)}\big) \big\rVert} < 0 \quad\iff\quad \big( \vec{d}^{(k)} \big)^T \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big) < 0  \label{eq:descentDirection}
				\end{align}
				This is called the "necessary descent condition".
			% end

			\subsubsection{Algorithmic Structure}
				The \autoref{alg:gradientBasedAlgorithmStructure} shows the basic structure of any gradient-based optimization algorithm.
			
				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
					\While{not converged}{
						Determine new search direction: \tabto{6cm} \( \vec{d}^{(k)} \in \R^{n_p} \) \;
						Determine new step size: \tabto{6cm} \( \alpha^{(k)} \in \R^+ \) \;
						Update the approximation: \tabto{6cm} \( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( k \gets k + 1 \) \;
					}
				
					\caption{Algorithmic structure of a gradient-based optimization algorithms.}
					\label{alg:gradientBasedAlgorithmStructure}
				\end{algorithm}
			% end
		% end

		\subsection{Steepest Descent}
			Steepest descent is the straightforward way for getting a search direction. The search direction is just set to the negative of the gradient:
			\begin{align*}
				\vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Often quickly reaches areas around the local minimum.
						\item No second derivatives needed.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Very slow in areas around the local minimum compared to (Quasi-) Newton Methods.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Conjugate Gradient}
			Basic approach for conjugate gradient:
			\begin{align*}
				\vec{d}^{(0)} &= -\grad{\varphi}\big(\vec{p}^{(0)}\big) \\
				\vec{d}^{(k)} &= \text{Component of } -\grad{\varphi}\big(\vec{p}^{(k)}\big) \text{ that is conjugate to } \vec{d}^{(0)}, \vec{d}^{(1)}, \cdots, \vec{d}^{(k - 1)}
			\end{align*}

			For a quadratic objective function
			\begin{align*}
				\varphi(\vec{p}) = \frac{1}{2} \vec{p}^T \mat{A} \vec{p} - \vec{b}^T \vec{p}
			\end{align*}
			with a positive semi-definite matrix \(\mat{A}\) and constant \(\mat{A}\), \(\vec{b}\), the search direction is given as the solution of:
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(j)} = 0,\quad j = 1, \cdots, k - 1
			\end{align*}
			
			With an optimal step size \( \alpha^{(k)} \), i.e.
			\begin{align*}
				\alpha^{(k)} = \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big)
					\quad\implies\quad \alpha^{(k)} = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \mat{A} \vec{d}^{(k)}} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \big(\vec{d}^{(k)}\big)
			\end{align*}
			the minimum of \(\varphi\) is reached in \(n_p\) steps.
			
			The extension for nonlinear objective functions is given in~\autoref{alg:conjugateGradient}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \vec{d}^{(0)} \gets -\grad{\varphi}\big(\vec{p}^{(0)}\big) \) and \( k \gets 0 \) \;
				\While{not converged}{
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( \beta^{(k + 1)} \gets \frac{\big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) \!\big)}{\big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)^T \big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\big)} \) \;
					\( \vec{d}^{(k + 1)} \gets -\grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \beta^{(k + 1)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}
				
				\caption{Conjugate Gradient for nonlinear Objective Function.}
				\label{alg:conjugateGradient}
			\end{algorithm}
		
			\begin{itemize}
				\item Exact line search necessary.
				\item Different variants of GC-algorithms mainly distinguish in the choice of of \( \beta^{(k)} \).
				\item Advantages:
					\begin{itemize}
						\item Faster then steepest descent.
						\item No explicit storing of the Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \) necessary.
						\item No explicit matrix-vector multiplication.
						\item Useful even for extreme high dimensions \( n_p \).
						\item Exact for quadratic objectives \( \varphi(\vec{p}) \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item A lot slower then (Quasi-) Newton Methods.
						\item In general not useful for optimizing simulation models.
					\end{itemize}
			\end{itemize}
		% end

		\subsection{Newton Method}
			Assuming the approximation of each iteration, \( \vec{p}^{(k)} \), is close to the minimum \( \vec{p}^\ast \), the gradient \( \grad{\varphi}(\vec{p}^\ast) \) can be taylor-expanded around \( \vec{p}^{(k)} \):
			\begin{align*}
				\grad{\varphi}(\vec{p}^\ast) \overset{T\big(\!\vec{p}^{(k)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \big( \vec{p}^\ast - \vec{p}^{(k)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			By leaving our the higher order terms the search direction \( \vec{d}^{(k)} \coloneqq \vec{p}^\ast - \vec{p}^{(k)} \) is given by the solution of the system of linear equations
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The realization is shown in~\autoref{alg:newtonMethod}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( k \gets 0 \) \;
				\While{not converged}{
					Solve \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
					\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
					\( k \gets k + 1 \) \;
				}
				
				\caption{Newton Method}
				\label{alg:newtonMethod}
			\end{algorithm}
		
			When plugging the search direction into the necessary descent condition~\eqref{eq:descentDirection}
			\begin{align*}
				\big(\vec{d}^{(k)}\big)^T \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
					= -\Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)^T\Big) \Big(\!\mat{H}_\varphi\big(\vec{p}^{(k)}\big)\!\Big)^{-1} \Big(\!\grad{\varphi}\big(\vec{p}^{(k)}\big)\!\Big)
					< 0
			\end{align*}
			it is clear that this is only fulfilled iff the Hessian is positive definite. But this is only the case in a region around the minimum! If the approximation is far away from the minimum, the search direction might also be an ascent direction causing the Newton method to diverge. There are two main solutions to this problem:
			\begin{enumerate}
				\item If the Hessian is not positive definite, replace it by an identity matrix. That is, set the search direction to the steepest descent.
				\item Regularize the equation system with a weight \( \nu > 0 \) such that the new matrix is positive definite (this "rotates" the matrix in the direction of the steepest descent such that the new search direction always fulfills the descent condition):
			\end{enumerate}
			\begin{align*}
				\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \nu \mat{I} \Big) \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item Near to strong local minima of twice continuous differentiable objective, the Newton method is quadratic convergent.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Computationally expensive as a linear system has to be solved in every iteration.
						\item Not only first, but also second-order derivatives have to be available. This is a big disadvantage:
							\begin{itemize}
								\item In practice, the first derivative is rarely and the second derivative is never available.
								\item Even a single wrong component in the gradient or the Hessian destroys the quadratic convergence.
							\end{itemize}
					\end{itemize}
			\end{itemize}

			\subsubsection{Availability of Second-Order Derivatives}
				The obvious idea is to approximate the Hessian using finite differences. The approximated Hessian is then given as
				\begin{align*}
					\mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T )  \label{eq:approxHessianSymm}
				\end{align*}
				where \(\tilde{\mat{H}}\) is given by
				\begin{align*}
					\tilde{\mat{H}}_i = \pdv{p_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big) \approx \frac{1}{h_i} \Big( \grad{\varphi}\big(\vec{p}^{(k)} + h_i \vec{e}_i\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \Big)
				\end{align*}
				where \( \tilde{\mat{H}}_i \) is the \(i\)-th column of \(\tilde{\mat{H}}\). The equation~\eqref{eq:approxHessianSymm} is used to force the Hessian to be symmetric.
				
				Problems:
				\begin{itemize}
					\item The Hessian \( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) = \frac{1}{2} \big( \tilde{\mat{H}} + \tilde{\mat{H}}^T ) \) is not necessarily positive definite.
					\item In every iteration the Gradient has to be evaluated \(n_p\) times more.
					\item The linear system still needs to be solved.
					\item Only useful for high-dimensional problems with sparse gradients!
				\end{itemize}
				Another possibility are \emph{Quasi-Newton Methods}.
			% end
		% end

		\subsection{Quasi-Newton Methods}
			Quasi-Newton methods are equivalent to the Newton method, however, the Hessian (or its inverse) is approximated by a positive definite matrix
			\begin{align*}
				\hat{\mat{H}}^{(k)} \approx \mat{H}_\varphi\big(\vec{p}^{(k)}\big)
			\end{align*}
			that is updated in every iteration. This yields a lot of advantages over the classic Newton method:
			\begin{itemize}
				\item Only first-order derivatives needed.
				\item As \( \hat{\mat{H}} \) constructed positive definite, the descent condition is fulfilled anytime.
				\item If the inverse Hessian is directly approximated, only \( \mathcal{O}(n_p^2) \) multiplications instead of \( \mathcal{O}(n_p^3) \) for solving the linear system.
			\end{itemize}
		
			But how to do the Hessian update? By Taylor-expanding the gradient \( \grad{\varphi}\big(\vec{p}^{(k)}\big) \) around \( \vec{p}^{(k + 1)} \)
			\begin{align*}
				\grad{\varphi}\big(\vec{p}^{(k)}\big) \overset{T\big(\!\vec{p}^{(k + 1)}\!\big)}{=} \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) + \mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \big( \vec{p}^{(k)} - \vec{p}^{(k + 1)} \big) + \cdots \overset{!}{=} \vec{0}
			\end{align*}
			and cutting off the higher-order terms, the following approximation holds:
			\begin{align*}
				\mat{H}_\varphi\big(\vec{p}^{(k + 1)}\big) \vec{d}^{(k)} \approx \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			The approximation of the Hessian must therefore fulfill the \emph{secant condition}
			\begin{align*}
				\tilde{\mat{H}}^{(k + 1)} \vec{d}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big)
			\end{align*}
			
			There exist a lot of different approaches for doing the Hessian updates \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \mat{U}{(k)} \) for rank-1 or rank-2 matrices \(\mat{U}^{(k)}\):
			\begin{itemize}
				\item Approach for rank-1 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T \)
				\item Approach for rank-2 corrections: \tabto{6cm} \( \tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)} + \beta_1 \vec{u} \vec{u}^T + \beta_2 \vec{v} \vec{v}^T \)
			\end{itemize}
			The vectors \( \vec{u}, \vec{v} \in \R^{n_p} \) and scalars \( \beta_1, \beta_2 \in \R \) must have to be chosen such that \( \tilde{\mat{H}}^{(k + 1)} \) is
			\begin{itemize}
				\item positive definite,
				\item symmetric,
				\item fulfills the secant condition and
				\item adding up the matrices is efficient and robust.
			\end{itemize}

			\subsubsection{BFGS-Update}
				The most known rank-2 update for the Hessian is the \emph{BFGS-Update}\footnote{"BFGS" stands for the authors Broyden, Fletcher, Goldfarb and Shanno.}
				\begin{align*}
					\vec{u} & = \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} &   \beta_1 & = -\frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \\
					\vec{v} & = \vec{g}^{(k)}                       &   \beta_2 & = \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}}
				\end{align*}
				where \( \vec{g}^{(k)} = \grad{\varphi}\big(\vec{p}^{(k + 1)}\big) - \grad{\varphi}\big(\vec{p}^{(k)}\big) \). Plugging that into the general approach for rank-2 updates yields the update rule for BFGS-approximations:
				\begin{align*}
					\tilde{\mat{H}}^{(k + 1)} = \tilde{\mat{H}}^{(k)}
							- \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T
							+ \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T
				\end{align*}
				\begin{itemize}
					\item The direct approximation of the Hessian inverse is not really robust (e.g. for a non-optimal step size rule).
					\item A better alternative is to directly approximate a useful factorization, e.g. the Cholesky decomposition. This is more robust and equally efficient (\( \mathcal{O}(n_p^2) \)).
				\end{itemize}
			
				The pseudo code for the BFGS update is shown in~\autoref{alg:bfgs}.
				
				\begin{algorithm}  \DontPrintSemicolon
					\textbf{Initialization:} Choose an initial approximation \(\vec{p}^{(0)}\), set \( \tilde{\mat{H}}^{(0)} = \mat{I} \) and \( k \gets 0 \) \;
					\While{not converged}{
						Solve \( \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} = -\grad{\varphi}\big(\vec{p}^{(k)}\big) \) for \( \vec{d}^{(k)} \) \;
						\( \alpha^{(k)} \gets \arg\min_{\alpha} \varphi\big( \vec{p}^{(k)} + \alpha \vec{d}^{(k)} \big) \) \;
						\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \) \;
						\( \tilde{\mat{H}}^{(k + 1)} \gets \tilde{\mat{H}}^{(k)} - \frac{1}{\big(\vec{d}^{(k)}\big)^T \tilde{\mat{H}}^{(k)} \vec{d}^{(k)}} \tilde{\mat{H}}^{(k)} \vec{d}^{(k)} \big(\tilde{\mat{H}}^{(k)} \vec{d}^{(k)}\big)^T + \frac{1}{\big(\vec{g}^{(k)}\big)^T \vec{d}^{(k)}} \vec{g}^{(k)} \big(\vec{g}^{(k)}\big)^T \) \;
						\( k \gets k + 1 \) \;
					}
					
					\caption{Quasi-Newton Method with BFGS Update.}
					\label{alg:bfgs}
				\end{algorithm}
			% end
		% end

		\subsection{Comparison} % 2.29, 2.30, 2.31, 2.32, 2.33, 2.34, 2.35, 2.36
			\todo{Content}
			
			\begin{itemize}
				\item 
			\end{itemize}
		% end
		
		\subsection{Notes and Discussion}
			\begin{itemize}
				\item The convergence of gradient-based methods can be shown under weak preconditions.
				\item As the search direction is only a local descent direction, gradient-based algorithms only yields local minima.
				\item There is no algorithm that can guarantee to find the global minimum!
				\item Some approaches for determining a global minimum:
					\begin{itemize}
						\item Choose the initialization well, i.e. close to the global minimum.
						\item Execute the algorithm multiple times with different starting points.
						\item Validate the solution against properties of the original problem.
						\item Execute direct search methods beforehand to find promising regions for the local minimum search.
					\end{itemize}
			\end{itemize}
		
			\begin{itemize}
				\item Advantages:
					\begin{itemize}
						\item If gradient-based algorithms converge, they converge utterly fast.
						\item Efficient also for high-dimensional problems, i.e. a large \( n_p \).
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Only applicable for functions that are differentiable almost everywhere.
						\item Require gradient information exact up to four to eight decimal points.
						\item Convergence to a local minimum near the initialization \( \vec{p}^{(0)} \).
						\item Require some expert knowledge.
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Step Size Rules, Line Search}
		In every iteration of gradient-based algorithms, the step size has to be determined by minimizing the one-dimensional function:
		\begin{align*}
			\psi(\alpha) = \varphi\big(\vec{p}^{(k)} + \alpha \vec{d}^{(k)}\big)
		\end{align*}
		As the necessary first-order condition for a minimum, the derivative w.r.t. \(\alpha\) has to vanish:
		\begin{align*}
			\dv{\psi\big(\alpha^{(k)}\big)}{\alpha} = \dv{\alpha} \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) = \Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \vec{d}^{(k)} \overset{!}{=} 0
		\end{align*}
		Thus the gradient of \(\varphi\) at the minimum \(\alpha^{(k)}\) as to be orthogonal to the search direction \(\vec{d}^{(k)}\). Intuitively, the optimal step size has to be chosen such that the iteration step cannot go any further without ascending again ("hitting an ascending contour line").
		
		Goal of the line search is to reach the minimum of \(\psi\) with least invocations of \(\psi\) as possible. Most of the existing search methods can be classified into
		\begin{itemize}
			\item \emph{Polynomial approximation}, e.g. quadratic or cubic interpolation
			\item \emph{Direct search methods}, e.g. Fibonacci-search, golden ratio search
			\item \emph{Optimal vs. non-optimal search methods}, e.g. by finding an improvement but not the minimum
			\item Usage of the gradient information \( \psi' \) or not.
		\end{itemize}
	
		Requirements:
		\begin{itemize}
			\item Finding the \(\alpha^{(k)}\) with a minimal value of \(\psi\).
			\item Do not waste too much computation time on the line search.
		\end{itemize}
		In general, an exact line search requires lots of \(\psi\)-evaluations. But how far does \(\psi\) need to be reduced in order to guarantee convergence? In general, the condition \( \varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) < \varphi\big(\vec{p}^{(k)}\big) \) is not enough!
		
		\subsection{Inexact Line Search}
			Procedure: Generate and inspect a series of candidates for \(\alpha^{(k)}\) and terminate once one of the candidates fulfills specific criteria, e.g. the Armijo rule or Wolfe conditions.
			
			\subsubsection{Armijo Rule}
				The \emph{Armijo rule} guarantees a sufficient reduction in \(\varphi\):
				\begin{align*}
					\varphi\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big) \leq \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = \varphi\big(\vec{p}^{(k)}\big) + c_1 \alpha^{(k)} \psi'(0)
				\end{align*}
				Where \( 0 < c_1 < 1 \) is any constant, e.g. \( c_1 = 10^{-4} \).
				
				Hence, the minimal reduction has to be proportional to \(\alpha^{(k)}\) and the derivative \( \psi'(0) \).
			% end
			
			\subsubsection{Curvature Condition}
				But a sufficient descent condition is not enough as the step sizes must not be too small (otherwise progress would stop). Thus a second condition has to be employed, the \emph{curvature condition} that requires a minimum curvature on \(\psi\):
				\begin{align*}
					\Big(\!\grad{\varphi}\big(\vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)}\big)\!\Big)^T \geq c_2 \cdot \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)} = c_2 \psi'(0) \quad\iff\quad \psi'\big(\vec{p}^{(k)}\big) \geq c_2 \psi'(0)
				\end{align*}
				Where \( c_1 < c_2 < 1 \) is any constant, e.g. \( c_2 = 0.9 \).
			% end
			
			\subsubsection{Wolfe and Goldstein Conditions}
				Combining the Armijo rule and the curvature condition yields the Wolfe conditions that guarantee both a minimal reduction and a minimal curvature. They are especially useful for Quasi-Newton methods as the Wolfe conditions are scale invariant, i.e. independent of
				\begin{itemize}
					\item multiplying \(\varphi\) with any constant and
					\item affine transformations of \(\vec{p}\).
				\end{itemize}
			
				There are other possibilities are, e.g. the \emph{Goldstein conditions}
				\begin{align*}
					\varphi\big(\vec{p}^{(k)}\big) + (1 - c) \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
						\leq \varphi\big( \vec{p}^{(k)} + \alpha^{(k)} \vec{d}^{(k)} \big)
						\leq \varphi\big( \vec{p}^{(k)} \big) + c \alpha^{(k)} \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d}^{(k)}
				\end{align*}
				with any \( 0 < c < 1/2 \), that are useful for Newton, but not for Quasi-Newton methods.
			% end
		% end
		
		\subsection{Notes}
			\begin{itemize}
				\item For gradient-based methods a step size \( \alpha^{(k)} > 1 \) is in general not useful because:
					\begin{itemize}
						\item The search direction \( \vec{d}^{(k)} \) is determined using a linear or quadratic Taylor approximation.
						\item The Taylor approximation is only valid in a small region around the current approximation \( \vec{p}^{(k)} \), i.e. for \( 0 < \alpha^{(k)} \leq 1 \).
					\end{itemize}
				\item The local quadratic or super-linear convergence of Newton-type methods is visible in practice as the last step can be executed with full step size \( \alpha^{(k)} = 1 \).
			\end{itemize}
		% end
	% end

	\section{Trust Region Methods}
		Gradient-based methods with line search determine a fixed search direction and adjust the step size \( \alpha^{(k)} \) according to that search direction to reach global convergence.
		
		Another approach is to determine both the length and direction of \( \vec{d}^{(k)} \). The iteration step then becomes
		\begin{align*}
			\vec{p}^{(k + 1)} = \vec{p}^{(k)} + \vec{d}^{(k)}
		\end{align*}
		without any explicit step size. The length and direction of \( \vec{d}^{(k)} \) are then determined as a solution of the quadratic sub-problem
		\begin{align*}
			\min_{\vec{d} \in \R^{n_p}} &\, \Big(\! \grad{\varphi}\big(\vec{p}^{(k)}\big) \!\Big)^T \vec{d} + \frac{1}{2} \vec{d}^T \mat{H}_\varphi\big(\vec{p}^{(k)}\big) \vec{d} \\
			\mathrm{s.t.} &\,
				\begin{alignedat}{2}
					\lVert \vec{d} \rVert_2 & \leq \delta
				\end{alignedat}
		\end{align*}
		where \(\delta\) describes the area around the current approximation \( \vec{p}^{(k)} \) where the quadratic approximation of \( \varphi\big(\vec{p^{(k)}} + \vec{d}\big) \) makes sense, i.e. the \emph{trust region}.
		
		The value of \(\delta\) is extremely important for the efficiency of the method.
		\begin{itemize}
			\item If \(\delta\) is too small, opportunities for large steps are missed.
			\item If \(\delta\) is too large, the minimum of the quadratic approximation might be far off the minimum of the objective if the Hessian is indefinite or negative definite.
		\end{itemize}
		It is possible to add a regularization parameter \(\beta \geq 0\) to the quadratic approximation
		\begin{align*}
			\Big( \mat{H}_\varphi\big(\vec{p}^{(k)}\big) + \beta \mat{I} \Big) \vec{d} = -\grad{\varphi}\big(\vec{p}^{(k)}\big)
		\end{align*}
		such that the matrix is positive semidefinite. The solution of this regularized problem also solves the trust region problem if either \( \beta = 0 \), \( \lVert \vec{d} \rVert \leq \delta \) or \( \beta \geq 0 \), \( \lVert \vec{d} \rVert = \delta \).
	% end

	\section{Rate of Convergence}
		A common criteria to measure the performance of a gradient method are \emph{rates of convergence}. These provide information on how fast an algorithm converges, i.e. how fast \( \vec{p}^{(k)} \to \vec{p}^\ast \) or \( \big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert \to 0 \).
		
		\textbf{Definition:}
		Let \( \big(\{\, \vec{p}^{(k)} \,\big\} \) be the series of approximations produced by an optimization method. Then this series has rate of convergence \(r\) if \(r\) is the greatest positive number such that the limit
		\begin{align*}
			0 \leq \liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert^r} = \gamma < \infty
		\end{align*}
		converges (where \( \vec{p}^\ast = \liminfty[k] \vec{p}^{(k)} \)). If \( r = 1 \), then \( \gamma < 1 \) has to hold for the method to converge.
		
		A sequence is said to converge superlinearly if
		\begin{align*}
			\liminfty[k] \frac{\big\lVert \vec{p}^{(k + 1)} - \vec{p}^\ast \big\rVert}{\big\lVert \vec{p}^{(k)} - \vec{p}^\ast \big\rVert} = 0
		\end{align*}
		holds. Even though technically this condition holds for \( r > 1 \), in practice only methods with \( 1 < r < 2 \) are said to converge superlinearly (e.g. for \(r = 2\), the sequence is called to converge quadratically).

		\paragraph{Examples} % 2.53
			\todo{Content}
		% end
		
		\subsection{Gradient-Based Methods}
			Under ideal conditions (i.e. \(\varphi\) is twice continuously differentiable and \( \mat{H}_\varphi(\vec{p}^\ast) \) is positive definite), all of the following hold:
			\begin{itemize}
				\item Steepest descent is (locally) linearly convergent (with exact line search).
				\item The Newton method is (locally) quadratically convergent.
				\item Quasi-Newton methods with BFGS-update are (locally) superlinearly convergent (for inexact line search using the Wolfe conditions).
			\end{itemize}
			But: Even a single wrong component in the Hessian reduced the quadratic convergence of the Newton method to linear convergence!
		% end
	% end
% end

\chapter{Gradient-Free Optimization without Constraints}
	This chapter covers different types of sampling methods (direct search methods):
	\begin{enumerate}
		\item Metaheuristics (random search),
		\item Deterministic Sampling Methods (pattern search) and
		\item Surrogate optimization.
	\end{enumerate}
	These optimization methods only use evaluations of the objective \(\varphi\) and do not user gradient information (neither analytically nor using numerical differentiation). That is, \(\varphi\) is used as a "Black Box" for function evaluations.

	Even for \(\varphi\) that are not differentiable and have lots of local minima, gradient-free algorithms are remarkably robust, but can also fail fast.

	\section{Introduction}
		The objective \(\varphi\) that is to be optimized often has suboptimal properties, e.g.:
		\begin{itemize}
			\item the evaluation is noisy
			\item not differentiable
			\item high computation time for evaluation (e.g. when a simulation has to be run for each evaluation)
		\end{itemize}
	
		But gradient-free techniques have a wide range of applications, e.g. in automobile, aerospace industry, robotics, financial, etc. The goal is to reduce the objective function as much as possible and find regions in which the objective is differently sensitive w.r.t. changes in the optimization variables.

		\subsection{Simulation-Based Optimization}
			In a simulation-based setting, the objective is evaluated by running a simulation (e.g. by solving a set of differential equations or running a real experiment). This yields a suboptimal setting for optimization, yet a common one:
			\begin{itemize}
				\item Only function values are computable and not gradient information is available.
				\item Source code of the optimization is often not available. Hence, no information about how the simulation is computed.
				\item Discontinuous/non-differentiable systems and model properties, e.g. due to collisions.
				\item Non-differentiable structure inside the simulation (e.g. if-else).
				\item Discontinuities caused by subprograms, heuristics, table data, \dots
				\item Function evaluations are computationally expensive.
				\item Numerical "noise" overlay the actual system properties.
				\item Non-deterministic simulations (e.g. due to complex friction).
			\end{itemize}
			Hence, simulation-based optimization is a black box problem and can be solved (or approximated) using gradient-free methods!
		% end

		\subsection{Black-Box Optimization}
			Naturally, gradient-based optimization methods are not well-suited for problems with "low" differentiability and computationally expensive function evaluations. These problems may be solved using gradient-free optimization methods. But\dots
			\begin{itemize}
				\item Gradient-based techniques are really slow in comparison to gradient-based ones for differentiable optimization problems.
				\item They need a lot of function evaluations for high-dimensional problems and are thus practically only applicable for problems with dimensions \( n_p < 100 \), better \( n_p < 20 \).
				\item Have lots of problems with nonlinear equality constraints!
				\item The theory of gradient-free methods is not as mature as the theory for gradient-based methods.
			\end{itemize}
		% end
	% end

	\section{Metaheuristics}
		\subsection{Evolutionary Algorithm (EA)}
			A \emph{evolutionary algorithm} mimics the biological evolutionary strategy with random search methods.
			
			\begin{enumerate}
				\item Initialization: \tabto{2.5cm} Choose one "Parent" \( \vec{p}^{(0)} \) and a number of descendants \(\ell\).
				\item Iteration: \tabto{2.5cm} Create \(\ell\) descendants via "mutation"
			\end{enumerate}
			\begin{align*}
				\vec{p}^{(k, i)} = \vec{p}^{(k)} + \alpha_i^{(k)} \vec{d}_i,\quad i = 1, \cdots, \ell
			\end{align*}
			\begin{itemize}
				\item[] where \( \vec{d}_i \in \R^{n_p} \) are vectors of Gaussian distributed variables and \( \alpha_i \in \R \) art suitable "mutation step sizes" Then select the descent with the lower \( \varphi \) value and repeat.
			\end{itemize}
		% end

		\subsection{Genetic Algorithms (GA)}
			\emph{Genetic algorithms} are inspired by biological evolutionary strategies the positive properties caused by mutation are kept through natural selection. GAs are applicable for both real and discrete optimization variables \(\vec{p}\).
			\begin{enumerate}
				\item Initialization: \tabto{3cm} Choose a suitable set of different "individuals" (first generation).
				\item Evaluation: \tabto{3cm} Determine the "fitness" of each candidate using the objective/fitness function.
				\item Selection: \tabto{3cm} Randomly select candidates of the current generation (the higher the fitness, the higher the probability to be chosen).
				\item Recombination: \tabto{3cm} Combine values (genomes) of the selected individuals and create new individuals.
				\item Mutation: \tabto{3cm} Randomly change the genomes.
				\item New Generation: \tabto{3cm} Select new individuals as the new generation and continue with step 2.
			\end{enumerate}

			\paragraph{Example} % 3.13, 3.14
				\todo{Content}
			% end
		% end

		\subsection{Further Metaheuristics}
			Further metaheuristics based on real representations of \(\vec{p}\) like in evolutionary algorithms:
			\begin{itemize}
				\item Particle Swam: Population method, uses idea of combining local and swarm knowledge, direction and speed for particles are adjusted
				\item \dots
			\end{itemize}
		
			Further metaheuristics based on binary representations of \(\vec{p}\) like in genetic algorithms:
			\begin{itemize}
				\item Tabu Search: A list of possible manipulations is given, another lists dynamically the inverses of them, these cannot be applied any more
				\item \dots
			\end{itemize}
		% end
	% end

	\section{Deterministic Sampling Methods (Pattern Search Methods)}
		Deterministic sampling methods can be further categorized into
		\begin{itemize}
			\item Qualitative methods: Only ranking w.r.t. to the function value.
				\begin{itemize}
					\item Simplex Methods
					\item Coordinate- or compass-search
					\item Multidirectional search
					\item Pattern search methods
					\item \dots
				\end{itemize}
			\item Quantitative methods: Consideration of the real function values.
				\begin{itemize}
					\item Implicit filtering (based on the simplex method)
					\item DIRECT (dividing rectangles)
					\item \dots
				\end{itemize}
		\end{itemize}

		\subsection{Nelder-Mead Simplex Method}
			A \emph{simplex} is a simple object that consists of \( n_p + 1 \) points \( \vec{p}^{(i)} \) in the parameter space (which is \(n_p\)-dimensional). In a 2D space, the three points form a triangle. In the iteration phase the values of \(\varphi\) at the corners are compared and the simplex is transformed according to specific rules (see~\autoref{subsec:nelderMeadIteration}). The algorithm terminates once the simplex contracts onto a single point.

			\subsubsection{Iteration Phase}
				\label{subsec:nelderMeadIteration}
			
				The iteration phase starts by sorting the edges according to its \(\varphi\)-values:
				\begin{align*}
					\varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p^{(2)}}\big) \leq \cdots \leq \varphi\big(\vec{p}^{(n_p + 1)}\big)
				\end{align*}
				where \( \vec{p}^{(1)} \) is called the \emph{best} point and \( \vec{p}^{(n_p + 1)} \) is called the \emph{worst}. The algorithm now tried to replace the worst point \( \vec{p}^{(n_p + 1)} \) with another point of the form
				\begin{align*}
					\vec{p}(\mu) = (1 + \mu) \bar{\vec{p}} - \mu \vec{p}^{(n_p + 1)}
				\end{align*}
				Where \(\bar{\vec{p}}\) is the centroid of the of all points \emph{except the worst}, i.e.:
				\begin{align*}
					\bar{\vec{p}} = \frac{1}{n_p} \sum_{i = 1}^{n_p} \vec{p}^{(i)}
				\end{align*}
				This corresponds to a reflection of the worst point over the centroid with a weight \(\mu\) that specifies "how far the point point gets pushed out", i.e. the ratio of the original distance of the worst point to the centroid that is preserved while reflecting. If \(\mu = 1\), the point is mirrored.
				
				In every iteration, the value \(\mu\) is chosen of a set of four values
				\begin{align*}
					-1 < \mu_{ic} < 0 < \mu_{oc} < \mu_{r} < \mu_{e}
				\end{align*}
				for example \( (\mu_{ic}, \mu_{oc}, \mu_r, \mu_\mathit{e}) = (-0.5, 0.5, 1, 2) \).
			% end

			\subsubsection{Algorithm}
				Some termination criteria are for example:
				\begin{itemize}
					\item Exactness in the objective: \( \varphi\big( \vec{p}^{(n_p + 1)} \big) - \varphi\big(\vec{p}^{(1)}\big) \leq \varepsilon \)
					\item Maximum number of function evaluations: \( k = k_\mathrm{max} \)
					\item Sufficient small distance on the simplex corners.
				\end{itemize}
			
				\begin{itemize}
					\item Initialization: Choose a start simplex, evaluate the objective at the corners, sort them and set \( k = n_p + 1 \).
					\item Iteration: While \( \varphi\big( \vec{p}^{(n_p + 1)} \big) - \varphi\big(\vec{p}^{(1)}\big) > \varepsilon \) and \( k < k_\mathrm{max} \), do:
						\begin{enumerate}[label = (\alph*)]
							\item Calculate the centroid \( \bar{\vec{p}} = \frac{1}{n_p} \sum_{i = 1}^{n_p} \vec{p}^{(i)} \).
							\item \emph{Reflection:} If \( \varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(n_p)}\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_r) \); go to (g). \\
									"Use the reflected point if it is better than the second worst, but not better than the best."
							\item \emph{Expansion:} If \( \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(1)}\big) \), then: \\
									"If the reflected point is better than the best, \dots"
								\begin{itemize}
									\item If \( \varphi\big( \vec{p}(\mu_e) \big) < \varphi\big( \vec{p}(\mu_r) \big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_e) \); go to (g). \\
											"\dots and the expanded point is better than the reflected point, use the expanded point."
									\item If \( \varphi\big( \vec{p}(\mu_r) \big) < \varphi\big( \vec{p}(\mu_e) \big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_r) \); go to (g). \\
											"\dots and the expanded point is worst than the reflected point, use the reflected point."
								\end{itemize}
							\item \emph{Outer Contraction:} If \( \varphi\big(\vec{p}^{(n_p)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) < \varphi\big(\vec{p}^{(n_p + 1)}\big) \), then: \\
									"If the reflected point is better than the worst, but worst than second worst, \dots"
								\begin{itemize}
									\item If \( \varphi\big(\vec{p}(\mu_{oc})\big) < \varphi\big(\vec{p}(\mu_r)\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_{oc}) \); go to (g). \\
											"\dots and the outer contraction point is better than the reflected point, use the outer contraction point."
									\item Else, go to (f).
								\end{itemize}
							\item \emph{Inner Contraction:} If \( \varphi\big(\vec{p}^{(n_p + 1)}\big) \leq \varphi\big(\vec{p}(\mu_r)\big) \), then: \\
									"If the reflected point is worst than the worst point, \dots"
								\begin{itemize}
									\item If \( \varphi\big(\vec{p}(\mu_{ic})\big) < \varphi\big(\vec{p}^{(n_p + 1)}\big) \), replace \( \vec{p}^{(n_p + 1)} \) with \( \vec{p}(\mu_{ic}) \); go to (g). \\
											"\dots and the inner contraction point is better than the worst, use the inner contraction point."
									\item Else, go to (f).
								\end{itemize}
							\item \emph{Shrink:} For all \( 2 \leq i \leq n_p + 1 \), set \( \vec{p}^{(i)} = \vec{p}^{(1)} - \frac{1}{2} \big( \vec{p}^{(i)} - \vec{p}^{(1)} \big) \).
							\item \emph{Sort:} Sorting the current simplex corners such that \( \varphi\big(\vec{p}^{(1)}\big) \leq \varphi\big(\vec{p^{(2)}}\big) \leq \cdots \leq \varphi\big(\vec{p}^{(n_p + 1)}\big) \) holds again. Repeat.
						\end{enumerate}
				\end{itemize}
			
				The Nelder-Mead method therefore always tries to create a big simplex and only shrink if every other action would yield a worst corner/simplex.
			% end
			
			\subsubsection{Notes}
				\begin{itemize}
					\item The method is not guaranteed to converge. But in practice, it yields good results.
					\item Can get stuck on a suboptimal point such that the algorithm has to be restarted with other initial simplex corners.
				\end{itemize}
			
				\paragraph{Simplex-Gradient}
					It is possible to detect stagnation using a \emph{Simplex-Gradient} \( \vec{D}^{(k)} \in \R^{n_p} \), \( \vec{D}^{(k)} = \big( \mat{V}^{(k)} \big)^{-T} \vec{\delta}^{(k)} \) where \( \mat{V}^{(k)} \) is the matrix of the simplex directions
					\begin{align*}
						\mat{V}^{(k)} =
							\begin{bmatrix}
								\vec{p}^{(2)} - \vec{p}^{(1)} & \vec{p}^{(3)} - \vec{p}^{(1)} & \cdots & \vec{p}^{(n_p + 1)} - \vec{p}^{(1)}
							\end{bmatrix}
						\eqqcolon
							\begin{bmatrix}
								\vec{v}^{(1)} & \vec{v}^{(2)} & \cdots & \vec{v}^{(n_p)}
							\end{bmatrix}
						\in \R^{n_p \times n_p}
					\end{align*}
					and \( \vec{\delta}^{(k)} \) is the vector of the objective differences:
					\begin{align*}
						\vec{\delta}^{(k)} =
							\begin{bmatrix}
								\varphi\big(\vec{p}^{(2)}\big) - \varphi\big(\vec{p}^{(1)}\big) \\
								\varphi\big(\vec{p}^{(3)}\big) - \varphi\big(\vec{p}^{(1)}\big) \\
								\vdots \\
								\varphi\big(\vec{p}^{(n_p + 1)}\big) - \varphi\big(\vec{p}^{(1)}\big)
							\end{bmatrix}
						\in \R^{n_p}
					\end{align*}
					Analogous to a gradient-based method, this yields a condition for \emph{minimum progress}
					\begin{align*}
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} < -\alpha \big\lVert \vec{D}^{(k)} \big\rVert^2,\qquad
						\hat{\varphi} = \frac{1}{n_p + 1} \sum_{i = 1}^{n_p + 1} \varphi\big(\vec{p}^{(i)}\big)
					\end{align*}
					with a small \( \alpha > 0 \). One approach for a condition on when to restart is to restart if both
					\begin{align*}
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} > -\alpha \big\lVert \vec{D}^{(k)} \big\rVert^2
						\quad\text{and}\quad
						\hat{\varphi}^{(k + 1)} - \hat{\varphi}^{(k)} < 0
					\end{align*}
					hold.
				% end
			% end
		% end

		\subsection{Multidirectional Search Methods}
			\begin{itemize}
				\item In the Nelder-Mead method a bad conditioning of the simplex, i.e. the matrix \( \mat{V}^{(k)} \), leads to problems that cannot be avoided.
				\item In multidirectional search methods this problem is avoided by making every simplex congruent to its predecessors.
				\item The algorithm uses similar steps for reflection, expansion and contraction, but possibly needs a lot more function evaluations.
			\end{itemize}
		% end

		\subsection{Asynchronous Parallel Pattern Search (APPS)}
			\begin{itemize}
				\item \emph{Asynchronous Parallel Pattern Search} is a pattern-based search method on a grid.
				\item The direction of the pattern determines the descent direction.
				\item Patterns can be varied while maintaining their mathematical properties.
				\item It is "naturally" parallelizable.
			\end{itemize}
		% end

		\subsection{Implicit Filtering}
			\emph{Implicit filtering} is a descent method using "smooth" approximations of the gradients. It uses a central approximation of the simplex gradient
			\begin{align*}
				\vec{D}_C^{(k)} = \frac{1}{2} \big( \vec{D}^{(k)} + \vec{D}_R^{(k)} \big)
			\end{align*}
			where \( \vec{D}_R^{(k)} \) is the gradient of the simplex that is reflected around \( \vec{p}^{(k)} \).
			
			The structure of implicit filtering is sketched in\autoref{alg:implicitFiltering}.
			
			\begin{algorithm}  \DontPrintSemicolon
				\textbf{Initialization:} Choose \( \alpha, \beta \in (0, 1) \) and set \( \mat{H} = \mat{I} \) \;
				\While{not converged}{
					Calculate \( \varphi\big(\vec{p}^{(k)}\big) \), \( \vec{D}_C^{(k)} \) and the search direction \( \vec{d}^{(k)} = -\mat{H}^{-1} \vec{D}_C^{(k)} \) \;
					Inexact line search for \( j = 1, \cdots, j_\mathrm{max} \), \( \lambda \coloneqq \beta^j \) until the following holds:
						\begin{align*}
							\varphi(\vec{p} + \lambda \vec{d}) - \varphi(\vec{p}) \leq \alpha \lambda \grad_{\Delta \vec{p}_k} \varphi^T(\vec{p}) \vec{d}^{(k)}
						\end{align*} \;
					\( \vec{p}^{(k + 1)} \gets \vec{p}^{(k)} + \lambda \vec{d}^{(k)} \) \;
					\If{line search successful}{
						Quasi-Newton update of the Hessian \(\mat{H}\) with \( \vec{p}^{(k + 1)} - \vec{p}^{(k)} \) and \( \vec{D}_C^{(k + 1)} - \vec{D}_C^{(k)} \) \;
					} \Else{
						\( \mat{H} \gets \mat{I} \) \;
						Shrink the simplex \;
					}
				}
				
				\caption{Implicit Filtering.}
				\label{alg:implicitFiltering}
			\end{algorithm}
		% end
	% end

	\section{Surrogate Optimization}
		In \emph{surrogate optimization methods}, the (complex) objective is replaced with a simpler approximation that maintains the key properties of the objective (e.g. a noisy measurement might be replaced by a simpler regression model). This surrogate function is then minimized and adjusted in order to find a good approximation of the solution of the original problem (this can also be applied for constraint functions).
		
		Requirements for the surrogate function \( \hat{\varphi} : \R^{n_p} \to \R \): For all function evaluation (or "sampling") points \( \Big( \vec{p}^{(i)}, \varphi\big(\vec{p}^{(i)}\big) \Big) \), \( i = 1, \cdots, m \) it must hold that
		\begin{align*}
			\varphi\big( \vec{p}^{(i)} \big) = \hat{\varphi}\big( \vec{p}^{(i)} \big) + \epsilon
		\end{align*}
		where \( \epsilon \in \R \) is some "slack" constant that determines how exact the surrogate function shall be.
		\begin{itemize}
			\item For \(\epsilon = 0\), the problem is the same as interpolation.
			\item For \(\epsilon > 0\), the surrogate function does not perfectly reproduce the objective, but might be smoother.
		\end{itemize}
		Further requirements are that \( \hat{\varphi} \) should be fast to compute and the gradients of \(\hat{\varphi}\) should be available in closed form.
		
		This raises some questions:
		\begin{enumerate}
			\item \(\varphi\) might be too complex for a simple approximation \(\implies\) which approximation method should be used?
			\item How to generate the data basis of the function evaluations? Generating all in one point will not yields good results\dots
			\item Which method is feasible to minimize \(\hat{\varphi}\)?
		\end{enumerate}

		\subsection{Approximation Methods}
			\subsubsection{Response Surface Methods (RSMs)}
				\emph{Response surface methods} use simple polynomials of a low degree as the model function \(\hat{\varphi}\), e.g.:
				\begin{itemize}
					\item Degree one (linear): \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \beta_1^T \vec{p} \)
					\item Degree one with mixed terms: \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1^T \vec{p} + \sum_{i} \sum_{\substack{j \\ j \neq i}} \beta_2^{i, j} p_i p_j \)
					\item Degree two (quadratic): \tabto{5.5cm} \( \hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1 \vec{p} + \vec{p}^T \mat{\beta}_2 \vec{p} \)
					\item Higher degree: \dots
				\end{itemize}
				The unknown parameters \( \beta_1 \in \R \), \( \vec{\beta}_2 \in \R^{n_p} \) and \( \mat{\beta}_2 \in \R^{n_p \times n_p} \) can be approximated using least squares:
				\begin{align*}
					\min_{\beta_1, \vec{\beta}_2, \mat{\beta}_2} \sum_i \Big( \varphi\big(\vec{p}^{(i)}\big) - \hat{\varphi}\big(\vec{p}^{(i)}\big) \Big)
				\end{align*}
				
				\begin{itemize}
					\item Advantage: Simple and the approximations are easy to compute.
					\item Disadvantage: The RSMs cover only the global behavior and not local accuracy.
				\end{itemize}
			% end

			\subsubsection{Radial Basis Functions (RBFs)}
				Now the surrogate function \( \hat{\varphi} \) uses a linear combination of \emph{radial basis functions}:
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \sum_{i = 1}^{m} \gamma_i h\Big( \big\lVert \vec{p} - \vec{p}^{(i)} \big\rVert \Big)
				\end{align*}
				with basis function \( h(\cdot) \) based only on the euclidean distance from the interpolation point, e.g.
				\begin{itemize}
					\item Linear: \tabto{2cm} \( h(r_i) = r_i \)
					\item Cubic: \tabto{2cm} \( h(r_i) = r_i^3 \)
					\item Thin-Plate: \tabto{2cm} \( h(r_i) = r_i^2 \log r \)
				\end{itemize}
				where \( r_i = \big\lVert \vec{p} - \vec{p}^{(i)} \big\rVert \).
				
				A suitable combination of RSM and RBF yield cubic spline-approximation:
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \beta_0 + \vec{\beta}_1 \vec{p} + \sum_i \gamma_i h(\vec{p}),\quad \beta_1 \in \R, \beta_2 \in \R^{n_p}, \gamma_i \in \R
				\end{align*}
				\begin{itemize}
					\item Univariate: \tabto{2cm} \( h(\vec{p}) = \frac{1}{12} \sum_i r_i^3 \)
					\item Bivariate: \tabto{2cm} \( h(\vec{p}) = \frac{1}{16 \pi} \sum_i r_i^2 \log r_i \)
				\end{itemize}
			% end

			\subsubsection{Design and Analysis of Computer Experiments (DACE)}
				Assuming the model function is a realization of a stochastic process
				\begin{align*}
					\hat{\varphi}(\vec{p}) = \vec{v}^T(\vec{p}) \vec{\beta} + Z(\vec{p})
				\end{align*}
				where \( \vec{v}(\vec{p}) \) is a vector of basis functions (e.g. RSM, RBF) and \( Z(\vec{p}) \) is a stationary random variable that is Gaussian distributed with zero mean. The covariance between two points \( \vec{p}^{(l)} \) and \( \vec{p}^{(k)} \) is given as
				\begin{align*}
					\Cov\Big[ Z\big(\vec{p}^{(l)}\big), Z\big(\vec{p}^{(k)}\big) \Big] = \sigma^2 R\big(\vec{p}^{(l)}, \vec{p}^{(k)}\big)
					\quad\text{with}\quad
					R\big(\vec{p}^{(l)}, \vec{p}^{(k)}\big) = \prod_{i = 1}^{n_p} e^{-\theta_i d_i^2},\quad d_i = \big\lVert \vec{p}_i^{(l)} - \vec{p}_i^{(k)} \big\rVert_2
				\end{align*}
				Die unknown parameters \(\vec{\beta}\), \(\vec{\theta}\) and \(\sigma^2\) are then estimated using statistical estimators, e.g. maximum likelihood.
			% end
		% end

		\subsection{Select of the Sampling Points}
			\subsubsection{Design of Experiments (DoE)}
				The classical strategy for selection sampling points, \emph{design of experiments} is mainly designed for physical experiments, not for deterministic ones! Typical approach:
				\begin{itemize}
					\item Classical selection
						\begin{itemize}
							\item orthogonal arrays
							\item latin hypercubes
							\item combinations
						\end{itemize}
					\item Metric-bases methods
						\begin{itemize}
							\item MiniMax: minimizing the maximal distance between the sampling points
							\item MaxiMin: maximizing the minimal distance
						\end{itemize}
					\item Stochastic selection for Gaussian processes
						\begin{itemize}
							\item Entropy Design or D-Opt: maximizing the determinant of the covariance matrix
							\item A-Opt: depends on the trace of the covariance matrix
							\item G-Opt: minimize the maximum mean squared error
						\end{itemize}
				\end{itemize}
			% end
		% end

		\subsection{Minimizing the Surrogate Function}
			To successfully minimize the original objective function, the data basis of the surrogate function has to be expanded sequentially. The following sections describe two methods for this, the Strawman and the Shoemaker method.

			\subsubsection{Strawman}
				\begin{enumerate}
					\item Calculate the current minimum of the surrogate function (e.g. using gradient descent).
					\item Add the minimum of the surrogate function as a sampling point.
					\item Calculate the new surrogate function. Repeat.
				\end{enumerate}
			% end

			\subsubsection{Shoemaker}
				\begin{enumerate}
					\item Calculate the minimum with a minimal distance to all sampling points.
					\item Extend the sampling points by this point.
					\item Determine the new surrogate function. Repeat.
				\end{enumerate}
			% end

			\subsubsection{DACE-Based, Sequential Update Strategy}
				\begin{itemize}
					\item The the expected mean error as a criteria for the quality of the surrogate function.
					\item Weigh small function values and uncertainties in the approximations.
					\item There exist different strategies following this basic idea.
					\item Termination criteria:
						\begin{itemize}
							\item Number of function evaluations
							\item No more improvements in the objective function
							\item \(\varphi(\vec{p})\) close to the minimal possible function value
						\end{itemize}
					\item Sequential methods are better on normal computers, for parallel computers a special scheme should be used.
				\end{itemize}
			% end
		% end

		\subsection{Discussion}
			\begin{itemize}
				\item Independent of the approximation method, the surrogate function has to be minimized one or more times per iteration (depending on the actual method).
				\item But the effort for the minimization is negligible as one simulation run for the evaluation of \(\varphi\) often takes a lot longer.
				\item Every method of~\autoref{c:gradientOptimization} can be used for minimizing \(\hat{\varphi}\) as the gradients are available by design.
				\item Advantages:
					\begin{itemize}
						\item \(\hat{\varphi}\) is given in closed form as well as the gradientd.
						\item Easy to compute, Newton-type methods applicable!
						\item "Smooth" surrogate function.
					\end{itemize}
				\item Disadvantages:
					\begin{itemize}
						\item Approximation accuracy is limited.
						\item The number of sample points rises a lot for high-dimensional problems (curse of dimensionality).
					\end{itemize}
			\end{itemize}
		% end
	% end

	\section{Comparison}
		\subsection{Magnetic Bearing Design} % 3.56, 3.57, 3.58, 3.59, 3.60, 3.61
			\todo{Content}
		% end

		\subsection{Walking Optimization of a Humanoid Robot} % 3.62, 3.63
			\todo{Content}
		% end
	% end

	\section{Discussion}
		\begin{itemize}
			\item Advantages:
				\begin{itemize}
					\item Application is easy, no or little prior knowledge required.
					\item Robust toward discontinuities of \(\varphi\) or \(\grad{\varphi}\).
					\item No calculation of gradients necessary.
					\item No need to start "close to" a solution.
					\item Some methods (e.g. evolutionary algorithms) are highly parallelizable, some are not (e.g. Nelder-Mead).
				\end{itemize}
			\item Disadvantages:
				\begin{itemize}
					\item Slow convergence, high number of steps needed and high computation time due to many \(\varphi\)-evaluations.
					\item Inefficient for large \( n_p \).
					\item Major difficulties for nonlinear constraints in \(\vec{p}\).
				\end{itemize}
		\end{itemize}
	% end
% end

\chapter{Gradientenbasierte Optimierung mit BeschrÃ¤nkungen} % 4.1, 4.2, 4.3, 4.4, 4.5
	\todo{Content}

	\section{Charakterisierung der LÃ¶sung} % 4.6
		\todo{Content}

		\subsection{Motivation} % 4.7, 4.8
			\todo{Content}
		% end

		\subsection{Lagrange-Funktion} % 4.9
			\todo{Content}
		% end

		\subsection{Notwendige OptimalitÃ¤tsbedingungen 1. Ordnung (Karush-Kuhn-Tucker, KKT)} % 4.10
			\todo{Content}
		% end

		\subsection{Notwendige OptimalitÃ¤tsbedingungen 2. Ordnung} % 4.12
			\todo{Content}
		% end

		\subsection{Beispiel} % 4.11, 4.13
			\todo{Content}
		% end
	% end

	\section{Einfache Schranken} % 4.14, 4.15, 4.16, 4.17
		\todo{Content}
	% end

	\section{Straffunktionsverfahren} % 4.18, 4.19
		\todo{Content}

		\subsection{ÃuÃere Straffunktionsverfahren} % 4.20, 4.21
			\todo{Content}

			\paragraph{Beispiel} % 4.22, 4.23
				\todo{Content}
			% end
		% end

		\subsection{Innere Straffunktionsverfahren} % 4.24, 4.25
			\todo{Content}

			\paragraph{Beispiel} % 4.26
				\todo{Content}
			% end
		% end

		\subsection{Exakte Straffunktionen} % 4.27
			\todo{Content}

			\paragraph{Beispiel 1} % 4.28
				\todo{Content}
			% end

			\paragraph{Beispiel 2} % 4.29
				\todo{Content}
			% end
		% end

		\subsection{Erweiterte Lagrange-Funktion} % 4.30
			\todo{Content}

			\paragraph{Beispiel 1} % 4.31, 4.32
				\todo{Content}
			% end

			\paragraph{Beispiel 2} % 4.33
				\todo{Content}
			% end

			\subsubsection{Eigenschaften} % 4.34, 4.35
				\todo{Content}
			% end
		% end
	% end

	\section{Elimination von BeschrÃ¤nkungen} % 4.36, 4.37, 4.41
		\todo{Content}

		\paragraph{Beispiel 1} % 4.38
			\todo{Content}
		% end

		\paragraph{Beispiel 2} % 4.39
			\todo{Content}
		% end

		\paragraph{Beispiel 3} % 4.40
			\todo{Content}
		% end
	% end

	\section{Verfahren der Sequentiellen Quadratischen Optimierung (SQP)} % 4.42
		\todo{Content}

		\subsection{Einleitung} % 4.43, 4.44
			\todo{Content}
		% end

		\subsection{Bestimmung der Suchrichtung} % 4.45, 4.46, 4.47
			\todo{Content}

			\subsubsection{Quadratisches Problem (QP)} % 4.48
				\todo{Content}
			% end
		% end

		\subsection{Bestimmung der Schrittweite} % 4.50
			\todo{Content}
		% end

		\subsection{Approximation der Lagrange-Multiplikatoren} % 4.52
			\todo{Content}
		% end

		\subsection{Terminierungskriterien} % 4.54, 4.55
			\todo{Content}
		% end

		\subsection{Approximation der Hesse-Matrix} % 4.58, 4.59
			\todo{Content}

			\subsubsection{Naiver Ansatz} % 4.60
				\todo{Content}
			% end

			\subsubsection{Reduzierte Hesse-Matrix} % 4.61, 4.63, 4.64, 4.65, 4.66, 4.67, 4.68
				\todo{Content}

				\paragraph{Beispiel} % 4.62
					\todo{Content}
				% end
			% end

			\subsubsection{Approximation der reduzierten Hesse-Matrix} % 4.69
				\todo{Content}
			% end
		% end

		\subsection{SQP-Verfahren} % 4.52, 4.53, 4.57, 4.75
			\todo{Content}
		% end

		\subsection{Bemerkungen} % 4.49, 4.70, 4.75, 4.76, 4.77, 4.78, 4.79
			\todo{Content}
		% end

		\subsection{Beispiele} % N/A
			\todo{Content}

			\subsubsection{Optimale Steuerung eines 6-gelenkigen Industrieroboters} % 4.71, 4.72
				\todo{Content}
			% end

			\subsubsection{PKW-Fahrt} % 4.73, 4.74
				\todo{Content}
			% end
		% end
	% end
% end

\chapter{Berechnung von Ableitungen} % 5.1, 5.2, 5.3, 5.4
	\todo{Content}

	\section{Finite-Differenzen-Approximation (numerische Differentiation)} % 5.5
		\todo{Content}

		\subsection{VorwÃ¤rtsdifferenzen-Approximation} % 5.6, 5.13
			\todo{Content}

			\subsubsection{Fehler} % 5.8, 5.12
				\todo{Content}

				\paragraph{Approximationsfehler} % 5.9
					\todo{Content}
				% end

				\paragraph{Funktionsgenauigkeit} % 5.10
					\todo{Content}
				% end

				\paragraph{Rundungsfehler} % 5.11
					\todo{Content}
				% end
			% end

			\subsubsection{Wahl der Schrittweite} % 5.14, 5.15, 5.16, 5.17
				\todo{Content}
			% end
		% end

		\subsection{Zentrale-Differenzen-Approximation} % 5.18, 5.19, 5.21, 5.22
			\todo{Content}
		% end
	% end

	\section{Numerische Differentiation von Simulationsmodellen} % 5.23, 5.24, 5.25
		\todo{Content}

		\subsection{Ableitung von ODE-Simulationsmodellen} % 5.27, 5.28
			\todo{Content}
		% end

		\subsection{Externe numerische Differentiation} % N/A
			\todo{Content}

			\subsubsection{Naiver Ansatz} % 5.29, 5.30, 5.31
				\todo{Content}
			% end

			\subsubsection{Gekoppelte VorwÃ¤rtsdifferenzen-Approximation} % 5.32
				\todo{Content}
			% end
		% end

		\subsection{Interne numerische Differentiation} % 5.33, 5.34
			\todo{Content}
		% end
	% end

	\section{Symbolische Differentiation} % 5.35, 5.36
		\todo{Content}
	% end

	\section{Automatisches Differenzieren} % 5.37, 5.38, 5.39, 5.40
		\todo{Content}

		\paragraph{Beispiel} % 5.41, 5.42, 5.43
			\todo{Content}
		% end
	% end
% end

\chapter{Minimierung von Abweichungen} % 6.1, 6.2, 6.3
	\todo{Content}

	\section{ParameterschÃ¤tzung bei ODE-Systemen} % 6.5, 6.7
		\todo{Content}
	% end

	\section{GÃ¼tekriterien zur Minimierung von Abweichungen} % 6.8, 6.9
		\todo{Content}
	% end

	\section{Lineare Ausgleichsrechnung} % 6.10, 6.11
		\todo{Content}
	% end

	\section{OptimalitÃ¤tsbedingungen und Spezialverfahren} % 6.12, 6.13, 6.17
		\todo{Content}

		\subsection{Quasi-Newton} % 6.14
			\todo{Content}
		% end

		\subsection{GauÃ-Newton Verfahren} % 6.15
			\todo{Content}
		% end

		\subsection{Levenberg-Marquardt Verfahren} % 6.16
			\todo{Content}
		% end

		\subsection{SQP-Verfahren} % 6.18
			\todo{Content}
		% end
	% end

	\section{Normalen-Gleichungen} % 6.22, 6.23
		\todo{Content}

		\paragraph{Beispiel} % 6.24, 6.25, 6.26, 6.27, 6.28
			\todo{Content}
		% end
	% end

	\section{Interpretation von Berechnungsergebnissen} % 6.29
		\todo{Content}

		\subsection{MÃ¶gliche Ursachen fÃ¼r Schwierigkeiten} % 6.30, 6.31
			\todo{Content}
		% end
	% end

	\section{Die Varianz-Kovarianz-Matrix} % 6.32, 6.33, 6.34
		\todo{Content}
	% end

	\section{Optimale Experimentgestaltung} % 6.35, 6.36, 6.37
		\todo{Content}
	% end

	\section{Beispiele} % 6.39
		\todo{Content}

		\subsection{ParameterabhÃ¤ngige Gesamtfahrzeugdynamik} % 6.39, 6.40
			\todo{Content}

			\subsubsection{Simulierte Messwerte} % 6.41, 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Echte Messwerte} % 6.44
				\todo{Content}
			% end

			\subsubsection{Vergleich der Verfahren} % 6.45
				\todo{Content}
			% end
		% end

		\subsection{ParameterschÃ¤tzung fÃ¼r "BioBiped"} % 6.47, 6.48, 6.49
			\todo{Content}
		% end
	% end
% end
