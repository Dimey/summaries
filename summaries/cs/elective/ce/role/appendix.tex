\chapter{Self-Test Questions}
	The text below also contains answers for the self-test questions! Make sure to not spoiler you!

	\section{Introduction} % KI-Campus
		\todo{Content}
	% end

	\section{Robotics}
		\paragraph{How to compute the racket position, orientation and velocity in a game of table tennis?}
		\answer{The position and orientation can directly be computed using the forward kinematics model, e.g. using the Denavit-Hartenberg convention. To compute the velocity of the racket, the forward kinematics model has to be differentiated w.r.t. to the time. By using the chain rule, only the Jacobian of the model has to be computed w.r.t. the joint displacements. Multiplying this with the joint velocities gives the racket velocities.}

		\paragraph{What is an inverse dynamics model? What is a forward dynamics model?}
		\answer{The inverse dynamics model computes the joint torques/forces given the respective accelerations. The forward dynamics model computes the accelerations from the torques/forces.}

		\paragraph{What kind of models are needed to build a robot simulator?}
		\answer{The forward kinematics and dynamics models are needed. The latter is first used to compute the accelerations and then, after integrating the accelerations numerically two times, the former can be used to animate the robot.}

		\paragraph{How to represent trajectories in such a way that they can be tracked?}
		\answer{Trajectories have to be at least once, better twice, continuous differentiable, to avoid jumps in the positions and velocities and possibly the accelerations. This can be achieved by modeling a trajectory as a cubic or quintic spline across given via-points. These via-points represent the support points of the spline.}

		\paragraph{What does feedback control mean?}
		\answer{In feedback control, the actual state of the system is used to compute the control inputs. This allows for error correction if the robot does not behave exactly like the model predicts, for example.}

		\paragraph{What control laws are common for robots?}
		\answer{It is common to use PD-controllers with gravity compensation and PID-controllers as well as model-based feedback and feedforward controllers. But the model-based controllers need a really good model.}

		\paragraph{What is model-based feedback control?}
		\answer{In model-based feedback control, a reference acceleration is computed using a PD-controller that assesses the position, velocity and acceleration of the joints. This reference acceleration is then fed into the inverse dynamics model, giving the joint torques/forces that are then applied to the joints.}

		\paragraph{How can be inverse kinematics be computed?}
		\answer{It is sometimes possible to compute the inverse kinematics analytically. If this is not possible, it might be possible to compute them numerically, e.g. with the Newton method. However, it is better to use the inverse differential kinematics model to compute the velocities of the joints and then integrate them to recover the positions. For square Jacobians this is possible straightforwardly, for non-square Jacobians numerical methods have to be used.}

		\paragraph{What is task-space control?}
		\answer{In task-space control, the trajectory is planned in the task-space rather than in the joint-space. Then the task-space data has to be converted into the joint-space to then apply joint-space controllers like the PID-controller. Common methods are for example the Jacobian transpose method and the Jacobian pseudo-inverse method.}

		\paragraph{KI-Campus: Given the joint state of a robot, which model is used to compute the end-effector position?}
		\answer{Using the forward kinematics model.}

		\paragraph{KI-Campus: Given the joint state of a robot, which model is used to compute the torques/forces applied by the physics?}
		\answer{Using the inverse dynamics model.}

		\paragraph{KI-Campus: Given the desired end-effector state of a robot, which model is used to compute the joint positions to achieve it?}
		\answer{Using the inverse kinematics model.}

		\paragraph{KI-Campus: How to compute the forward kinematics?}
		\answer{The forward kinematics can be computed straightforwardly, e.g. by using the Denavit-Hartenberg convention and the respective homogeneous transformation matrices. They can also be computed by simple geometric observations in some cases.}

		\paragraph{KI-Campus: What are the limitations of the P-controller?}
		\answer{It oscillates around the desired position and does not include velocity-control.}

		\paragraph{KI-Campus: How can model-based control deal with mismatches between the real system and the model?}
		\answer{Using feedforward control, a model-based controller is combined with a "standard" PD-controller to eradicate modeling errors.}

		\paragraph{KI-Campus: How to compute the analytical solution for inverse kinematics?}
		\answer{This can be done by inverting the forward kinematics or by geometric observations in the system. This is, however, rather tedious and not always possible.}

		\paragraph{KI-Campus: What are a few examples in which null-space control would make sense.}
		\answer{For example for saving energy by being in rest postures in a redundant robot. In a redundant prismatic robot, this may be that no joint is fully stretched but all joint are located around the center.}
	% end

	\section{Machine Learning Foundations} % 5b.139, 5c.96
		\todo{Content}
	% end

	\section{Optimal Control}
		\subsection{Discrete Optimal Control}
			\paragraph{What is an MDP, a policy, a value function, a state-action value function?}
			\answer{An MDP, a Markov decision process, is a system with discrete states and actions that behaves Markovian, i.e. a state only depends on the previous state and the action taken and not on any other states. The initial state is drawn from the initial state distribution. A policy (that can be either deterministic or stochastic) prescribed what action to take given an action. The value function assesses the quality of a state, i.e. it gives the expected long-term reward when following a given policy. The state-action function, also called the Q-function, assesses the quality of state-action pairs, i.e. it gives the expected long-term reward when taking the said action in said state and subsequently following a policy. Maximizing the optimal Q-function yields the optimal policy.}

			\paragraph{What is policy evaluation, policy improvement, policy iteration and value iteration?}
			\answer{Policy evaluation estimated the (state-action) value function for a given policy by iterating the Bellman equation. Policy improvement takes a state-action value and deduces the policy from it w.r.t. the action for every state. Policy iteration iterates policy evaluation and improvement until convergence to find the optimal policy. Value iteration also finds the optimal policy but iterated the Bellman equation directly.}

			\paragraph{What is the main difference between policy iteration vs. value iteration?}
			\answer{In value iteration a lot of redundant maximization operations are performed for computing the value function. In policy iteration this is circumvented using the embedded policy evaluation.}

			\paragraph{What is the Bellman equation?}
			The Bellman equation describes how to compute the (optimal) value function from the (optimal) state-action value function. For infinite horizon problems, it is given as
			\begin{equation*}
				V^\ast(\vec{s}) = \max_{\vec{a}} \, \Big( r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\ast(\vec{s}') \Big).
			\end{equation*}

			\paragraph{What are the differences between finite and infinite horizon objectives? Give examples of robotics problems in both settings.}
			\answer{For finite-horizon problems, all of the transition dynamics, reward function and (state-action= value functions and therefore also the policy are time-dependent. Also there is a last reward \( r_T(\vec{s}_T) \) that is independent of the action. This can be interpreted as that it is relevant how much steps are left to make decisions. For infinite-horizon objectives, this is no longer relevant and the time-dependencies is dropped for all components. An infinite-horizon problem is, for example, balancing an inverted pendulum where more reward is gained the longer the pendulum can be held upright. An example for a finite-horizon problem is ball-in-cup, where once the ball is in the cup the problem is no longer interesting.}

			\paragraph{Why is dynamic programming difficult to apply directly to robotics?}
			\answer{Dynamic programming as discussed in this chapter is only applicable for discrete state-action spaces. This can be achieved by separating the world in buckets, but this would cause an exponential explosion in the memory required to store the value function. Using LQR (next chapter), dynamic programming can also be used for continuous state-action spaces, but this requires the system to be linear and the reward to be quadratic with Gaussian noise. But the world and therefore robots are not linear, but in most cases highly nonlinear.}
		% end

		\subsection{Continuous Optimal Control}
			\paragraph{What is the LQR problem?}
			\answer{An LQR problem is given with continuous states and actions with linear state transition dynamics with Gaussian noise, i.e. \( p(\vec{x}_{t + 1} \given \vec{x}_t, \vec{u}_t) = \mathcal{N}\big( \vec{x}_{t + 1} \biggiven \mat{A}_t \vec{x}_t + \mat{B}_t \vec{u}_t\, \mat{\Sigma}_t \big) \). The reward function is quadratic in the states and actions, i.e. \( r_t(\vec{x}, \vec{u}) = -\vec{x}^T \mat{R}_t \vec{x} - \vec{u}^T \mat{H}_t \vec{u} \) with symmetric and positive definite matrices \( \mat{R}_t \) and \( \mat{H}_t \). The optimal policy \(\pi^\ast\) then maximizes the cumulative reward \( J_\pi \) over a finite horizon \(T\). This is, besides the discrete case, the only solvable optimal control problem.}

			\paragraph{Derive the LQR value function and optimal policy for the basic case.}
			\answer{This can be done by following the principle of dynamic programming, i.e. first compute the value function for the last time step and subsequently calculate the Q-function, the optimal policy and the value function for the previous time step. This is done by applying the Bellman equation. See \autoref{subsec:lqr} for the full derivation.}

			\paragraph{What is the form of the solution (value function and policy)? What is their interpretation (qualitatively)?}
			\answer{The value function is of the form \( V_t(\vec{x}) = \vec{x}^T \mat{V}_t \vec{x} + \vec{x}^T \vec{v}_t \), i.e. it is quadratic-linear. The optimal control input is of the form \( \vec{u}_t^\ast = \mat{K}_t (\vec{x}_t - \vec{x}_d) + \vec{k}_t \), i.e. it is a time-varying P-controller!}

			\paragraph{What to do when dynamics and/or rewards are not linear? What the pitfalls?}
			\answer{It is possible to linearize the dynamics and/or the rewards using a Taylor expansion and cutting off the higher-order terms. This, however, leads to oscillations and does only work for systems that are not too nonlinear. This also causes the policies to often not work on the real system due to modeling errors.}

			\paragraph{What are the potential issues when using learning models for trajectory optimization?}
			\answer{One major issue is that the optimizer is prone to exploit error in the model. When using learned models, this might mean to jump out of the space the system learned in, causing unpredictable behaviors. Such policies will most likely never work on the real system as it does not exhibit the same errors.}
		% end
	% end

	\section{Approximate Optimal Control} % 4a.31, 4b.29
		\todo{Content}
	% end

	\section{State Estimation} % 6.64
		\todo{Content}
	% end

	\section{Model Learning} % 7.66
		\todo{Content}
	% end

	\section{Policy Representations} % 8.65
		\todo{Content}
	% end

	\section{Model-Based Reinforcement Learning} % 9.59
		\todo{Content}
	% end

	\section{Value Function Methods} % 11.42
		\todo{Content}
	% end

	\section{Policy Search} % 10.59, 12.61
		\todo{Content}
	% end

	\section{Imitation Learning} % N/A
		\todo{Content}
	% end

	\section{Bayesian Reinforcement Learning} % 14.54
		\todo{Content}
	% end
% end
