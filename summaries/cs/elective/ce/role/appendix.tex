\chapter{Self-Test Questions}
	The text below also contains answers for the self-test questions! Make sure to not spoiler you!

	\section{Introduction} % KI-Campus
		\todo{Content}
	% end

	\section{Robotics}
		\paragraph{How to compute the racket position, orientation and velocity in a game of table tennis?}
		\answer{The position and orientation can directly be computed using the forward kinematics model, e.g. using the Denavit-Hartenberg convention. To compute the velocity of the racket, the forward kinematics model has to be differentiated w.r.t. to the time. By using the chain rule, only the Jacobian of the model has to be computed w.r.t. the joint displacements. Multiplying this with the joint velocities gives the racket velocities.}

		\paragraph{What is an inverse dynamics model? What is a forward dynamics model?}
		\answer{The inverse dynamics model computes the joint torques/forces given the respective accelerations. The forward dynamics model computes the accelerations from the torques/forces.}

		\paragraph{What kind of models are needed to build a robot simulator?}
		\answer{The forward kinematics and dynamics models are needed. The latter is first used to compute the accelerations and then, after integrating the accelerations numerically two times, the former can be used to animate the robot.}

		\paragraph{How to represent trajectories in such a way that they can be tracked?}
		\answer{Trajectories have to be at least once, better twice, continuous differentiable, to avoid jumps in the positions and velocities and possibly the accelerations. This can be achieved by modeling a trajectory as a cubic or quintic spline across given via-points. These via-points represent the support points of the spline.}

		\paragraph{What does feedback control mean?}
		\answer{In feedback control, the actual state of the system is used to compute the control inputs. This allows for error correction if the robot does not behave exactly like the model predicts, for example.}

		\paragraph{What control laws are common for robots?}
		\answer{It is common to use PD-controllers with gravity compensation and PID-controllers as well as model-based feedback and feedforward controllers. But the model-based controllers need a really good model.}

		\paragraph{What is model-based feedback control?}
		\answer{In model-based feedback control, a reference acceleration is computed using a PD-controller that assesses the position, velocity and acceleration of the joints. This reference acceleration is then fed into the inverse dynamics model, giving the joint torques/forces that are then applied to the joints.}

		\paragraph{How can be inverse kinematics be computed?}
		\answer{It is sometimes possible to compute the inverse kinematics analytically. If this is not possible, it might be possible to compute them numerically, e.g. with the Newton method. However, it is better to use the inverse differential kinematics model to compute the velocities of the joints and then integrate them to recover the positions. For square Jacobians this is possible straightforwardly, for non-square Jacobians numerical methods have to be used.}

		\paragraph{What is task-space control?}
		\answer{In task-space control, the trajectory is planned in the task-space rather than in the joint-space. Then the task-space data has to be converted into the joint-space to then apply joint-space controllers like the PID-controller. Common methods are for example the Jacobian transpose method and the Jacobian pseudo-inverse method.}

		\paragraph{KI-Campus: Given the joint state of a robot, which model is used to compute the end-effector position?}
		\answer{Using the forward kinematics model.}

		\paragraph{KI-Campus: Given the joint state of a robot, which model is used to compute the torques/forces applied by the physics?}
		\answer{Using the inverse dynamics model.}

		\paragraph{KI-Campus: Given the desired end-effector state of a robot, which model is used to compute the joint positions to achieve it?}
		\answer{Using the inverse kinematics model.}

		\paragraph{KI-Campus: How to compute the forward kinematics?}
		\answer{The forward kinematics can be computed straightforwardly, e.g. by using the Denavit-Hartenberg convention and the respective homogeneous transformation matrices. They can also be computed by simple geometric observations in some cases.}

		\paragraph{KI-Campus: What are the limitations of the P-controller?}
		\answer{It oscillates around the desired position and does not include velocity-control.}

		\paragraph{KI-Campus: How can model-based control deal with mismatches between the real system and the model?}
		\answer{Using feedforward control, a model-based controller is combined with a "standard" PD-controller to eradicate modeling errors.}

		\paragraph{KI-Campus: How to compute the analytical solution for inverse kinematics?}
		\answer{This can be done by inverting the forward kinematics or by geometric observations in the system. This is, however, rather tedious and not always possible.}

		\paragraph{KI-Campus: What are a few examples in which null-space control would make sense.}
		\answer{For example for saving energy by being in rest postures in a redundant robot. In a redundant prismatic robot, this may be that no joint is fully stretched but all joint are located around the center.}
	% end

	\section{Machine Learning Foundations} % 5b.139, 5c.96
		\todo{Content}
	% end

	\section{Optimal Control}
		\subsection{Discrete Optimal Control}
			\paragraph{What is an MDP, a policy, a value function, a state-action value function?}
			\answer{An MDP, a Markov decision process, is a system with discrete states and actions that behaves Markovian, i.e. a state only depends on the previous state and the action taken and not on any other states. The initial state is drawn from the initial state distribution. A policy (that can be either deterministic or stochastic) prescribed what action to take given an action. The value function assesses the quality of a state, i.e. it gives the expected long-term reward when following a given policy. The state-action function, also called the Q-function, assesses the quality of state-action pairs, i.e. it gives the expected long-term reward when taking the said action in said state and subsequently following a policy. Maximizing the optimal Q-function yields the optimal policy.}

			\paragraph{What is policy evaluation, policy improvement, policy iteration and value iteration?}
			\answer{Policy evaluation estimated the (state-action) value function for a given policy by iterating the Bellman equation. Policy improvement takes a state-action value and deduces the policy from it w.r.t. the action for every state. Policy iteration iterates policy evaluation and improvement until convergence to find the optimal policy. Value iteration also finds the optimal policy but iterated the Bellman equation directly.}

			\paragraph{What is the main difference between policy iteration vs. value iteration?}
			\answer{In value iteration a lot of redundant maximization operations are performed for computing the value function. In policy iteration this is circumvented using the embedded policy evaluation.}

			\paragraph{What is the Bellman equation?}
			The Bellman equation describes how to compute the (optimal) value function from the (optimal) state-action value function. For infinite horizon problems, it is given as
			\begin{equation*}
				V^\ast(\vec{s}) = \max_{\vec{a}} \, \Big( r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\ast(\vec{s}') \Big).
			\end{equation*}

			\paragraph{What are the differences between finite and infinite horizon objectives? Give examples of robotics problems in both settings.}
			\answer{For finite-horizon problems, all of the transition dynamics, reward function and (state-action= value functions and therefore also the policy are time-dependent. Also there is a last reward \( r_T(\vec{s}_T) \) that is independent of the action. This can be interpreted as that it is relevant how much steps are left to make decisions. For infinite-horizon objectives, this is no longer relevant and the time-dependencies is dropped for all components. An infinite-horizon problem is, for example, balancing an inverted pendulum where more reward is gained the longer the pendulum can be held upright. An example for a finite-horizon problem is ball-in-cup, where once the ball is in the cup the problem is no longer interesting.}

			\paragraph{Why is dynamic programming difficult to apply directly to robotics?}
			\answer{Dynamic programming as discussed in this chapter is only applicable for discrete state-action spaces. This can be achieved by separating the world in buckets, but this would cause an exponential explosion in the memory required to store the value function. Using LQR (next chapter), dynamic programming can also be used for continuous state-action spaces, but this requires the system to be linear and the reward to be quadratic with Gaussian noise. But the world and therefore robots are not linear, but in most cases highly nonlinear.}
		% end

		\subsection{Continuous Optimal Control}
			\paragraph{What is the LQR problem?}
			\answer{An LQR problem is given with continuous states and actions with linear state transition dynamics with Gaussian noise, i.e. \( p(\vec{x}_{t + 1} \given \vec{x}_t, \vec{u}_t) = \mathcal{N}\big( \vec{x}_{t + 1} \biggiven \mat{A}_t \vec{x}_t + \mat{B}_t \vec{u}_t\, \mat{\Sigma}_t \big) \). The reward function is quadratic in the states and actions, i.e. \( r_t(\vec{x}, \vec{u}) = -\vec{x}^T \mat{R}_t \vec{x} - \vec{u}^T \mat{H}_t \vec{u} \) with symmetric and positive definite matrices \( \mat{R}_t \) and \( \mat{H}_t \). The optimal policy \(\pi^\ast\) then maximizes the cumulative reward \( J_\pi \) over a finite horizon \(T\). This is, besides the discrete case, the only solvable optimal control problem.}

			\paragraph{Derive the LQR value function and optimal policy for the basic case.}
			\answer{This can be done by following the principle of dynamic programming, i.e. first compute the value function for the last time step and subsequently calculate the Q-function, the optimal policy and the value function for the previous time step. This is done by applying the Bellman equation. See \autoref{subsec:lqr} for the full derivation.}

			\paragraph{What is the form of the solution (value function and policy)? What is their interpretation (qualitatively)?}
			\answer{The value function is of the form \( V_t(\vec{x}) = \vec{x}^T \mat{V}_t \vec{x} + \vec{x}^T \vec{v}_t \), i.e. it is quadratic-linear. The optimal control input is of the form \( \vec{u}_t^\ast = \mat{K}_t (\vec{x}_t - \vec{x}_d) + \vec{k}_t \), i.e. it is a time-varying P-controller!}

			\paragraph{What to do when dynamics and/or rewards are not linear? What the pitfalls?}
			\answer{It is possible to linearize the dynamics and/or the rewards using a Taylor expansion and cutting off the higher-order terms. This, however, leads to oscillations and does only work for systems that are not too nonlinear. This also causes the policies to often not work on the real system due to modeling errors.}

			\paragraph{What are the potential issues when using learning models for trajectory optimization?}
			\answer{One major issue is that the optimizer is prone to exploit error in the model. When using learned models, this might mean to jump out of the space the system learned in, causing unpredictable behaviors. Such policies will most likely never work on the real system as it does not exhibit the same errors.}
		% end
	% end

	\section{Approximate Optimal Control} % 4a.31, 4b.29
		\todo{Content}
	% end

	\section{State Estimation} % 6.64
		\todo{Content}
	% end

	\section{Model Learning}
		\paragraph{Why do we need to learn models?}
		\answer{Building models by hand requires a lot of work (e.g. for disassembling and measuring the robot) and can often not cover complex behavior like friction. But not modeling e.g. friction may result in policies that are not transferable to the real robot, or the motion planning to fail. Learning models from data can overcome these issues.}

		\paragraph{What models exist in robotics?}
		\answer{Forward/inverse kinematics, forward/inverse dynamics, sensor models, discrete-time models, continuous-time models, \dots}

		\paragraph{What's a white-box? What's a black-box? What's a gray-box?}
		\answer{In a white-box model, lots of prior domain knowledge is used to build the model. Example are using physics-based approaches like Lagrangian mechanics to find the equations and then utilize system identification to find e.g. the masses. A black-box model is the opposite, where no prior knowledge is used and the complete model is inferred from data, e.g. using a neural network. A gray-box combined both worlds. It used some prior knowledge (like energy conservation), but also uses a black-box model for learning non-modeled behavior (e.g. friction or handling the residual error).}

		\paragraph{How to excite a system to get good data for model learning?}
		% TODO: Discuss the answer below with Joe.
		\answer{Use impulse responses for analog signals and step functions for digital signals. As these are not practical (due to high frequencies that may damage the robot), a low-pass filter has to be used. But this again limits the diversity of signals. Another approach is active learning, i.e. letting the model decide where to get new data is also a good option. In practice, out-of-phase sinuses create very diverse end-effector trajectories that are good for model learning.}

		\paragraph{How to learn a linear Gaussian dynamical system model?}
		\answer{The LGDS system model can be learned using an expectation-maximization algorithm that in the E-step uses a Kalman smoother for finding the expected latent states and subsequently maximized the expected log-likelihood in the M-step. This guarantees monotonic improvement.}

		\paragraph{How to make black-box models physically safe to learn?}
		% TODO: Discuss the answer below with Joe.
		\answer{This can be done by impose inductive biases, e.g. energy conservation, on the model. This way it is less likely for the model to be unsafe on the real system. Also for generating data, it is required to low-pass filter the frequencies as high frequencies may cause unsafe movements of the robot.}

		\paragraph{How can differentiable physics help?}
		\answer{Differentiable physics makes it possible to use black-box models combined with white-box models like the Newton-Euler algorithm. By implementing the algorithm in an autograd engine, the parameters can be trained with gradient decent, making it simple to add a neural network, e.g. for handling the residual error by adding it to the result of the Newton-Euler algorithm.}
	% end

	\section{Policy Representations}
		\paragraph{What is a policy?}
		\answer{A policy maps a state \(\vec{s}\) to an action \(\vec{a}\). By using a probabilistic policy \( \pi(\vec{a} \given \vec{s}) \), lot of difference behaviors like exploration can be encoded.}

		\paragraph{What off-the-shelf representations can be used for robot learning?}
		\answer{A lot of standard machine learning regression models can be used to represent policies, e.g. neural networks, linear regression with RBF features, Gaussian process, \dots}

		\paragraph{How do off-the-self representations compare to trajectory-based representations?}
		\answer{Off-the-shelf policies often do not generalize well which can lead to drastic failures. Also it is hard to encode inductive biases, achieve scalability for a high number of degrees of freedom, combine movements. Sample efficiency is also a problem as off-the-shelf methods usually require lots of training data.}

		\paragraph{What are potential advantages of movement primitives?}
		\answer{With movement primitives, it is usually easy to learn a specific movement from demonstrations using simple linear regression. As the forcing function vanishes over time, they are stable by design, and they can encode temporal scaling to execute a movement faster/slower as needed. Also they scale really well for a higher number of degrees of freedom.}

		\paragraph{What are the main ideas of using movement primitives?}
		\answer{The stability of a second-order linear dynamical systems can be easily assured by choosing the eigenvalues accordingly. To produce a moving attractor, a forcing function is added to the dynamical system, causing the goal attractor to move accordingly. As the forcing function is designed to vanish over time, the stability of the system is assured as it becomes a PD-controller for \( t \to \infty \).}

		\paragraph{Why to use dynamical systems? What are advantages and disadvantages?}
		\answer{Dynamical systems are highly expressive in terms of the trajectories they can generate. Also they usually only need a few parameters for generating a variety of different trajectories, exhibiting a desirable parsimonious parameter space. Advantages are that for linear systems, stability analysis is well-studied and simple. For nonlinear dynamics, however, this is different: stability analysis is not well studied and can in most cases only be analyzed using Lyapunov functions which are hard to find. Hence, instability is a real problem with using dynamical systems as trajectory representations!}

		\paragraph{Why use a probabilistic representation?}
		\answer{Using probabilistic representations enables quantifying the uncertainty on the trajectory, marking how important certain via-points are for executing the trajectory by choosing the variance low. A high variance allows variability in the movement and allows for combining and blending trajectories using probabilistic operators. Also, using conditioning, it is possible to specialize the primitive on visiting a specific state.}

		\paragraph{How can nonlinear stable dynamics be obtained?}
		\answer{This is possible using an (easily achievable) stable linear systems and combining it with a bijective transformation function. Because of the bijectivity, the potential energy is not changed and the nonlinear system is stable too. Using invertible flows (bijective neural networks), a huge range of different nonlinear transformations are possible, allowing for various nonlinear stable dynamics.}
	% end

	\section{Model-Based Reinforcement Learning} % 9.59
		\todo{Content}
	% end

	\section{Value Function Methods} % 11.42
		\todo{Content}
	% end

	\section{Policy Search} % 10.59, 12.61
		\todo{Content}
	% end

	\section{Imitation Learning} % N/A
		\todo{Content}
	% end

	\section{Bayesian Reinforcement Learning} % 14.54
		\todo{Content}
	% end
% end
