\lstset{language = Python}



\chapter{Introduction} % 1.1, 1.2, 1.7
	Most of the content in this summary, the ideas, the underlying structure and the image ideas are taken from the lecture \href{https://www.ias.informatik.tu-darmstadt.de/Teaching/RobotLearningLecture}{"Robot Learning"} by \href{https://www.ias.informatik.tu-darmstadt.de/Team/JanPeters}{Prof. Jan Peters}. It is really just a \emph{summary} of the contents of the lecture.

	\todo{Content}

	\section{History} % 1.8, 1.9, 1.10, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.20
		\todo{Content}

		\subsection{Deep Learning} % 1.21, 1.22, 1.23
			\todo{Content}
		% end

		\subsection{How Long Does Learning Take?} % 1.24, 1.25
			\todo{Content}
		% end

		\subsection{Inductive Biases} % 1.26, 1.27, 1.28
			\todo{Content}
		% end
	% end
% end

\chapter{Statistics, Linear Algebra and Calculus Refresher} % 5b.4
	\todo{Content}

	\section{Basics} % 5b.5, 5b.6, 5b.7, 5b.8
		\todo{Content}

		\subsection{Entropy} % 5b.9
			\todo{Content}
		% end

		\subsection{Gaussian Distribution and Properties} % 5b.17, 5b.18, 5b.19
			\todo{Content}
		% end
	% end

	\section{Kullback-Leibler Divergence} % 10.36, 10.37, 10.38
		\todo{Content}

		\subsection{Fisher Information Matrix} % 10.39, 10.40
			\todo{Content}
		% end
	% end

	\section{Monte-Carlo Integration and Gradient Estimation} % 5b.10
		\todo{Content}

		\subsection{Gradient Estimation} % 5b.11, 5b.12, 5b.13
			\todo{Content}
		% end
	% end

	\section{Linear Algebra and Calculus} % 5b.15
		\todo{Content}

		\subsection{Moore-Penrose Pseudo-Inverse} % 5b.16
			\todo{Content}
		% end
	% end
% end

\chapter{Robotics} % 2.1, 2.2, 2.5
	\todo{Content}

	\section{Modeling Robots} % 2.6, 2.7, 2.10, 2.11, 2.12, 2.13
		\todo{Content}

		\subsection{Kinematics} % 2.15, 2.16, 2.17
			\todo{Content}

			\subsubsection{Rotations and Euler Angles} % 2.18, 2.19, 2.20, 2.21, 2.22
				\todo{Content}
			% end

			\subsubsection{Homogeneous Transformations} % 2.23, 2.24, 2.25
				\todo{Content}
			% end

			\subsubsection{Denavit-Hartenberg Convention} % 2.26, 2.27
				\todo{Content}
			% end
		% end

		\subsection{Inverse Kinematics} % 2.90, 2.92, 2.94, 2.95
			\todo{Content}
		% end

		\subsection{Differential Forward Kinematics (Velocities)} % 2.28, 2.30, 2.31, 2.33
			\todo{Content}

			\subsubsection{Singularities} % 2.32
				\todo{Content}
			% end
		% end

		\subsection{Differential Inverse Kinematics} % 2.96, 2.97
			\todo{Content}
		% end

		\subsection{Dynamics} % 2.35, 2.36, 2.37, 2.38
			\todo{Content}

			\subsubsection{Computing the Forces} % 2.39
				\todo{Content}

				\paragraph{Newton-Euler Method} % 2.40
					\todo{Content}
				% end

				\paragraph{Lagrangian Methods} % 2.41, 2.42, 2.43
					\todo{Content}
				% end

				\paragraph{Comparison of Newton-Euler and Lagrangian} % 2.44
					\todo{Content}
				% end
			% end

			\subsubsection{General Forms} % 2.45, 2.47, 2.48, 2.49
				\todo{Content}
			% end
		% end
	% end

	\section{Representing Trajectories} % 2.50, 2.51, 2.52, 2.53
		\todo{Content}

		\subsection{Splines} % N/A
			\todo{Content}

			\subsubsection{Cubic Splines} % 2.54, 2.55, 2.56
				\todo{Content}
			% end

			\subsubsection{Quintic Splines} % 2.57, 2.58
				\todo{Content}
			% end
		% end

		\subsection{Alternatives} % 2.59
			\todo{Content}
		% end
	% end

	\section{Control in Joint Space} % 2.60, 2.62, 2.63, 2.64
		\todo{Content}

		\subsection{Linear Feedback Control} % 2.65, 2.66, 2.67, 2.68, 2.69, 2.70
			\todo{Content}

			\subsubsection{P-Controller} % 2.71
				\todo{Content}
			% end

			\subsubsection{PD-Controller} % 2.74, 2.80
				\todo{Content}

				\paragraph{With Gravity Compensation} % 2.76, 2.77
					\todo{Content}
				% end
			% end

			\subsubsection{PID-Controller} % 2.79
				\todo{Content}
			% end
		% end

		\subsection{Model-Based Control} % 2.81, 2.82, 2.83
			\todo{Content}
		% end

		\subsection{Feedforward Control} % 2.84, 2.85, 2.86
			\todo{Content}
		% end
	% end

	\section{Control in Task Space} % 2.87, 2.88, 2.89
		\todo{Content}

		\subsection{Differential Inverse Kinematics} % 2.97
			\todo{Content}
		% end

		\subsection{Jacobian Transpose} % 2.98, 2.99
			\todo{Content}
		% end

		\subsection{Jacobian Pseudo-Inverse} % 2.100, 2.101
			\todo{Content}
		% end

		\subsection{Task-Prioritization with Null-Space Movements} % 2.102
			\todo{Content}
		% end

		\subsection{More Advanced Solutions} % 2.103
			\todo{Content}
		% end

		\subsection{Singularities and Damped Pseudo-Inverse} % 2.104, 2.105
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 2.107
		\todo{Content}
	% end
% end

\chapter{Machine Learning Foundations} % 5b.1, 5b.2, 5b.21, 5b.22, 5b.23, 5b.24, 5b.25, 5b.26
	\todo{Content}

	\section{The Six Machine Learning Choices} % 5b.27, 5b.28, 5b.62
		\todo{Content}

		\subsection{Problem Class} % 5b.30, 5b.31, 5b.32, 5b.33, 5b.34, 5b.35
			\todo{Content}
		% end

		\subsection{Problem Assumptions} % 5b.36
			\todo{Content}
		% end

		\subsection{Evaluation} % 5b.37, 5b.38, 5b.39, 5b.40, 5b.41
			\todo{Content}
		% end

		\subsection{Model Type} % 5b.42
			\todo{Content}
		% end

		\subsection{Model Class Selection} % 5b.43, 5b.44, 5b.45
			\todo{Content}
		% end

		\subsection{Algorithm Realization} % 5b.46
			\todo{Content}
		% end

		\subsection{Example} % 5b.47, 5b.48, 5b.49, 5b.50, 5b.51, 5b.52, 5b.53, 5b.54, 5b.55, 5b.56, 5b.57, 5b.58, 5b.59, 5b.60, 5b.61
			\todo{Content}
		% end
	% end

	\section{Evaluation} % 5b.63, 5b.65
		\todo{Content}

		\subsection{Occams Razor} % 5b.66
			\todo{Content}
		% end

		\subsection{Bias and Variance} % 5b.67, 5b.68, 5b.69
			\todo{Content}
		% end

		\subsection{Model Selection} % 5b.70, 5b.71, 5b.72, 5b.73
			\todo{Content}
		% end
	% end

	\section{Frequentis vs. Bayesian Assumptions} % 5b.74, 5b.75
		\todo{Content}

		\subsection{Maximum Likelihood Estimation} % 5b.76, 5b.77, 5b.78, 5b.79
			\todo{Content}
		% end

		\subsection{Bayesian Thinking and Maximum A-Posteriori} % 5b.80, 5b.81, 5b.82
			\todo{Content}

			\subsubsection{Ridge Regression (Tikhonov Regularized Regression)} % 5b.83, 5b.84
				\todo{Content}
			% end

			\subsubsection{Predictions} % 5b.85, 5b.86, 5b.87
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Regression} % 5b.88, 5b.89, 5b.90, 5b.91, 5b.92
			\todo{Content}
		% end
	% end

	\section{Hand-Crafted Feature Construction} % 5b.93, 5b.94
		\todo{Content}

		\subsection{Discrete Inputs} % 5b.95, 5b.96
			\todo{Content}
		% end

		\subsection{Continuous Inputs} % 5b.97
			\todo{Content}

			\subsubsection{One-Hot} % 5b.98
				\todo{Content}
			% end

			\subsubsection{Radial Basis Functions (RBFs)} % 5b.99, 5b.100, 5b.101
				\todo{Content}
			% end
		% end
	% end

	\section{Automatic (Linear) Feature Construction} % 5b.102, 5b.103, 5b.104, 5b.105, 5b.106, 5b.107
		\todo{Content}

		\subsection{Respective Field Weighted Regression (RFWR)} % 5b.108, 5b.109, 5b.110
			\todo{Content}
		% end

		\subsection{Automatic Adaption of RFWR} % 5b.111, 5b.112, 5b.113
			\todo{Content}
		% end
	% end

	\section{Non-Parametric Approaches} % 5b.114, 5b.115, 5b.116, 5b.117
		\todo{Content}

		\subsection{Weighted Linear Regression} % 5b.118, 5b.119, 5b.120, 5b.121, 5b.122
			\todo{Content}
		% end

		\subsection{Locally Weighted Bayesian Linear Regression} % 5b.123
			\todo{Content}
		% end

		\subsection{Kernel Methods} % 5b.124, 5b.125, 5b.128
			\todo{Content}

			\subsubsection{Kernel Ridge Regression} % 5b.126, 5b.127
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Kernel Regression: Gaussian Processes (GPs)} % 5b.129, 5b.130, 5b.131, 5b.136
			\todo{Content}

			\subsubsection{GP-Posterior} % 5b.132, 5b.133, 5b.134, 5b.135
				\todo{Content}
			% end
		% end
	% end

	\section{Neural Networks} % 5c.1, 5c.2, 5c.4, 5c.5, 5c.6, 5c.7, 5c.7, 5c.8, 5c.9
		\todo{Content}

		\subsection{Biology and Neuron Abstraction} % 5c.10, 5c.11, 5c.12, 5c.13
			\todo{Content}
		% end

		\subsection{Components of a Neural Network} % N/A
			\todo{Content}

			\subsubsection{Single- and Multi-Layer Networks} % 5c.14, 5c.15, 5c.16, 5c.17, 5c.18, 5c.19
				\todo{Content}
			% end

			\subsubsection{Topologies} % 5c.20, 5c.21
				\todo{Content}
			% end

			\subsubsection{Activation Functions} % 5c.22, 5c.23, 5c.24, 5c.25
				\todo{Content}
			% end

			\subsubsection{Output Neurons} % 5c.26
				\todo{Content}
			% end

			\subsubsection{Loss Functions} % 5c.27
				\todo{Content}
			% end
		% end

		\subsection{Forward- and Backpropagation} % 5c.28, 5c.29, 5c.30
			\todo{Content}

			\subsubsection{Forwardpropagation} % 5c.31, 5c.32
				\todo{Content}
			% end

			\subsubsection{Backpropagation} % 5c.33, 5c.34, 5c.35, 5c.37
				\todo{Content}

				\paragraph{Skip Connections} % 5c.36
					\todo{Content}
				% end
			% end

			\subsubsection{Finite Differences} % 5c.38, 5c.39
				\todo{Content}
			% end

			\subsubsection{Automatic Differentiation} % 5c.40
				\todo{Content}
			% end
		% end

		\subsection{Efficient Gradient Descent} % 5c.42, 5c.43, 5c.44, 5c.47
			\todo{Content}

			\subsubsection{Stochastic Gradient Descent} % 5c.45, 11.19, 11.20
				\todo{Content}
			% end

			\subsubsection{Mini-Batch Gradient Descent} % 5c.46
				\todo{Content}
			% end
		% end

		\subsection{Choosing the Learning Rate} % 5c.48, 5c.49, 5c.50, 5c.51
			\todo{Content}

			\subsubsection{Plateaus and Valleys} % 5c.52
				\todo{Content}
			% end

			\subsubsection{Adaptive Learning Rates} % N/A
				\todo{Content}

				\paragraph{Momentum} % 5c.53
					\todo{Content}
				% end

				\paragraph{Adadelta} % 5c.54
					\todo{Content}
				% end

				\paragraph{Adam} % 5c.55
					\todo{Content}
				% end
			% end
		% end

		\subsection{Choosing the Descent Direction} % N/A
			\todo{Content}

			\subsubsection{Hessian Approaches} % 5c.56
				\todo{Content}
			% end

			\subsubsection{Conjugate Gradient} % 5c.57
				\todo{Content}
			% end

			\subsubsection{Levenberg-Marquardt} % 5c.58
				\todo{Content}
			% end
		% end

		\subsection{Initialization of the Parameters} % 5c.59, 5c.60
			\todo{Content}
		% end

		\subsection{Overfitting} % 5c.61, 5c.62, 5c.63, 5c.64, 5c.65, 5c.66
			\todo{Content}

			\subsubsection{Weight Decay} % 5c.67
				\todo{Content}
			% end

			\subsubsection{Early Stopping} % 5c.68
				\todo{Content}
			% end

			\subsubsection{Input Noise Augmentation} % 6.69
				\todo{Content}
			% end

			\subsubsection{Dtopout} % 5c.70
				\todo{Content}
			% end

			\subsubsection{Batch Normalization} % 5c.71
				\todo{Content}
			% end
		% end

		\subsection{Theoretical Analysis} % 5c.72, 5c.73, 5c.75, 5c.76, 5c.77
			\todo{Content}

			\subsubsection{Universal Function Approximation Theorem} % 5c.74
				\todo{Content}
			% end
		% end

		\subsection{Network Architectures} % 5c.78, 5c.79, 5c.80
			\todo{Content}

			\subsubsection{Convolutional Neural Networks (CNNs)} % 5c.81, 5c.82, 5c.83, 5c.84, 5c.85, 5c.86
				\todo{Content}
			% end

			\subsubsection{Recurrent Neural Networks (RNNs)} % 5c.78
				\todo{Content}
			% end
		% end

		\subsection{Neural Networks in Robotics} % 5c.88, 5c.89
			\todo{Content}

			\subsubsection{Value Functions} % 5c.90, 5c.91
				\todo{Content}
			% end

			\subsubsection{Policies} % 5c.92, 5c.93
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 5b.138, 5c.95
		\todo{Content}
	% end
% end

\chapter{Optimal Control} % N/A
	\todo{Content}

	\section{Discrete State-Action Space} % 3a.1, 3a.2
		\todo{Content}

		\subsection{Foundations} % N/A
			\todo{Content}

			\subsubsection{Dynamic Programing and Principle of Optimality} % 3a.6, 3a.7
				\todo{Content}
			% end

			\subsubsection{Markov Decision Process and Policies} % 3a.9, 3a.10
				\todo{Content}
			% end
		% end

		\subsection{Finite-Horizon Optimal Control} % 3a.12, 3a.13
			\todo{Content}

			\subsubsection{Value and State-Action Value Functions} % 3a.14, 3a.15, 3a.17
				\todo{Content}
			% end

			\subsubsection{Value Iteration} % 3a.18, 3a.19
				\todo{Content}
			% end

			\subsubsection{Consequences of a Finite Time Horizon} % 3a.21
				\todo{Content}
			% end
		% end

		\subsection{Infinite-Horizon Optimal Control} % 3a.22, 3a.23, 3a.24, 3a.25
			\todo{Content}

			\subsubsection{Value and State-Action Value Functions} % 3a.30, 3a.31, 3a.32, 3a.33
				\todo{Content}
			% end

			\subsubsection{Value Iteration} % 3a.26
				\todo{Content}
			% end

			\subsubsection{Policy Iteration} % 3a.29, 3a.34, 3a.36, 3a.37
				\todo{Content}
			% end
		% end
	% end

	\section{Continuous State-Action Space} % 3b.1, 3b.2
		\todo{Content}

		\subsection{Modeling the System} % 3b.5, 3b.7, 3b.8, 3b.9, 3b.10, 3b.11
			\todo{Content}
		% end

		\subsection{Linear Quadratic Regulator (LQR)} % 3b.12, 3b.13, 3b.14, 3b.15
			\todo{Content}

			\subsubsection{Recap of Value Iteration} % 3b.17, 3b.18
				\todo{Content}
			% end

			\subsubsection{Solving the Optimal Control Problem} % 3b.19, 3b.20, 3b.21, 3b.22, 3b.23, 3b.24, 3b.25, 3b.26, 2.27
				\todo{Content}

				\paragraph{Computing the Expectation} % 3b.21
					\todo{Content}
				% end

				\paragraph{Computing the Maximization} % 3b.23
					\todo{Content}
				% end
			% end
		% end

		\subsection{Approximating Nonlinear Systems} % 3b.32, 3b.33, 3b.34, 3b.35, 3b.36
			\todo{Content}

			\subsubsection{Local Solutions by Linearization} % 3b.37, 3b.38, 3b.39
				\todo{Content}
			% end
		% end

		\subsection{Optimal Control with Learned Models} % 3b.42, 3b.43, 3b.44, 3b.45, 3b.46, 3b.48, 3b.51
			\todo{Content}

			\subsubsection{State-Of-The-Art Approaches} % 3b.52, 3b.55
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 3a.39, 3a.40, 3a.41, 3a.42, 3b.59
		\todo{Content}
	% end
% end

\chapter{Approximate Optimal Control} % N/A
	\todo{Content}

	\section{Differential Dynamic Programming (DDP)} % 4a.1, 4a.2, 4a.3, 4a.12
		\todo{Content}

		\subsection{Locally Nonlinear LQR} % 4a.13, 4a.14
			\todo{Content}
		% end

		\subsection{Differential Dynamic Programming} % 4a.15, 4a.16, 4a.17
			\todo{Content}

			\subsubsection{Implementation Details} % 4a.18, 4a.19, 4a.20
				\todo{Content}
			% end
		% end

		\subsection{Iterative LQR} % 4a.22, 4a.23, 4a.24
			\todo{Content}
		% end

		\subsection{Stochastic DDP} % 4a.25, 4a.26
			\todo{Content}
		% end

		\subsection{Guided Policy Search} % 4a.27, 4a.28
			\todo{Content}
		% end
	% end

	\section{Approximate Dynamic Programming} % 4b.1, 4b.2, 4b.6
		\todo{Content}

		\subsection{Function Approximation} % 4b.7, 4b.8, 4b.9, 4b.10, 4b.11, 4b.12
			\todo{Content}
		% end

		\subsection{Approximate Value Iteration} % 4b.13, 4b.14
			\todo{Content}

			\subsubsection{Theoretical Analysis and Bellman Operator} % 4b.15, 4b.16, 4b.17
				\todo{Content}
			% end

			\subsubsection{Approximation Error and Performance Loss} % 4b.18, 4b.19, 4b.20
				\todo{Content}
			% end
		% end

		\subsection{Approximate Policy Iteration} % 4b.21, 4b.23, 4b.24, 4b.25
			\todo{Content}

			\subsubsection{Approximation Error and Performance Loss} % 4b.26
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 4a.30, 4b.28
		\todo{Content}
	% end
% end

\chapter{State Estimation} % 6.1, 6.2, 6.5, 6.6
	\todo{Content}

	\section{Kalman Filter as an Optimal Filter} % 6.8
		\todo{Content}

		\subsection{Observers} % 6.9, 6.10
			\todo{Content}
		% end

		\subsection{Optimal Observers} % 6.11, 6.12, 6.13, 6.14, 6.15, 6.16, 6.17, 6.18
			\todo{Content}
		% end

		\subsection{Geometric Perspective} % 6.19
			\todo{Content}
		% end
	% end

	\section{Kalman Filter as Bayesian Inference} % 6.20, 6.21, 6.22, 6.23, 6.24
		\todo{Content}
	% end

	\section{Partially Observed Optimal Control} % 6.27, 6.28, 6.29, 6.30
		\todo{Content}
	% end

	\section{Extended, Unscented and Particle Filter} % 6.32, 6.33
		\todo{Content}

		\subsection{Extended Kalman Filter (EKF)} % 6.34
			\todo{Content}
		% end

		\subsection{Cubature Kalman Filter (CKF)} % 6.35, 6.36, 6.37, 6.38
			\todo{Content}
		% end

		\subsection{Unscented Kalman Filter (UKF)} % 6.39
			\todo{Content}
		% end

		\subsection{Particle Filter / Sequential Monte Carlo (PF/SMC)} % 6.40, 6.41, 6.56
			\todo{Content}

			\subsubsection{Importance Sampling} % 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Sampling} % 6.48, 6.49, 6.50, 6.51, 6.52
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Resampling} % 6.53, 6.54, 6.55
				\todo{Content}
			% end
		% end

		\subsection{Examples} % N/A
			\todo{Content}

			\subsubsection{Approximate Message Passing} % 6.44, 6.45, 6.46, 6.47
				\todo{Content}
			% end

			\subsubsection{Pendulum} % 6.57, 6.58, 6.59, 6.60
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 6.62, 6.63
		\todo{Content}
	% end
% end

\chapter{Model Learning} % 7.1, 7.2
	\todo{Content}

	\section{Models in Robotics} % 7.4, 7.5, 7.6, 7.7, 7.8, 7.9
		\todo{Content}
	% end

	\section{Modling Assumptions: White, Black and Gray} % 7.10, 7.11
		\todo{Content}

		\subsection{White-Box Strategy} % 7.12
			\todo{Content}
		% end

		\subsection{Black-Box Strategy} % 7.13
			\todo{Content}
		% end

		\subsection{Gray-Box Strategy} % 7.14
			\todo{Content}
		% end
	% end

	\section{System Identification and Signal Processing} % 7.15, 7.16, 7.17
		\todo{Content}

		\subsection{Impulse Response} % 7.18, 7.19, 7.20, 7.21, 7.22, 7.23
			\todo{Content}
		% end

		\subsection{Step Response} % 7.24, 7.25
			\todo{Content}
		% end

		\subsection{Characterization of Dynamical Systems} % 7.26, 7.27
			\todo{Content}
		% end

		\subsection{Frequency Analysis} % 7.28, 7.29
			\todo{Content}
		% end

		\subsection{Ornstein-Uhlenbeck Process} % 7.30, 7.31
			\todo{Content}
		% end

		\subsection{Active Learning} % 7.32, 7.33, 7.34
			\todo{Content}
		% end
	% end

	\section{Learning Models} % 7.36, 7.37
		\todo{Content}

		\subsection{Linear Gaussian Dynamical Systems (LGDS)} % 7.38, 7.39, 7.40, 7.41, 7.42, 7.43, 7.44
			\todo{Content}
		% end
	% end

	\section{Case Studies} % 7.47
		\todo{Content}

		\subsection{Combining Rigid Body Dynamics and GPs} % 7.48, 7.49, 7.50
			\todo{Content}
		% end

		\subsection{Deep Lagrangian Networks} % 7.51, 7.52, 7.53, 7.54, 7.55
			\todo{Content}
		% end

		\subsection{The Differentiable Recusive Newton-Euler Algorithm} % 7.56, 7.57, 7.58, 5.59, 7.60, 7.61, 7.62, 7.63
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 7.65
		\todo{Content}
	% end
% end

\chapter{Policy Representations} % 8.1, 8.4
	\todo{Content}

	\section{Parametric Policies} % 8.3, 8.7, 8.8, 8.9
		\todo{Content}
	% end

	\section{Off-The-Shelf Policies} % 8.10, 8.16
		\todo{Content}

		\subsection{Linear Basis Functions} % 8.11, 8.12, 8.13
			\todo{Content}
		% end

		\subsection{Radial Basis Functions (RBFs)} % 8.14, 8.15
			\todo{Content}
		% end
	% end

	\section{Movement Primitives} % 8.17, 8.18, 8.20, 8.21, 8.22, 8.23, 8.24, 8.25
		\todo{Content}

		\subsection{Dynamic Movement Primitives (DMDs)} % 8.26, 8.27, 8.28
			\todo{Content}

			\subsubsection{Temporal Scaling} % 8.29, 8.30, 8.34
				\todo{Content}
			% end

			\subsubsection{Multiple Degrees of Freedom} % 8.33
				\todo{Content}
			% end

			\subsubsection{Imitation Learning} % 8.35
				\todo{Content}
			% end
		% end

		\subsection{Probabilistic Movement Primitives (ProMPs)} % 8.40, 8.41, 8.42, 8.43, 8.44, 8.45
			\todo{Content}

			\subsubsection{Conditioning} % 8.46
				\todo{Content}
			% end

			\subsubsection{Combination} % 8.47
				\todo{Content}
			% end
		% end

		\subsection{Time-Independent Stable Movement Primitives} % 8.51
			\todo{Content}

			\subsubsection{Nonlinear Stable Dynamical Systems} % 8.52, 8.53, 8.54
				\todo{Content}
			% end
		% end

		\subsection{Imitation Flows} % 8.55, 8.56, 8.57
			\todo{Content}
		% end

		\subsection{Libraries of Primitives} % 8.59, 8.60, 8.61
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 8.64
		\todo{Content}
	% end
% end

\chapter{Model-Based Reinforcement Learning} % 9.1, 9.2, 9.5, 9.6
	\todo{Content}

	\section{Differentiation of Model-Based and Model-Free} % 9.7, 9.8
		\todo{Content}

		\subsection{Sample Efficiency} % 9.9, 9.10, 9.11
			\todo{Content}
		% end
	% end

	\section{Domain Knowledge in Reinforcement Learning} % 9.13, 9.14
		\todo{Content}

		\subsection{Performance Bias} % 9.15, 9.16, 9.17, 9.22, 9.25
			\todo{Content}
		% end

		\subsection{Local Optima and Sample-Based Methods} % 9.18, 9.21
			\todo{Content}

			\subsubsection{The Cross-Entropy Method (CEM)} % 9.19
				\todo{Content}
			% end

			\subsubsection{(Model Predictive) Path Integral Control (MPPI)} % 9.20
				\todo{Content}
			% end
		% end

		\subsection{Numerical Sensitivity} % 9.23
			\todo{Content}

			\subsubsection{Backprop-Through-Time} % 9.24
				\todo{Content}
			% end
		% end

		\subsection{Catastrophic Model Errors} % 9.26
			\todo{Content}
		% end
	% end

	\section{Optimism and Pessimism in Reinforcement Learning} % 9.27, 9.28, 9.29
		\todo{Content}

		\subsection{Aleatoric and Epistemic Uncertainty} % 9.30, 9.31
			\todo{Content}
		% end

		\subsection{Optimism Under Uncertainty} % 9.31, 9.32, 9.33, 9.34, 9.35, 9.39
			\todo{Content}

			\subsubsection{Neural Linear Models} % 9.36
				\todo{Content}
			% end

			\subsubsection{Variational Inference} % 9.37
				\todo{Content}
			% end

			\subsubsection{Ensembles} % 9.38
				\todo{Content}
			% end
		% end
	% end

	\section{Replanning} % 9.40, 9.41, 9.42
		\todo{Content}
	% end

	\section{Case Studies} % 9.43
		\todo{Content}

		\subsection{Probabilistic Learning for Control (PILCO)} % 9.44, 9.45, 9.46
			\todo{Content}

		\subsection{Guided Policy Search (GPS)} % 9.48, 9.49, 9.50, 9.51, 9.52
			\todo{Content}
		% end

		\subsection{Model Predictive Control (MPC)} % 9.54, 9.56
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 9.58, 12.63
		\todo{Content}
	% end
% end

\chapter{Value Function Methods} % 11.1, 11.2
	\todo{Content}

	\section{Recap of Dynamic Programming} % 11.4, 11.5, 11.6, 11.7
		\todo{Content}
	% end

	\section{Temporal Differences} % 11.8, 11.9, 11.10
		\todo{Content}

		\subsection{Policy Evaluation} % 11.11, 11.12, 11.13
			\todo{Content}
		% end

		\subsection{Policy Improvement} % 11.14, 11.15
			\todo{Content}
		% end
	% end

	\section{Value Function Approximation} % 11.16, 11.17, 11.18, 11.21, 11.22, 11.23
		\todo{Content}
	% end

	\section{Batch Reinforcement Learning Methods} % 11.25, 11.26
		\todo{Content}

		\subsection{Least-Squares Temporal Differences (LSTD)} % 11.27, 11.28, 11.29, 11.30
			\todo{Content}
		% end

		\subsection{Fitted Q-Iteration} % 11.31, 11.32, 11.33
			\todo{Content}
		% end
	% end

	\section{Case Study: Robot Soccer} % 11.34, 11.35, 11.36, 11.37, 11.38, 11.39
		\todo{Content}
	% end

	\section{Wrap-Up} % 11.41, 12.64
		\todo{Content}
	% end
% end

\chapter{Policy Search} % N/A
	\todo{Content}

	\section{Categorization of Policy Search} % 10.6, 10.7, 10.8
		\todo{Content}

		\subsection{Episode- vs. Step-Based Evaluation} % 10.9, 10.10, 10.11
			\todo{Content}
		% end

		\subsection{Epsiode-Based Policy Search} % 10.12, 10.13, 10.14
			\todo{Content}
		% end

		\subsection{Exploration vs. Exploitation} % 10.15, 10.16, 10.17, 10.18, 10.19
			\todo{Content}
		% end
	% end

	\section{Policy Gradient Methods} % 10.1, 10.2, 10.5
		\todo{Content}

		\subsection{Policy Gradients} % 10.20, 10.21
			\todo{Content}

			\subsubsection{Gradient Computation} % N/A
				\todo{Content}

				\paragraph{Finite Differences} % 10.22
					\todo{Content}
				% end

				\paragraph{Likelihood Policy Gradients} % 10.23
					\todo{Content}
				% end

				\paragraph{Baselines} % 10.24
					\todo{Content}
				% end
			% end

			\subsubsection{Step-Based Policy Gradient Methods} % 10.25, 10.26
				\todo{Content}

				\paragraph{Likelihood Ratio Gradient} % 10.27, 10.28
					\todo{Content}
				% end

				\paragraph{Using the Rewards to Come} % 10.29
					\todo{Content}
				% end
			% end

			\subsubsection{Choosing the Step Size, Metrics in Standard Gradients} % 10.30, 10.31, 10.32, 10.33
				\todo{Content}
			% end
		% end

		\subsection{Relative Entropy and Natural Gradient} % 10.34, 10.35, 10.41, 10.42, 10.43
			\todo{Content}

			\subsubsection{Computing the Natural Gradient} % N/A
				\todo{Content}

				\paragraph{Episode-Based} % 10.44
					\todo{Content}
				% end

				\paragraph{Step-Based} % 10.45
					\todo{Content}
				% end
			% end

			\subsubsection{Compatible Function Approximation} % 10.46, 10.47, 10.48
				\todo{Content}

				\paragraph{Connection to Value Function Approximation} % 10.49, 10.50, 10.51
					\todo{Content}
				% end

				\paragraph{Episodic Natural Actor-Critic} % 10.52, 10.53, 10.54
					\todo{Content}
				% end
			% end
		% end
	% end

	\section{Probabilistic Policy Search} % 12.1, 12.2
		\todo{Content}

		\subsection{Success Matching Principle} % 12.5, 12.6, 12.7
			\todo{Content}
		% end

		\subsection{Weighted Maximum Likelihood} % 12.8, 12.9, 12.10, 12.11
			\todo{Content}

			\subsubsection{Difference to Policy Gradients} % 12.12
				\todo{Content}
			% end

			\subsubsection{Computing the Weights} % 12.13
				\todo{Content}

				\paragraph{Notes on Expectation Maximization} % 12.14
					\todo{Content}
				% end

				\paragraph{Notes on the Exponential Transformation} % 12.15
					\todo{Content}
				% end
			% end

			\subsubsection{Illustration and Results} % 12.16, 12.17, 12.19, 12.21
				\todo{Content}
			% end
		% end

		\subsection{Relative Entropy Policy Search (REPS)} % 12.22, 12.23
			\todo{Content}

			\subsubsection{Optimization Problem} % 12.24
				\todo{Content}

				\paragraph{Solving} % 12.25, 12.26
					\todo{Content}
				% end
			% end
		% end

		\subsection{REPS for Contextual Policy Search} % 12.28, 12.29, 12.30
			\todo{Content}

			\subsubsection{Optimization Problem} % 12.31
				\todo{Content}

				\paragraph{Solving} % 12.32, 12.33, 12.34, 12.35
					\todo{Content}
				% end
			% end

			\subsubsection{Contextual Policies with Weighted ML} % 12.36
				\todo{Content}
			% end
		% end

		\subsection{Learning Versatile Solutions} % 12.40, 12.41, 12.47
			\todo{Content}

			\subsubsection{Illustration} % 12.42, 12.43
				\todo{Content}
			% end

			\subsubsection{Hierarchy} % 12.44
				\todo{Content}

				\paragraph{Naive Hierarichal Approach} % 12.45, 12.46
					\todo{Content}
				% end

				\paragraph{Hierarichal REPS (HiREPS)} % 12.48, 12.49
					\todo{Content}
				% end
			% end
		% end

		\subsection{Sequencing Movement Primitives} % 12.54, 12.55, 12.56
			\todo{Content}

			\subsubsection{Sequential REPS} % 12.57, 12.65, 12.66
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 10.55, 10.56, 10.58, 12.60
		\todo{Content}
	% end
% end

\chapter{Imitation Learning: Behavioral Cloning and Inverse RL} % 13.1, 13.3, 13.4, 13.5, 13.8, 13.9, 13.10, 13.11, 13.12
	\todo{Content}

	\section{Distribution Matching} % 13.13
		\todo{Content}

		\subsection{Behavioral Cloning} % 13.14, 13.43
			\todo{Content}

			\subsubsection{Direct Behavioral Cloning} % 13.15, 13.16, 13.17, 13.18, 13.19, 13.20
				\todo{Content}
			% end

			\subsubsection{DAGGER: New Samples to Learn to Recover} % 13.21
				\todo{Content}
			% end

			\subsubsection{DART: Robustness in Imitation Learning} % 13.22
				\todo{Content}
			% end
		% end

		\subsection{Generative Adversarial Learning} % 13.23, 13.24, 13.25
			\todo{Content}
		% end
	% end

	\section{Inverse Reinforcement Learning} % 13.26, 13.27, 13.42, 13.44
		\todo{Content}

		\subsection{Basic Principle} % 13.46, 13.49, 13.55
			\todo{Content}

			\subsubsection{Feature-Based Reward Function} % 13.47, 13.48
				\todo{Content}
			% end

			\subsubsection{Constraint Generation} % 13.50, 13.51, 13.58
				\todo{Content}
			% end

			\subsubsection{Ill-Posed Problem} % 13.52, 13.54
				\todo{Content}
			% end

			\subsubsection{(Structured) Max. Margin Solution} % 13.53, 13.56
				\todo{Content}
			% end

			\subsubsection{Expert Suboptimality} % 13.57, 13.60
				\todo{Content}
			% end
		% end

		\subsection{Feature Matching by Max. Entropy} % 13.61
			\todo{Content}

			\subsubsection{Max. Entropy Approach to Inference} % 13.62
				\todo{Content}
			% end

			\subsubsection{Max. Entropy Approach to Inverse RL} % 13.63
				\todo{Content}
			% end

			\subsubsection{Maximum-Casual-Entropy Inverse RL} % 13.64, 13.65
				\todo{Content}
			% end
		% end

		\subsection{Reward-Parameterized Policies} % 13.66, 13.67
			\todo{Content}
		% end
	% end

	\section{Case Studies} % 13.68, 13.69
		\todo{Content}

		\subsection{Highway Driving} % 13.70
			\todo{Content}
		% end

		\subsection{Max. Margin} % 13.71, 13.72
			\todo{Content}
		% end

		\subsection{Parking Lot Navigation} % 13.73, 13.74, 13.75, 13.76, 13.77
			\todo{Content}
		% end

		\subsection{Human Path Planning} % 13.78, 13.79, 13.80, 13.81, 13.82, 13.83
			\todo{Content}
		% end

		\subsection{Goal Inference} % 13.84, 13.85
			\todo{Content}
		% end

		\subsection{Quadruped} % 13.86, 13.87, 13.88
			\todo{Content}
		% end

		\subsection{Extreme Helicopter Flight} % 13.89, 13.90, 13.91, 13.92
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 13.45, 13.94
		\todo{Content}
	% end
% end

\chapter{Bayesian Reinforcement Learning} % 14.1, 14.2, 14.15, 15.16, 14.17, 14.18
	\todo{Content}

	\section{Recap of Bayesian Methods} % 14.4, 14.5, 14.6, 14.7, 14.8, 14.9, 14.14
		\todo{Content}

		\subsection{Conjugate Prior} % 14.10, 14.11, 14.12, 14.13
			\todo{Content}
		% end
	% end

	\section{Bandits and Thompson Sampling} % 14.19, 14.20, 14.21
		\todo{Content}

		\subsection{Restaurant Selection} % 14.22, 14.23, 14.24, 14.25, 14.26, 14.27, 14.28, 14.29
			\todo{Content}
		% end
	% end

	\section{Model-Based Bayesian RL for Discrete MDPs} % 14.30, 14.31, 14.32, 14.33
		\todo{Content}

		\subsection{Belief} % 14.34, 14.35
			\todo{Content}
		% end

		\subsection{State Transition Model} % 14.36, 14.37
			\todo{Content}
		% end

		\subsection{Optimal Value Function for BAMPDs} % 14.38, 14.39
			\todo{Content}
		% end

		\subsection{Solving for the Optimal Value Function} % 14.40
			\todo{Content}
		% end
	% end

	\section{Continuous MDPs and Dual Control} % 14.41, 14.42, 14.43, 14.44
		\todo{Content}

		\subsection{One-Dimensional Linear Gaussian Dual Control} % 14.45, 14.46, 14.47, 14.48, 14.49, 14.50
			\todo{Content}
		% end

		\subsection{Practical Dual Control} % 14.51
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 14.53
		\todo{Content}
	% end
% end

\chapter{Outlook} % 15.1
	\todo{Content}

	\section{Recap} % 15.4, 15.5, 15.6, 15.7, 15.8, 15.9, 15.10, 15.11
		\todo{Content}
	% end

	\section{Open Research Question} % 15.13
		\todo{Content}
	% end
% end
