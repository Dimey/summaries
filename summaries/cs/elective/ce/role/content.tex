\lstset{language = Python}



\chapter{Introduction} % 1.1, 1.2, 1.7
	Most of the content in this summary, the ideas, the underlying structure and the image ideas are taken from the lecture \href{https://www.ias.informatik.tu-darmstadt.de/Teaching/RobotLearningLecture}{"Robot Learning"} by \href{https://www.ias.informatik.tu-darmstadt.de/Team/JanPeters}{Prof. Jan Peters}. It is really just a \emph{summary} of the contents of the lecture.

	\todo{Content}

	\section{History} % 1.8, 1.9, 1.10, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.20
		\todo{Content}

		\subsection{Deep Learning} % 1.21, 1.22, 1.23
			\todo{Content}
		% end

		\subsection{How Long Does Learning Take?} % 1.24, 1.25
			\todo{Content}
		% end

		\subsection{Inductive Biases} % 1.26, 1.27, 1.28
			\todo{Content}
		% end
	% end
% end

\chapter{Statistics, Linear Algebra and Calculus Refresher} % 5b.4
	\todo{Content}

	\section{Basics} % 5b.5, 5b.6, 5b.7, 5b.8
		\todo{Content}

		\subsection{Entropy} % 5b.9
			\todo{Content}
		% end

		\subsection{Gaussian Distribution and Properties} % 5b.17, 5b.18, 5b.19
			\todo{Content}
		% end
	% end

	\section{Kullback-Leibler Divergence} % 10.36, 10.37, 10.38
		\todo{Content}

		\subsection{Fisher Information Matrix} % 10.39, 10.40
			\todo{Content}
		% end
	% end

	\section{Monte-Carlo Integration and Gradient Estimation} % 5b.10
		\todo{Content}

		\subsection{Gradient Estimation} % 5b.11, 5b.12, 5b.13
			\todo{Content}
		% end
	% end

	\section{Linear Algebra and Calculus} % 5b.15
		\todo{Content}

		\subsection{Moore-Penrose Pseudo-Inverse} % 5b.16
			\todo{Content}
		% end
	% end
% end

\chapter{Robotics}
	This chapter covers the basics of robotics, covering modeling position, velocity, acceleration and forces as well as representing trajectories. Also the main concepts of linear and model-based control are covered.

	The definition of what a robot are quite diverse. The robotics institute of America defines a robot as follows: "A robot is a reprogrammable multi-functional manipulator designed to move material, parts, tools, or specialized devices through variable programmed motions for the performance of a variety of tasks." Another, rather inverse, definition is from G. Randelov: "A computer is just an amputee robot."

	\section{Modeling Robots}
		Modeling of a robot can be split into two separate categories: kinematics and dynamics. In \emph{kinematics}, only the geometric properties of the robot are modeled, e.g. the link length. In \emph{dynamics}, the forces acting on the links and joints are analyzed. There are two types of joints that can be used to model every joint out there: revolute and prismatic joints, whilst revolute joints are the most common ones. The displacement of a revolute joint is an angle (typically in radians), the displacement of a prismatic joint is a distance (typically in meters).

		The displacements of all joints in a robot is denoted with a vector \(\vec{q}\), the task-space (e.g. the Cartesian coordinate system of the world) is denoted by \(\vec{x}\) and the state (i.e. the variables of the robot and the environment) is denoted by \(\vec{s}\). The set of places \(\vec{x}\) in the task-space that the robot can reach is called the \emph{workspace}. That is, everything outside the workspace is not reachable.

		Actions that can be taken by the controlled are denoted by \(\vec{u}\) or \(\vec{a}\) and are most often in some way related to torques in the joints. A (control) policy \(\pi\) then maps the state \(\vec{s}\) onto some action \(\vec{u}\) to take, either in a deterministic or stochastic way:
		\begin{itemize}
			\item \eqmakebox[modelingRobotsPolicy][l]{Deterministic:} \( \vec{u} = \pi(\vec{s}) \)
			\item \eqmakebox[modelingRobotsPolicy][l]{Stochastic:}    \( \vec{u} \sim \pi(\vec{u} \given \vec{s}) \)
		\end{itemize}
		The complete system of a robot including the generation of the trajectory to perform as well as the controller is shown in \autoref{fig:robotsCompleteBlockDiagram}.

		\begin{figure}
			\centering
			\begin{tikzpicture}[block/.style = { draw, rectangle, minimum height = 1cm, minimum width = 2.5cm }]
				\node [block] (t) {\textbf{Trajectory}};
				\node [block, right = 2 of t] (c) {\textbf{Control}};
				\node [block, right = 2 of c] (d) {\textbf{Dynamics}};
				\node [block, right = 2 of d] (k) {\textbf{Kinematics}};

				\coordinate [right = 0 of t] (tr1);
				\coordinate [left = 0 of c] (cl1);
				\coordinate [right = 0 of c] (cr1);
				\coordinate [left = 0 of d] (dl1);
				\coordinate [right = 0 of d] (dr1);
				\coordinate [left = 0 of k] (kl1);

				\coordinate [below = 0 of c] (cb);
				\coordinate [left = 0.5 of cb] (cbl);
				\coordinate [below = 1.75 of cbl] (cblb);
				\coordinate [right = 0.5 of cb] (cbr);
				\coordinate [below = 0.75+0.125 of cbr] (cbrb);
				\coordinate [below = 0 of d] (db);
				\coordinate [below = 0.75+0.125 of db] (dbb);
				\coordinate [below = 0 of k] (kb);
				\coordinate [below = 1.75 of kb] (kbb);

				\draw [->] (tr1) -- node[above]{\( \vec{x}_d \), \( \dot{\vec{x}}_d \), \( \ddot{\vec{x}}_t \)} (cl1);
				\draw [->] (cr1) -- node[above]{\( \vec{u} \)} (dl1);
				\draw [->] (dr1) -- node[above]{\( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \)} (kl1);
				\draw [->] (db) -- (dbb) -- node[above]{\( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \)} (cbrb) -- (cbr);
				\draw [->] (kb) -- (kbb) -- node[above]{\( \vec{x} \), \( \dot{\vec{x}} \), \( \ddot{\vec{x}} \)} (cblb) -- (cbl);
			\end{tikzpicture}
			\caption{Block diagram of a complete system with the desired values \( \vec{x}_d \), \( \dot{\vec{x}}_d \), \( \ddot{\vec{x}}_t \), the joint angles/velocities/accelerations \( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \), the end-effector positions/velocities/accelerations \( \vec{x} \), \( \dot{\vec{x}} \), \( \ddot{\vec{x}} \), and the motor commands/torques \( \vec{u} \).}
			\label{fig:robotsCompleteBlockDiagram}
		\end{figure}

		\subsection{Kinematics}
			Forward kinematics tackle the problem to get the position of the end-effector given the current joint positions or, more general, the position of any point in reference to a set coordinate system. Forward kinematics therefore describe a mapping from joint- to task-space:
			\begin{equation*}
				\vec{x} = \vec{f}(\vec{q})
			\end{equation*}
			For simple robots, e.g. a two-dimensional robot with only few revolute joints or a robot with only prismatic joints, the forward kinematics can be found straightforwardly with geometric knowledge. For more complex robots, however, more sophisticated methods are needed to not waste a lot of time finding the forward kinematics model.

			\subsubsection{Rotations and Euler Angles}
				To model revolute joints, a decent modeling of rotations is needed. In two-dimensional settings, a rotary transformation around an arbitrary angle \(\alpha\) is described by the following matrix:
				\begin{equation*}
					\mat{R}(\alpha) =
						\begin{bmatrix}
							\cos\alpha & -\sin\alpha \\
							\sin\alpha &  \cos\alpha
						\end{bmatrix}
				\end{equation*}
				In three-dimensional settings, one transformation is needed for a rotation around one of the Cartesian axis. Letting \(x\), \(y\) and \(z\) be the first, second and third Cartesian axis, respectively, the corresponding rotation matrices around an angle \(\alpha\) are given as:
				\begin{align*}
					\mat{R}_x(\alpha) =
						\begin{bmatrix}
							1 & 0          & 0           \\
							0 & \cos\alpha & -\sin\alpha \\
							0 & \sin\alpha &  \cos\alpha
						\end{bmatrix}
					&&
					\mat{R}_y(\alpha) =
						\begin{bmatrix}
							 \cos\alpha & 0 & \sin\alpha \\
							 0          & 1 & 0          \\
							-\sin\alpha & 0 & \cos\alpha
						\end{bmatrix}
					&&
					\mat{R}_z(\alpha) =
						\begin{bmatrix}
							\cos\alpha & -\sin\alpha & 0 \\
							\sin\alpha &  \cos\alpha & 0 \\
							0          &  0          & 1
						\end{bmatrix}
				\end{align*}

				One of many methods of representing arbitrary rotations are \emph{Euler angles} which are parameterized by roll \(\psi\), pitch \(\theta\) and yaw \(\phi\). They transform from a coordinate system \(1\) into a coordinate system \(0\) as follows:
				\begin{equation*}
					\mat{R}_1^0 = \mat{R}_z(\phi) \mat{R}_y(\theta) \mat{R}_x(\psi)
				\end{equation*}
				Problems with Euler angles are that they are not unique, i.e. different Euler angles may describe the same rotation, and it is hard to quantify differences between Euler angles. An alternative for describing rotations are \emph{unit quaternions} and angle-axis formulations, where both a rotation axis as well as a rotation angle are given. Both of them solve the singularities with Euler angles and it is easier to computer differences of orientations.
			% end

			\subsubsection{Homogeneous Transformations}
				\emph{Homogeneous transformations} are a ways to represent combined translation and rotation transformations, e.g.
				\begin{equation*}
					\vec{p}^1 = \mat{R}_2^1 \vec{p}^2 + \vec{\delta}^1
					\quad\implies\quad
					\vec{p}^0 = \mat{R}_1^0 \vec{p}^1 + \vec{\delta}^0 = \mat{R}_1^0 \big( \mat{R}_2^1 \vec{p}^2 + \vec{\delta}^1 \big) + \vec{\delta}^0
				\end{equation*}
				into a single matrix-vector multiplication, making the computation less clumsy:
				\begin{equation*}
					\vec{p}^0 = \mat{R}_1^0 \vec{p}^1 + \vec{\delta}^0
					\quad\implies\quad
					\underbrace{\begin{bmatrix}
						\vec{p}^1 \\
						1
					\end{bmatrix}}_{\tilde{\vec{p}}^1}
					=
					\underbrace{\begin{bmatrix}
						\mat{R}_1^0 & \vec{\delta}^1 \\
						\vec{0}     & 1
					\end{bmatrix}}_{\mat{H}_1^0}
					\underbrace{\begin{bmatrix}
						\vec{p}^0 \\
						1
					\end{bmatrix}}_{\tilde{\vec{p}}^0}
				\end{equation*}
				Hence, multiple translation-rotation transformations can be stacked as follows:
				\begin{equation*}
					\tilde{\vec{p}}^0 = \mat{H}_1^0 \mat{H}_2^1 \cdots \mat{H}_n^{n - 1} \tilde{\vec{p}}^n
				\end{equation*}
			% end

			\subsubsection{Denavit-Hartenberg Convention}
				A common convention for describing the coordinate systems of a robot is the \emph{Denavit-Hartenberg convention} which describes the kinematics of a complete robot with only four parameters per joint. These four parameters are:
				\begin{itemize}
					\item \(\theta_i\), the angle between \(x_{i - 1}\) and \(x_i\), measured around \(z_{i - 1}\). Variable if \(i\) is a revolute joint.
					\item \(d_i\), the distance of the origin of \(S_{i - 1}\) along \(z_{i - 1}\) to the intersection with \(x_i\). Variable if \(i\) is a prismatic joint.
					\item \(a_i\), the distance between the intersection of \(z_{i - 1}\) and \(x_i\) along \(x_i\) towards the origin of \(S_i\).
					\item \(\alpha_i\), the angle between \(z_{i - 1}\) and \(z_i\), measured around \(x_i\).
				\end{itemize}
				Along with that, the following conditions have to hold for the coordinate systems \(S_i\):
				\begin{itemize}
					\item The origins of the coordinate systems lie on the movement axis.
					\item The \(z_{i - 1}\)-axis lies along the movement axis of the \(i\)-th joint.
					\item The \(x_i\)-axis is perpendicular to the \(z_{i - 1}\)-axis and point away from it.
					\item The \(x_i\)-axis and the \(z_{i - 1}\)-axis have an intersection.
				\end{itemize}
				Placing the coordinate system can be formulated as an algorithm, check out the foundations of robotics summary\footnote{\url{https://projects.frisp.org/documents/29} (German)} for more details.
			% end
		% end

		\subsection{Differential Forward Kinematics (Velocities and Accelerations)}
			Often it is necessary to get the velocities \(\dot{\vec{x}}\) and accelerations \(\ddot{\vec{x}}\) of the end-effector. These can be computed straightforwardly using the chain rule
			\begin{align}
				\dot{\vec{x}} &= \dv{t} \vec{f}(\vec{q}) = \dv{\vec{f}(\vec{q})}{\vec{q}} \dv{\vec{q}}{t} = \underbrace{\mat{J}(\vec{q})}_{\mathclap{\text{Jacobian}}} \dot{\vec{q}}  \label{eq:diffForwardKin} \\
				\ddot{\vec{x}} &= \dot{\mat{J}}(\vec{q}) \dot{\vec{q}} + \mat{J}(\vec{q}) \ddot{\vec{q}}  \nonumber
			\end{align}
			where the end-effector velocities and accelerations are computed from the joint velocities and accelerations.

			\subsubsection{Singularities}
				In some cases, the Jacobian \( \mat{J}(\vec{q}) \) might get rank-deficient, i.e. \( \det \mat{J}(\vec{q}) = 0 \). In this case, infinite velocities in the joints are required to reach a desired end-effector velocities as the Jacobian is not invertible anymore. These positions are called \emph{singularities} of the robot and correspond to a loss in the degrees of freedom, e.g. when the robot stretches out its arm.
			% end

			\subsubsection{Computing the Jacobians}
				There are two main ways of computing the Jacobian:
				\begin{description}
					\item[Analytical] As before, the Jacobian is derived analytically and has the problem of singularities.
					\item[Geometric]  These are derived from geometric insights, maybe circumventing the "representational singularities".
				\end{description}
				Both of these ways are just different representations of the same concept.
			% end
		% end

		\subsection{Inverse Kinematics}
			\label{subsec:inverseKinematics}

			The obvious next problem is how to get the required joint positions to reach a given end-effector pose\footnote{A pose refers to both the position and the orientation at the same time.}. Hence, the \emph{inverse kinematics} are a mapping from the task- to the joint-space:
			\begin{equation*}
				\vec{q} = \vec{f}^{-1}(\vec{x})
			\end{equation*}
			For really simple robots, this can again be solved from a geometric perspective. However, there is most likely more than one solution for a given end-effector position! With certain redundancies in the robot, it is even possible to get infinite solutions.

			An additional problem is that the inverse kinematics equations are often not solvable analytically, hence numerical methods have to be used. But those do not guarantee to find all solutions which might be necessary to move into an "optimal" joint configuration. However, for a lot of industrial robots, invertible solutions are possible due to intelligent joint and link design.
		% end

		\subsection{Dynamics}
			For the dynamics of a robot, a forward model
			\begin{equation*}
				\ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}, \vec{u})
			\end{equation*}
			is wanted that gives the joint accelerations given the joint positions and velocities and torques/forces (in case of revolute/prismatic joints, respectively). Usually the dynamics are represented in the general form
			\begin{equation}
				\vec{u} = \mat{M}(\vec{q}) \ddot{\vec{q}} + \vec{c}(\vec{q}, \dot{\vec{q}}) + \vec{g}(\vec{q})  \label{eq:dynamicsInverseGeneral}
			\end{equation}
			with the motor commands \(\vec{u}\), the joint positions/velocities/accelerations \(\vec{q}\)/\(\dot{\vec{q}}\)/\(\ddot{\vec{q}}\), the mass matrix \(\mat{M}(\vec{q})\), the Coriolis and centripetal forces \(\vec{c}(\vec{q}, \dot{\vec{q}})\), and the gravity \(\vec{g}(\vec{q})\). This general form can be easily inverted to get the joint accelerations, as the mass matrix is always positive definite and hence invertible:
			\begin{equation}
				\ddot{\vec{q}} = \mat{M}^{-1}(\vec{q}) \big( \vec{u} - \vec{c}(\vec{q}, \dot{\vec{q}}) - \vec{g}(\vec{q}) \big)  \label{eq:dynamicsGeneral}
			\end{equation}

			\subsubsection{Computing the Forces}
				To compute the rigid body forces, there are two central methods:
				\begin{enumerate}
					\item Newton-Euler Method
						\begin{itemize}
							\item Force-dissection-based approach.
							\item Can be formalized nicely, check out the foundations of robotics summary\footnote{\url{https://projects.frisp.org/documents/29} (German)} for more details.
						\end{itemize}
					\item Lagrangian Method
						\begin{itemize}
							\item Energy-based approach.
							\item Based on Lagrangian mechanics, check out the theoretical physics: classical mechanics summary\footnote{\url{https://projects.frisp.org/documents/31} (German)} for more details.
						\end{itemize}
				\end{enumerate}
				Other forces than rigid body forces, e.g. friction, are a lot harder to model as there is not general recipe for modeling them (friction is not so well understood).

				\paragraph{Newton-Euler Method}
					The Newton-Euler method is based on \emph{force dissection} and exploiting of the required force equilibrium. It yields the \emph{recursive Newton-Euler algorithm} that can be used to straightforwardly compute the torques/forces in the joints.
				% end

				\paragraph{Lagrangian Methods}
					The \emph{Lagrangian method} is energy-based and works by defining the \emph{Lagrangian}
					\begin{equation*}
						L = T - V
					\end{equation*}
					where \(T\) is the kinetic energy and \(V\) is the potential energy. For a one-dimensional point mass with mass \(m\) in a gravity field with the gravity constant \(g\), the energies are given as \( T = \frac{1}{2} m\dot{y}^2 \) and \( V = mgy \), where \(y\) is the one degree of freedom. The Lagrangian is hence given as:
					\begin{equation*}
						L = \frac{1}{2} m \dot{y}^2 - mgy
					\end{equation*}
					Let \(f\) be the force of e.g. a motor that is somehow attached to the mass, the equations of motion are given as the \emph{Lagrangian equations of motion}:
					\begin{equation*}
						\dv{t} \pdv{L}{\dot{y}} - \pdv{L}{y} = f
						\quad\implies\quad
						m \ddot{y} + mg = f
					\end{equation*}
					For multi-dimensional systems, this equation can be generalized straightforwardly as
					\begin{equation*}
						\dv{t} \pdv{L}{\dot{x}_i} - \pdv{L}{x_i} = f_i
					\end{equation*}
					where \(x_i\) is one degree of freedom as \(f_i\) is the respective force.

					For robots, finding the potential energy is straightforward as it is just the sum of all potential energies in the links, i.e.
					\begin{equation*}
						V = \sum_{i = 1}^{n} V_i = \sum_{i = 1}^{n} m_i \vec{g}^T \vec{r}_{c_i},
					\end{equation*}
					where \(\vec{g}\) is the vector describing the gravitational acceleration and \(\vec{r}_{c_i}\) is the position of the center of mass of the \(i\)-th link. The kinetic energy is a tad more complicated as it involves rotational energy and interactions between the different joints, e.g. Coriolis forces. It is given as
					\begin{equation*}
						T = \frac{1}{2} \dot{\vec{q}}^T \Bigg[ \sum_{i = 1}^{n} m_i \mat{J}_{\vec{v}_i}^T(\vec{q}) \mat{J}_{\vec{v}_i}(\vec{q}) + \mat{J}_{\vec{\omega}_i}^T(\vec{q}) \mat{R}_i(\vec{q}) \mat{I}_i \mat{R}_i^T(\vec{q}) \mat{J}_{\vec{\omega}_i}(\vec{q}) \Bigg] \dot{\vec{q}},
					\end{equation*}
					where
					\begin{description}[leftmargin=2cm]
						\item[\(\vec{q}\)/\(\dot{\vec{q}}\)] are the positions/velocities of the joints,
						\item[\(m_i\)] is the mass of the \(i\)-th link,
						\item[\(\mat{J}_{\vec{v}_i}\)] is the linear Jacobian of the \(i\)-th joint,
						\item[\(\mat{J}_{\vec{\omega}_i}\)] is the rotational Jacobian of the \(i\)-th joint
						\item[\(\mat{I}_i\)] is the inertia tensor of the \(i\)-th link, and
						\item[\(\mat{R}_i(\vec{q})\)] is the orientation of the \(i\)-th link.
					\end{description}
				% end

				\paragraph{Comparison of Newton-Euler and Lagrangian}
					Using the Newton-Euler approach manually, i.e. to find the equations directly, is quite tedious and therefore not suitable for large robots. The Lagrangian approach, on the other hand, is a lot faster to execute by hand and therefore appropriate for finding the equations explicitly.

					In most cases, however, the algorithm is used computationally without formulating the equations explicitly. In this case, the Newton-Euler method is better suited as it has a computational complexity of \( \mathcal{O}(n) \) whilst the Lagrangian approach has a complexity of \( \mathcal{O}(n^3) \).
				% end
			% end

			\subsubsection{General Forms}
				If somehow an inverse dynamics model \( \vec{u} = \vec{f}(\vec{q}, \dot{\vec{q}}, \ddot{\vec{q}}) \) is achieved in the general form \eqref{eq:dynamicsInverseGeneral}, it is easy to compute the forward dynamics model \( \ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}, \vec{u}) \) by inverting the mass matrix as shown in \eqref{eq:dynamicsGeneral}. By integrating \(\ddot{\vec{q}}\), the velocities and positions can be recovered:
				\begin{align*}
					\dot{\vec{q}}(t) = \int_0^t \! \ddot{\vec{q}}(\tau) \dd{\tau}
					&&
					\vec{q}(t) = \int_0^t \! \dot{\vec{q}}(\tau) \dd{\tau}
				\end{align*}
				But this is (in most cases) not possible in closed form, so numerical integration techniques are required. One suitable method is for example the \emph{symplectic Euler method}, also called the \emph{semi-implicit Euler method}. It first integrates for the velocity with an explicit Euler method and subsequently integrates for the position with an implicit Euler method:
				\begin{align*}
					\dot{\vec{q}}_{k + 1} = \dot{\vec{q}}_k + h \ddot{\vec{q}}(t_k)
					&&
					\vec{q}_{k + 1} = \vec{q}_k + h \dot{\vec{q}}_{k + 1}
				\end{align*}
				Here, \(k\) is the step, \( h \in (0, 1) \) is the step size with \( t_k \coloneqq kh \), and \( \vec{q}_k \) and \( \dot{\vec{q}}_k \) are the respective approximations of the positions and velocities.
			% end
		% end
	% end

	\section{Representing Trajectories}
		A trajectory \( \vec{q}_d(t) \)/\( \dot{\vec{q}}_d(t) \)/\( \ddot{\vec{q}}_d(t) \) specifies the joint positions/velocities/accelerations at any point in time and is used to specify movements for the robot to execute. A common way to specify trajectories is to specify via-points the robot has to reach and to interpolate between them as shown in \autoref{fig:robotsViaPoints}. But as the motor commands can only influence the acceleration direction and the velocities and positions are found with integration, the trajectory and the time-derivative of the trajectory must not jump! This can be achieved with polynomial spline-interpolation which will be discussed further in the next sections.

		\begin{figure}
			\centering
			\begin{tikzpicture}[
						dot/.style = {
							circle,
							fill,
							minimum size = 5pt,
							inner sep = 0pt,
							outer sep = 0pt
						},
						cross/.style = {
							draw,
							cross out,
							thick,
							minimum size = 5pt,
							inner sep = 0pt,
							outer sep = 0pt
						}
					]

				\node [dot, label=left:Start] (start) at (0,  0) {};
				\node [dot, label=right:Goal] (goal)  at (4, -4) {};

				\node [cross] (v1) at (2,    0.25) {};
				\node [cross] (v2) at (2.5, -1) {};
				\node [cross] (v3) at (1.5, -3) {};

				\draw (0.5, -0.5) rectangle node[left,  xshift=-0.5cm]{Obstacle} (1.5, -1.5);
				\draw (3,   -2  ) rectangle node[right, xshift= 0.5cm]{Obstacle} (4,   -3);

				\draw plot [smooth] coordinates {(start) (v1) (v2) (v3) (goal)};
			\end{tikzpicture}
			\caption{Illustration of trajectory planning with via-points (the crosses) to avoid obstacles. The trajectory between the via-points is found with interpolation.}
			\label{fig:robotsViaPoints}
		\end{figure}

		\subsection{Splines}
			As already discussed, polynomial splines can be used to avoid jumps in the positions and velocities. The spline must be at least cubic as linear and quadratic splines do not have enough parameters to encode all the required information (and they have jumps in the velocities).

			\subsubsection{Cubic Splines}
				A cubic spline
				\begin{equation*}
					q(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3
				\end{equation*}
				has four parameters that can be found using the boundary conditions \( q(t_0) = q_0 \), \( \dot{q}(t_0) = v_0 \), \( q(t_f) = q_f \) and \( \dot{q}(t_f) = v_f \) where \(t_0\) and \(t_f\) are the initial and final time step, respectively. This yields a system of linear equations,
				\begin{equation*}
					\begin{bmatrix}
						1 & t_0 & t_0^2 & t_0^3   \\
						0 & 1   & 2 t_0 & 3 t_0^2 \\
						1 & t_f & t_f^2 & t_f^3   \\
						0 & 1   & 2 t_f & 3 t_f^2
					\end{bmatrix}
					\begin{bmatrix}
						a_0 \\
						a_1 \\
						a_2 \\
						a_3
					\end{bmatrix}
					=
					\begin{bmatrix}
						q_0 \\
						v_0 \\
						q_f \\
						v_f
					\end{bmatrix}
				\end{equation*}
				that can be solved for the parameters \(a_0\), \(a_1\), \(a_2\) and \(a_3\). The via-points are the initial and final position of the spline between them. In case of multiple degrees of freedom, multiple splines have to be used, i.e. the parameters become vectors.

				But cubic splines still exhibit jumps in the acceleration which are dangerous at high speeds and might damage the robot!
			% end

			\subsubsection{Quintic Splines}
				An alternative are quintic splines
				\begin{equation*}
					q(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5
				\end{equation*}
				which do not have jumps in the acceleration by imposing additional constraints on the accelerations, \( \ddot{q}(t_0) = a_0 \) and \( \ddot{q}(t_f) = a_f \). This again yields a system of linear equations:
				\begin{equation*}
					\begin{bmatrix}
						1 & t_0 & t_0^2 & t_0^3   & t_0^4    & t_0^5    \\
						0 & 1   & 2 t_0 & 3 t_0^2 & 4 t_0^3  & 5 t_0^4  \\
						0 & 0   & 2     & 6 t_0   & 12 t_0^2 & 20 t_0^3 \\
						1 & t_f & t_f^2 & t_f^3   & t_f^4    & t_f^5    \\
						0 & 1   & 2 t_f & 3 t_f^2 & 4 t_f^3  & 5 t_f^4  \\
						0 & 0   & 2     & 6 t_f   & 12 t_f^2 & 20 t_f^3
					\end{bmatrix}
					\begin{bmatrix}
						a_0 \\
						a_1 \\
						a_2 \\
						a_3 \\
						a_4 \\
						a_5
					\end{bmatrix}
					=
					\begin{bmatrix}
						q_0 \\
						v_0 \\
						a_0 \\
						q_f \\
						v_f \\
						a_f
					\end{bmatrix}
				\end{equation*}
				By equalizing the accelerations at the start and end of each spline, the acceleration becomes continuous, i.e. no jumps are present anymore.
			% end
		% end

		\subsection{Alternatives}
			Alternatives to splines are for example
			\begin{itemize}
				\item linear segments with parabolic blends,
				\item trapezoidal minimum time trajectories,
				\item potential fields \( V(\vec{q}) \) with \( \dot{\vec{q}} = \pdv{V(\vec{q})}{\vec{q}} \), and
				\item nonlinear dynamical systems \( \ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}, \vec{\theta}) \) parameterized by \(\vec{\theta}\).
			\end{itemize}
		% end
	% end

	\section{Control in Joint-Space}
		Given a desired trajectory \( \vec{q}_d(t) \)/\( \dot{\vec{q}}_d(t) \)/\( \ddot{\vec{q}}_d(t) \), it is still necessary to find the corresponding motor inputs \(\vec{u}\) to follow this trajectory. This problem is called \emph{control}.

		The generic idea of \emph{feedback control} is to take an action, measure the current state, compare it with the desired state, and adjust the action over and over again. A simple example for this is controlling the water temperature when taking a shower. An illustration of the control loop is shown in \autoref{fig:controlShower} with a simple model of the actual temperature where the control input is purely additive. Additionally, the measured data is noisy, illustrated by the measurement errors \(\epsilon\). The function \( f(T_d, y_t) \) is called the \emph{control law} and determines the control inputs. A special case for this is linear feedback control, which will be discussed in the next section.

		\begin{figure}
			\centering
			\begin{tikzpicture}[align = center, block/.style = { draw, rectangle, minimum height = 1cm, minimum width = 3cm }]
				\node [block, label = above:Controller] (controller) {\( u_t = f(T_d, y_t) \)};
				\node [block, label = above:Plant, right = 2 of controller] (plant) {\( T_{t + 1} = T_t + u_t \)};
				\node [block, label = right:Sensor, below = 1 of plant] (sensor) {\( y_t = T_t + \epsilon \)};
				\node [left = 2 of controller] (desired) {Desired Value \\ \( T_d = \SI{35}{\celsius} \)};

				\draw [->] (desired) -- (controller);
				\draw [->] (controller) -- (plant);
				\draw [->] (plant) -- (sensor);
				\draw [->] (sensor) -| (controller);
			\end{tikzpicture}
			\caption{Illustration of the feedback control loop in shower with a simple linear model of the temperature and the measurement errors \(\epsilon\).}
			\label{fig:controlShower}
		\end{figure}

		\subsection{Linear Feedback Control}
			For \emph{linear feedback control}, the control law \( f(T_d, y_t) \) is a linear function \( f(T_d, y_t) = K (T_d - y_t) \) that determines the control input based on the difference of the desired and the actual temperatures (the error). The parameter \(K\) is called the \emph{gain} which amplifies the error for the control input. Obviously, too high or too low gains would cause an uncomfortable way to shower, either by overshooting around the desired temperature (too high gain) or by never reaching the desired temperature (too small gain). Also, messing up the sign (by choosing a negative \(K\)) can be catastrophic, as in this case it would drive the temperature towards absolute zero.

			The following sections will generalize the idea of a linear feedback controller to multiple variables, where \(K\) becomes a matrix.

			\subsubsection{P-Controller}
				For a desired joint position \(\vec{q}_d\), given the actual joint position \(\vec{q}_t\), a \emph{P-controller} just includes terms proportional to the positioning error:
				\begin{equation*}
					\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t)
				\end{equation*}
				But a simple P-controller causes high oscillations in the position, making it not suitable for real control.
			% end

			\subsubsection{PD-Controller}
				An extended version, the \emph{PD-controller}, additionally includes differential values in terms of the joint velocities. The control law is then given as
				\begin{equation*}
					\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t)
				\end{equation*}
				with the gains \( \mat{K}_P \) and \( \mat{K}_D \). This control law eliminates the oscillations, but a steady-state error remains due to the gravitational force acting on the robot.

				\paragraph{With Gravity Compensation}
					If a model of the gravitational force is available, it can be used to remove the steady-state error by adding the gravitational acceleration to the control low:
					\begin{equation*}
						\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t) + \vec{g}(\vec{q})
					\end{equation*}
					But of course this required a reasonable model which is not always available.
				% end
			% end

			\subsubsection{PID-Controller}
				Another commonly used control law is the \emph{PID-controller} which, in addition to the proportional and differential terms, adds an integral part that keeps track of the error from the beginning of time:
				\begin{equation*}
					\vec{u}_t = \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t) + \mat{K}_I \int_0^t \! (\vec{q}_d - \vec{q}_\tau) \dd{\tau}
				\end{equation*}
				This approach also removes the steady-state error for steady-state control, i.e. when the robot should move to a single point. However, for tracking control, this approach is not suitable as there may always be a positioning error, letting the integral part become large very quick. This results in high motor commands may causing damage to the robot.

				Also, there is the problem of wind-up where if the position is not reached, the controller might get saturated due to limitations of the motor. This causes the control law to be effectively useless as it cannot react to other errors anymore.
			% end
		% end

		\subsection{Model-Based Feedback Control}
			Some problems with PD-control with gravity compensation include that there always needs to be an error in either the position of the velocity to generate a control signal. This implies that high gains are needed in order to be accurate. But high gains cause the robot to be really stiff and dangerous as it moves with higher power!

			If a model of the dynamics is present (e.g. using the recursive Newton-Euler algorithm), it can be used to compute the motor inputs for a desired acceleration. Being able to only set the acceleration is no limitation since dynamical systems are second-order systems anyway, so only the accelerations are directly controllable. This is called \emph{model-based feedback control} and it is described by the following equations:
			\begin{align*}
				\ddot{\vec{q}}_t^{\,\mathrm{ref}} &= \mat{K}_P (\vec{q}_d - \vec{q}_t) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}_t) + \ddot{\vec{q}}_d \\
				\vec{u}_t &= \mat{M}(\vec{q}_t) \ddot{\vec{q}}_t^{\,\mathrm{ref}} + \vec{c}(\vec{q}, \dot{\vec{q}}) + \vec{g}(\vec{q})
			\end{align*}
			A major drawback is of course that this needs an accurate model of the dynamics!
		% end

		\subsection{Feedforward Control}
			In feedforward control, it is assumed that \( \vec{q}_t \approx \vec{q}_d \) and \( \dot{\vec{q}}_t \approx \dot{\vec{q}}_d \). This directly yields the following control law:
			\begin{align*}
				\vec{u}_t^\mathrm{FF} &= \mat{M}(\vec{q}_d) \ddot{\vec{q}}_d + \vec{c}(\vec{q}_d, \dot{\vec{q}}_d) + \vec{g}(\vec{q}_d) \\
				\vec{u}_t^\mathrm{FB} &= \mat{K}_P (\vec{q}_d - \vec{q}) + \mat{K}_D (\dot{\vec{q}}_d - \dot{\vec{q}}) \\
				\vec{u}_t &= \vec{u}_t^\mathrm{FF} + \vec{u}_t^\mathrm{FB}
			\end{align*}
			As the inverse model does not play such a high role in feedforward control, it can also be used if the model is only approximate or even bad. Hence, feedforward control is highly relevant in practice as models are often not so good that model-based feedback control can be used. Additionally, it is often possible to pre-compute the feedforward terms \( \vec{u}_t^\mathrm{FF} \) such that less real-time computations are needed.
		% end
	% end

	\section{Control in Task-Space}
		So far it was assumed that the space to plan the movements in was the joint-space. However, sometimes it is more practical to plan in task-space, e.g. when trying to hit a ball with a table tennis racket. Hence, an inverse kinematics model \( \vec{q} = \vec{f}^{-1}(\vec{x}) \) as discussed in \autoref{subsec:inverseKinematics} is needed. But is is often hard if not impossible to find such a model!

		\subsection{Differential Inverse Kinematics}
			As already seen, the inverse kinematics problem is hard to solve. But assuming the robot is non-redundant (i.e. \( n = r \), where \(r\) is the dimensionality of the task-space), the Jacobian of the differential forward kinematics model in \eqref{eq:diffForwardKin} becomes square. Assuming not to be in a singularity, the joint velocities can therefore be obtained by inverting \( \mat{J}(\vec{q}) \):
			\begin{equation*}
				\dot{\vec{q}} = \mat{J}^{-1}(\vec{q}) \dot{\vec{x}}
			\end{equation*}
			By (numerically) integrating these velocities, the joint positions can be recovered (if the initial position is known).

			\subsubsection{Jacobian Transpose}
				If the Jacobian is not square, it is not possible to invert it directly. Hence, the task-space error
				\begin{equation*}
					E = \frac{1}{2} \big(\vec{x}_d - \vec{f}(\vec{q})\big)^T \big(\vec{x}_d - \vec{f}(\vec{q})\big)
				\end{equation*}
				has to be minimized (where \(\vec{f}(\vec{q})\) is the forward kinematics model). This can be done by gradient decent "in the system", i.e. the desired joint velocities are computed by minimizing the error. The desired joint positions can then again be recovered by numerical integration. Computing the gradient
				\begin{equation*}
					\pdv{E}{\vec{q}} = -\dv{\vec{f}(\vec{q})}{\vec{q}} \big(\vec{x} - \vec{f}(\vec{q})\big) \doteq -\mat{J}^T(\vec{q}) \big(\vec{x}_d - \vec{f}(\vec{q})\big)
				\end{equation*}
				therefore yields the following desired velocity, where \(\gamma\) is the step size:
				\begin{equation*}
					\dot{\vec{q}}_d = -\gamma \mat{J}^T(\vec{q}) \big(\vec{x}_d - \vec{f}(\vec{q})\big)
				\end{equation*}
				This is called the \emph{Jacobian transpose method}. As already said, the desired joint position \( \vec{q}_d \) can then be recovered by numerically integrating \( \dot{\vec{q}}_d \). All of this can then be fed into an joint-space controller, e.g. a PD- or model-based controller.

				By adding numerical differentiation, it is also possible to use joint-space controllers that control the acceleration.
			% end

			\subsubsection{Jacobian Pseudo-Inverse}
				Under the assumption that the robot is not too far away from the solution, it is possible to also set desired task space velocities whilst finding the smallest \( \dot{\vec{q}}_d \) that keeps that velocity. This can be formulated as the optimization problem
				\begin{equation}
					\begin{aligned}
						\dot{\vec{q}}_d = \arg\min_{\dot{\vec{q}}} \,& \frac{1}{2} \dot{\vec{q}}^T \dot{\vec{q}} \\
						\mathrm{s.t.} \quad&
							\begin{aligned}[t]
								\mat{J}(\vec{q}) \dot{\vec{q}} &= \dot{\vec{x}}_d
							\end{aligned}
					\end{aligned}  \label{eq:jacobianPseudoInverseOptProblem}
				\end{equation}
				This can be solved with Lagrangian optimization with the multipliers \( \vec{\lambda} \). For brevity, let \( \mat{J} \coloneqq \mat{J}(\vec{q}) \). Given the Lagrangian
				\begin{equation*}
					\mathcal{L} = \frac{1}{2} \dot{\vec{q}}^T \dot{\vec{q}} - \vec{\lambda}^T \big( \mat{J} \dot{\vec{q}} - \dot{\vec{x}}_d \big),
				\end{equation*}
				setting the derivative w.r.t. \( \dot{\vec{q}} \) to zero yields
				\begin{equation}
					\pdv{\mathcal{L}}{\dot{\vec{q}}} = \dot{\vec{q}} - \mat{J}^T \vec{\lambda} \overset{!}{=} \vec{0}
					\quad\implies\quad
					\dot{\vec{q}} = \mat{J}^T \vec{\lambda}.  \label{eq:jacobianPseudoInversePartial}
				\end{equation}
				Inserting this into the Lagrangian, taking the derivative w.r.t. \( \vec{\lambda} \) and setting it to zero then yields the solution of the optimization problem:
				\begin{align*}
					&& \mathcal{L} &= \frac{1}{2} \dot{\vec{q}}^T \dot{\vec{q}} - \vec{\lambda}^T \big( \mat{J} \dot{\vec{q}} - \dot{\vec{x}}_d \big) = \frac{1}{2} \vec{\lambda}^T \mat{J} \mat{J}^T \vec{\lambda} - \vec{\lambda}^T \big( \mat{J} \mat{J}^T \vec{\lambda} - \dot{\vec{x}}_d \big) \\
					\implies && \pdv{\mathcal{L}}{\vec{\lambda}} &= \mat{J} \mat{J}^T \vec{\lambda} - 2 \mat{J} \mat{J}^T \vec{\lambda} + \dot{\vec{x}}_d = -\mat{J} \mat{J}^T \vec{\lambda} + \dot{\vec{x}}_d \overset{!}{=} 0 \\
					\implies && \vec{\lambda} &= \big( \mat{J} \mat{J}^T \big)^{-1} \dot{\vec{x}}_d
				\end{align*}
				Plugging this result back into \eqref{eq:jacobianPseudoInversePartial} yields the desired joint velocity and the solution of the optimization problem:
				\begin{equation*}
					\dot{\vec{q}}_d
						= \underbrace{\mat{J}^T \big( \mat{J} \mat{J}^T \big)^{-1}}_{\mat{J}^\dagger} \dot{\vec{x}}_d
						\doteq \mat{J}^\dagger \dot{\vec{x}}_d
				\end{equation*}
			% end
		% end

		\subsection{Task-Prioritization with Null-Space Movements}
			It is possible to modify the task-space control law to simultaneously execute another action in the \emph{null-space}, a space that does not contradict the constraints of the optimization problem \eqref{eq:jacobianPseudoInverseOptProblem}. This can be, for example, to push the robot into a rest position \( \vec{q}_\mathrm{rest} \) where it does not consume energy. This base task can be formulated with a P-controller
			\begin{equation*}
				\dot{\vec{q}}_0 = \mat{K}_P (\vec{q}_\mathrm{rest} - \vec{q}).
			\end{equation*}
			The optimization problem then is as following:
			\begin{equation*}
				\begin{aligned}
					\dot{\vec{q}}_d = \arg\min_{\dot{\vec{q}}} \, & \frac{1}{2} (\dot{\vec{q}} - \dot{\vec{q}}_0)^T (\dot{\vec{q}} - \dot{\vec{q}}_0) \\
					\mathrm{s.t.} \quad&
						\begin{aligned}[t]
							\mat{J}(\vec{q}) \dot{\vec{q}} &= \dot{\vec{x}}_d
						\end{aligned}
				\end{aligned}
			\end{equation*}
			The result of this optimization problem is
			\begin{equation*}
				\dot{\vec{q}}_d = \mat{J}^\dagger \dot{\vec{x}}_d + \big( \mat{I} - \mat{J}^\dagger \mat{J} \big) \dot{\vec{q}}_0
			\end{equation*}
			where again \( \mat{J} \coloneqq \mat{J}(\vec{q}) \). The null-space is characterized by \( \big( \mat{I} - \mat{J}^\dagger \mat{J} \big) \) which includes all movements \( \dot{\vec{q}}_\mathrm{null} \) that do not contradict the constraint \( \mat{J} \dot{\vec{q}} = \dot{\vec{x}}_d \), i.e. \( \dot{\vec{x}}_d = \mat{J} (\dot{\vec{q}} + \dot{\vec{q}}_\mathrm{null}) \) or equivalent \( \mat{J} \dot{\vec{q}}_\mathrm{null} = \vec{0} \) holds.
		% end

		\subsection{More Advanced Solutions}
			It is also possible to use an acceleration formulation which has the solution
			\begin{equation*}
				\ddot{\vec{q}}_d = \mat{J}^\dagger ( \ddot{\vec{x}}_d - \dot{\mat{J}} \dot{\vec{q}} ) + \big( \mat{I} - \mat{J}^\dagger \mat{J} \big) \ddot{\vec{q}}_0.
			\end{equation*}
			There is a whole class of so-called \emph{operational space control laws}, i.e. task-space control laws, that can all be derived from the following most general optimization problem:
			\begin{equation*}
				\begin{aligned}
					\min_{\vec{u}} \,& \frac{1}{2} (\vec{u} - \vec{u}_0)^T (\vec{u} - \vec{u}_0) \\
					\mathrm{s.t.} \quad&
						\begin{aligned}[t]
							\mat{A}(\vec{q}, \dot{\vec{q}}, t) \ddot{\vec{q}} &= \dot{\vec{b}}(\vec{q}, \dot{\vec{q}}, t) \\
							                                        \vec{u}_0 &= \vec{g}(\vec{q}, \dot{\vec{q}}, t) \\
							                  \mat{M}(\vec{q}) \ddot{\vec{q}} &= \vec{u} + \vec{c}(\vec{q}, \dot{\vec{q}}) + \vec{g}(\vec{q})
						\end{aligned}
				\end{aligned}
			\end{equation*}
		% end

		\subsection{Singularities and Damped Pseudo-Inverse}
			If the Jacobian gets rank-deficient, i.e. singular, it is not longer possible to evaluate the pseudo-inverse \( \mat{J}^\dagger = \mat{J}^T \big( \mat{J} \mat{J}^T \big)^{-1} \) due to the last inversion. It is therefore numerically more stable to use a damped pseudo-inverse \( \mat{J}^{\dagger(\lambda)} = \mat{J}^T \big( \mat{J} \mat{J}^T + \lambda \mat{I} \big)^{-1} \) as a replacement of the regular pseudo-inverse. This avoids singularities in the inversion.
		% end
	% end
% end

\chapter{Machine Learning Foundations} % 5b.1, 5b.2, 5b.21, 5b.22, 5b.23, 5b.24, 5b.25, 5b.26
	\todo{Content}

	\section{The Six Machine Learning Choices} % 5b.27, 5b.28, 5b.62
		\todo{Content}

		\subsection{Problem Class} % 5b.30, 5b.31, 5b.32, 5b.33, 5b.34, 5b.35
			\todo{Content}
		% end

		\subsection{Problem Assumptions} % 5b.36
			\todo{Content}
		% end

		\subsection{Evaluation} % 5b.37, 5b.38, 5b.39, 5b.40, 5b.41
			\todo{Content}
		% end

		\subsection{Model Type} % 5b.42
			\todo{Content}
		% end

		\subsection{Model Class Selection} % 5b.43, 5b.44, 5b.45
			\todo{Content}
		% end

		\subsection{Algorithm Realization} % 5b.46
			\todo{Content}
		% end

		\subsection{Example} % 5b.47, 5b.48, 5b.49, 5b.50, 5b.51, 5b.52, 5b.53, 5b.54, 5b.55, 5b.56, 5b.57, 5b.58, 5b.59, 5b.60, 5b.61
			\todo{Content}
		% end
	% end

	\section{Evaluation} % 5b.63, 5b.65
		\todo{Content}

		\subsection{Occams Razor} % 5b.66
			\todo{Content}
		% end

		\subsection{Bias and Variance} % 5b.67, 5b.68, 5b.69
			\todo{Content}
		% end

		\subsection{Model Selection} % 5b.70, 5b.71, 5b.72, 5b.73
			\todo{Content}
		% end
	% end

	\section{Frequentis vs. Bayesian Assumptions} % 5b.74, 5b.75
		\todo{Content}

		\subsection{Maximum Likelihood Estimation} % 5b.76, 5b.77, 5b.78, 5b.79
			\todo{Content}
		% end

		\subsection{Bayesian Thinking and Maximum A-Posteriori} % 5b.80, 5b.81, 5b.82
			\todo{Content}

			\subsubsection{Ridge Regression (Tikhonov Regularized Regression)} % 5b.83, 5b.84
				\todo{Content}
			% end

			\subsubsection{Predictions} % 5b.85, 5b.86, 5b.87
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Regression} % 5b.88, 5b.89, 5b.90, 5b.91, 5b.92
			\todo{Content}
		% end
	% end

	\section{Hand-Crafted Feature Construction} % 5b.93, 5b.94
		\todo{Content}

		\subsection{Discrete Inputs} % 5b.95, 5b.96
			\todo{Content}
		% end

		\subsection{Continuous Inputs} % 5b.97
			\todo{Content}

			\subsubsection{One-Hot} % 5b.98
				\todo{Content}
			% end

			\subsubsection{Radial Basis Functions (RBFs)} % 5b.99, 5b.100, 5b.101
				\todo{Content}
			% end
		% end
	% end

	\section{Automatic (Linear) Feature Construction} % 5b.102, 5b.103, 5b.104, 5b.105, 5b.106, 5b.107
		\todo{Content}

		\subsection{Respective Field Weighted Regression (RFWR)} % 5b.108, 5b.109, 5b.110
			\todo{Content}
		% end

		\subsection{Automatic Adaption of RFWR} % 5b.111, 5b.112, 5b.113
			\todo{Content}
		% end
	% end

	\section{Non-Parametric Approaches} % 5b.114, 5b.115, 5b.116, 5b.117
		\todo{Content}

		\subsection{Weighted Linear Regression} % 5b.118, 5b.119, 5b.120, 5b.121, 5b.122
			\todo{Content}
		% end

		\subsection{Locally Weighted Bayesian Linear Regression} % 5b.123
			\todo{Content}
		% end

		\subsection{Kernel Methods} % 5b.124, 5b.125, 5b.128
			\todo{Content}

			\subsubsection{Kernel Ridge Regression} % 5b.126, 5b.127
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Kernel Regression: Gaussian Processes (GPs)} % 5b.129, 5b.130, 5b.131, 5b.136
			\todo{Content}

			\subsubsection{GP-Posterior} % 5b.132, 5b.133, 5b.134, 5b.135
				\todo{Content}
			% end
		% end
	% end

	\section{Neural Networks} % 5c.1, 5c.2, 5c.4, 5c.5, 5c.6, 5c.7, 5c.7, 5c.8, 5c.9
		\todo{Content}

		\subsection{Biology and Neuron Abstraction} % 5c.10, 5c.11, 5c.12, 5c.13
			\todo{Content}
		% end

		\subsection{Components of a Neural Network} % N/A
			\todo{Content}

			\subsubsection{Single- and Multi-Layer Networks} % 5c.14, 5c.15, 5c.16, 5c.17, 5c.18, 5c.19
				\todo{Content}
			% end

			\subsubsection{Topologies} % 5c.20, 5c.21
				\todo{Content}
			% end

			\subsubsection{Activation Functions} % 5c.22, 5c.23, 5c.24, 5c.25
				\todo{Content}
			% end

			\subsubsection{Output Neurons} % 5c.26
				\todo{Content}
			% end

			\subsubsection{Loss Functions} % 5c.27
				\todo{Content}
			% end
		% end

		\subsection{Forward- and Backpropagation} % 5c.28, 5c.29, 5c.30
			\todo{Content}

			\subsubsection{Forwardpropagation} % 5c.31, 5c.32
				\todo{Content}
			% end

			\subsubsection{Backpropagation} % 5c.33, 5c.34, 5c.35, 5c.37
				\todo{Content}

				\paragraph{Skip Connections} % 5c.36
					\todo{Content}
				% end
			% end

			\subsubsection{Finite Differences} % 5c.38, 5c.39
				\todo{Content}
			% end

			\subsubsection{Automatic Differentiation} % 5c.40
				\todo{Content}
			% end
		% end

		\subsection{Efficient Gradient Descent} % 5c.42, 5c.43, 5c.44, 5c.47
			\todo{Content}

			\subsubsection{Stochastic Gradient Descent} % 5c.45, 11.19, 11.20
				\todo{Content}
			% end

			\subsubsection{Mini-Batch Gradient Descent} % 5c.46
				\todo{Content}
			% end
		% end

		\subsection{Choosing the Learning Rate} % 5c.48, 5c.49, 5c.50, 5c.51
			\todo{Content}

			\subsubsection{Plateaus and Valleys} % 5c.52
				\todo{Content}
			% end

			\subsubsection{Adaptive Learning Rates} % N/A
				\todo{Content}

				\paragraph{Momentum} % 5c.53
					\todo{Content}
				% end

				\paragraph{Adadelta} % 5c.54
					\todo{Content}
				% end

				\paragraph{Adam} % 5c.55
					\todo{Content}
				% end
			% end
		% end

		\subsection{Choosing the Descent Direction} % N/A
			\todo{Content}

			\subsubsection{Hessian Approaches} % 5c.56
				\todo{Content}
			% end

			\subsubsection{Conjugate Gradient} % 5c.57
				\todo{Content}
			% end

			\subsubsection{Levenberg-Marquardt} % 5c.58
				\todo{Content}
			% end
		% end

		\subsection{Initialization of the Parameters} % 5c.59, 5c.60
			\todo{Content}
		% end

		\subsection{Overfitting} % 5c.61, 5c.62, 5c.63, 5c.64, 5c.65, 5c.66
			\todo{Content}

			\subsubsection{Weight Decay} % 5c.67
				\todo{Content}
			% end

			\subsubsection{Early Stopping} % 5c.68
				\todo{Content}
			% end

			\subsubsection{Input Noise Augmentation} % 6.69
				\todo{Content}
			% end

			\subsubsection{Dtopout} % 5c.70
				\todo{Content}
			% end

			\subsubsection{Batch Normalization} % 5c.71
				\todo{Content}
			% end
		% end

		\subsection{Theoretical Analysis} % 5c.72, 5c.73, 5c.75, 5c.76, 5c.77
			\todo{Content}

			\subsubsection{Universal Function Approximation Theorem} % 5c.74
				\todo{Content}
			% end
		% end

		\subsection{Network Architectures} % 5c.78, 5c.79, 5c.80
			\todo{Content}

			\subsubsection{Convolutional Neural Networks (CNNs)} % 5c.81, 5c.82, 5c.83, 5c.84, 5c.85, 5c.86
				\todo{Content}
			% end

			\subsubsection{Recurrent Neural Networks (RNNs)} % 5c.78
				\todo{Content}
			% end
		% end

		\subsection{Neural Networks in Robotics} % 5c.88, 5c.89
			\todo{Content}

			\subsubsection{Value Functions} % 5c.90, 5c.91
				\todo{Content}
			% end

			\subsubsection{Policies} % 5c.92, 5c.93
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 5b.138, 5c.95
		\todo{Content}
	% end
% end

\chapter{Optimal Control}
	In lots of scenarios it is too costly to program a desired behavior. This phenomenon is none from e.g. image understanding where the state-of-the-art approach is to use machine learning. So supervised learning is not enough for acquiring certain behaviors, e.g. due to imperfect demonstrations, the correspondence problem and the simple fact that it is impossible to demonstrate everything. The correspondence problem describes the fact that the robot works differently than humans, i.e. the transfer from a human motion to joint movements or a robot is not straightforward.

	Hence, self-improvement is needed! The robot explores the environment by trial-and-error and the environment (or the engineers of the environment) give feedback based on the current states and the executed actions. This is called the \emph{reward}. In optimal control and reinforcement learning settings, it is common to talk about the \emph{reward}, whereas in other literature it is more common to talk about the \emph{cost}, which is just an inverted notion of the same concept, so \( \mathit{Reward} = -\mathit{Cost} \) and maximizing the reward is equivalent to minimizing the cost.

	This chapter is split into two sections: optimal control for discrete and continuous state-action spaces. In both settings the time is assumed to be discrete (controlling a robot is most often done with a certain sampling rate anyway).

	The vast field of optimal control is based on the \emph{principle of optimality} by Richard Bellman (1957): "An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision." This principle can be broken done in two main ideas:
	\begin{enumerate}
		\item Breaking down the optimal solutions in small parts, each part must be optimal.
		\item The initial and previous states do not matter for the optimal policy, i.e. only the current state is relevant for determining the optimal action.
	\end{enumerate}
	This principle leads to the principle of \emph{dynamic programming}. It breaks down the problem into small sub-problems which are then solved optimally. All sub-solutions together than constitute an optimal (global) policy. Famous examples for algorithms implementing the paradigm of dynamic programming are for example Dijkstra's algorithm for the shortest-path-problem.

	\section{Discrete State-Action Space}
		Optimal control for discrete state-action spaces are based on \emph{Markov decision processes} (MDPs) which are defined by
		\begin{itemize}
			\item state space \( \vec{s} \in \mathcal{S} \),
			\item action space \( \vec{a} \in \mathcal{A} \),
			\item transition dynamics \( p_t(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t) \),
			\item reward function \( r_t : \mathcal{S} \times \mathcal{A} \to \R \), and
			\item initial state distribution \( \mu_0(\vec{s}) \).
		\end{itemize}
		The \emph{Markov property} describes that the transition dynamics are only dependent on the previous state and the taken action, but not on any previous states or actions, i.e.
		\begin{equation*}
			p_t(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t, \vec{s}_{t - 1}, \vec{a}_{t - 1}, \,\cdots\!\,) = p_t(\vec{s}_{t + 1} \given \vec{s}_t, \vec{a}_t)
		\end{equation*}
		holds. MDPs can be visualized nicely as transition diagrams as illustrated in \autoref{fig:smallMdp}.

		A \emph{policy} \(\pi\) in an MDPs describes the action to take given a state. It can either be \emph{deterministic}, i.e. \( \vec{a} = \pi(\vec{s}) \), \( \pi : \mathcal{S} \to \mathcal{A} \) or \emph{stochastic}, i.e. \( \vec{a} \sim \pi(\cdot \given \vec{s}) \), \( \pi : \mathcal{S} \times \mathcal{A} \to \R^+ \). As deterministic policies can also be represented as stochastic policies with an action probability of \num{1}, all of the following assumes a stochastic policy if not stated otherwise. But this stochastic policy might encode deterministic behavior.

		\begin{figure}
			\centering
			\begin{tikzpicture}[state/.style = { draw, circle, minimum width = 1cm, minimum height = 1cm }]
				\node [state] (s0) {\( s_{\,\mathclap{0}} \)};
				\node [state, right = 1.5 of s0] (s1) {\( s_{\,\mathclap{1}} \)};

				\draw [->] (s0) to[out=180+20, in=180-20, looseness=8] node[left]{\( a_0 \)} (s0);
				\draw [->] (s0) to[out=-20, in=180+20] node[below]{\( a_21\)} (s1);
				\draw [->] (s1) to[out=180-20, in=20] node[above]{\( a_0 \), \( a_1 \)} (s0);
			\end{tikzpicture}
			\caption{Simple Markov decision process with two states and actions.}
			\label{fig:smallMdp}
		\end{figure}

		\subsection{Finite-Horizon Optimal Control}
			For optimal control with a finite horizon \(T\), the cumulative reward over \(T\) time steps is maximized. That is, a policy \(\pi(\vec{a} \given \vec{s})\) is searched that maximizes the expected reward
			\begin{equation}
				J_\pi = \E_{\mu_0, p, \pi}\Bigg[ r_T(\vec{s}_T) + \sum_{t = 0}^{T - 1} r_t(\vec{s}_t, \vec{a}_t) \Bigg]\!,  \label{eq:expectedLongTermReward}
			\end{equation}
			where \( \E_{\mu_0, p, \pi}[\cdot] \) is a short form for \( \E_{\vec{s}_0 \sim \mu_0(\cdot),\, \vec{s}_{t + 1} \sim p(\cdot \given \vec{s}_t, \vec{a}_t),\, \vec{a}_t \sim \pi(\cdot \given \vec{s}_t) } \). The final reward \( r_T(\vec{s}_T) \) does not depend on the action as there is no action to take in the last state. In all of the following, the final reward might also be referred to as \( r_T(\vec{s}_T, \vec{a}_T) \), but it does still only depend on \( \vec{s}_T \) as there is no \( \vec{a}_T \)! It is convenient as it clears up some of the summations.

			\subsubsection{Value and State-Action Value Functions}
				To assess the "quality" of a state or state-action pair, two functions are defined: The value and the state-action value functions, where the latter is often called the Q-function. For some policy \(\pi\), they are defined as follows:
				\begin{align*}
					V_t^\pi(\vec{s})          &= \E_{p, \pi}\Bigg[ \sum_{\tau = t}^{T} r_\tau(\vec{s}_\tau, \vec{a}_\tau) \Bigggiven \vec{s}_t = \vec{s} \Bigg]                        \\
					Q_t^\pi(\vec{s}, \vec{a}) &= \E_{p, \pi}\Bigg[ \sum_{\tau = t}^{T} r_\tau(\vec{s}_\tau, \vec{a}_\tau) \Bigggiven \vec{s}_t = \vec{s},\, \vec{a}_t = \vec{a} \Bigg]
				\end{align*}
				The intuition of these functions is as follows:
				\begin{description}[leftmargin = 3.5cm]
					\item[Value Function] How "good" is it to be in state \( \vec{s} \) under the policy \(\pi\)?
					\item[Q-Function]     How "good" is it to take action \( \vec{a} \) in state \( \vec{s} \) when subsequently following the policy \(\pi\)?
				\end{description}
				Given the optimal policy \( \pi^\ast \), i.e. the policy that maximizes \eqref{eq:expectedLongTermReward}, the respective value and Q-functions are usually called \( V_t^\ast \) and \( Q_t^\ast \). For the optimal policy, the value and Q-function can be calculated from each other straightforwardly:
				\begin{equation*}
					\begin{aligned}
						V_t^\ast(\vec{s})          &= \max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a}) \\
						Q_t^\ast(\vec{s}, \vec{a}) &= r_t(\vec{s}, \vec{a}) + \E_{\vec{s}' \sim p(\cdot \given \vec{s}, \vec{a})}\big[ V_{t + 1}^\ast(\vec{s}') \big]
					\end{aligned}  \label{eq:valueQFuncRelations}
				\end{equation*}
			% end

			\subsubsection{Value Iteration}
				Applying the principle of dynamic programming to an MDP is straightforward. As in the last time step there is no action, the value function for the last time step is simply
				\begin{equation*}
					V_T^\ast(\vec{s}) = r_T(\vec{s})
				\end{equation*}
				for every state \(\vec{s}\) (this has to be evaluated). Then \emph{value iteration} iterates backwards in time, by maximizing the Q-function that is computed from the value function for \( t = \dotsrange{T - 1}{1} \):
				\begin{equation*}
					V_t^\ast(\vec{s}) = \max_{\vec{a}} \, Q_t(\vec{s}_t, \vec{a}) = \max_{\vec{a}} \, \Big( r_t(\vec{s}_t, \vec{a}) + \E_{\vec{s}' \sim p(\cdot \given \vec{s}_t, \vec{a})}\big[ V_{t + 1}^\ast(\vec{s}') \biggiven \vec{s}_t, \vec{a} \big] \Big)
				\end{equation*}
				Following this pattern, the optimal value function for time step \(t\) is obtained after \( T - t + 1 \) iterations. The (deterministic) optimal policy \( \pi_t^\ast : \mathcal{S} \to \mathcal{A} \) is then obtained by maximizing the Q-function
				\begin{equation*}
					\pi_t^\ast(\vec{s}) = \arg\max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a})
				\end{equation*}
				which can either be obtained from the value function or, ideally, has been stored whilst executing the value iteration.

				A pseudo-code version of value iteration is shown in \autoref{alg:valueIterationFiniteHorizon}. Note that the computation of the value function was split into computing the Q-function on \autoref{algline:valueIterationFiniteHorizon-qComputation} and then computing the value function on \autoref{algline:valueIterationFiniteHorizon-vComputation} for to simplify the final computation of the policy on \autoref{algline:valueIterationFiniteHorizon-piComputation}.

				\begin{algorithm}  \DontPrintSemicolon
					\( V_T^\ast(\vec{s}) \gets r_T(\vec{s}) \) for all \( \vec{s} \in \mathcal{S} \) \;
					\For{\( t = \dotsrange{T - 1}{0} \)}{
						\tcp{Compute Q-Function for time step \(t\) for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\( Q_t^\ast(\vec{s}, \vec{a}) \gets r_t(\vec{s}, \vec{a}) + \sum_{\vec{s}'} p_t(\vec{s}' \given \vec{s}, \vec{a}) V_{t + 1}^\ast(\vec{s}') \)  \label{algline:valueIterationFiniteHorizon-qComputation} \;

						\tcp{Compute V-Function for time step \(t\) for all \( \vec{s} \in \mathcal{S} \):}
						\( V_t^\ast(\vec{s}) \gets \max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a}) \)  \label{algline:valueIterationFiniteHorizon-vComputation} \;
					}
					\tcp{Return optimal policy for each time step \(t\) for all \( \vec{s} \in \mathcal{S} \):}
					\Return \( \pi_t^\ast(\vec{s}) \gets \arg\max_{\vec{a}} \, Q_t^\ast(\vec{s}, \vec{a}) \)  \label{algline:valueIterationFiniteHorizon-piComputation} \;

					\caption{Value Iteration for Finite-Horizon Problems}
					\label{alg:valueIterationFiniteHorizon}
				\end{algorithm}
			% end

			\subsubsection{Consequences of a Finite Time Horizon}
				Choosing a finite time horizon over an infinite one has some major consequences on the environment and the e.g. the value function. First of all, it matters how many time steps are left. This implies that not only the state transition model, but also the policy and the reward function might be time-dependent as denoted by the index \(t\). This leads to also the value and Q-function being time-dependent! However, having a finite amount of steps makes it possible to find the optimal value function in a finite amount of steps.
			% end
		% end

		\subsection{Infinite-Horizon Optimal Control} % 3a.22, 3a.23, 3a.24, 3a.25
			Using an infinite time horizon, i.e. \( T = \infty \), the time index is not part of the state! Hence, also the optimal policy as well as the value and Q-function are time-independent and also the reward function and the state transition model are time-independent. But this comes at the cost of the sum of rewards, i.e. the optimization objective, being infinite and hence divergent.

			The simplest and most straightforward approach is to introduce a \emph{discount factor} \( \gamma \in [0, 1) \) that trades of the long term vs. the immediate reward. The optimization objective, the discounted sum of rewards, now is
			\begin{equation*}
				J_\pi = \E_{\mu_0, p, \pi} \Bigg[ \sum_{t = 0}^{\infty} \gamma^t r(\vec{s}_t, \vec{a}_t) \Bigg]  \label{eq:expectedDiscountedLongTermReward}
			\end{equation*}
			analogous to the objective for the finite horizon \eqref{eq:expectedLongTermReward}. The value and state-action value functions are defined accordingly as
			\begin{align*}
				V^\pi(\vec{s}) &= \E_{p, \pi}\Bigg[ \sum_{t = 0}^{\infty} \gamma^t r(\vec{s}_t, \vec{a}_t) \Bigggiven \vec{s}_0 = \vec{s} \Bigg] \\
				Q^\pi(\vec{s}, \vec{a}) &= \E_{p, \pi}\Bigg[ \sum_{t = 0}^{\infty} \gamma^t r(\vec{s}_t, \vec{a}_t) \Bigggiven \vec{s}_0 = \vec{s},\, \vec{a}_0 = \vec{a} \Bigg]\!,
			\end{align*}
			with the relations
			\begin{align*}
				V^\pi(\vec{s}) &= \E_\pi\big[ Q^\pi(\vec{s}, \vec{a}) \biggiven \vec{a} \big] \\
				Q^\pi(\vec{s}, \vec{a}) &= r(\vec{s}, \vec{a}) + \gamma \E_{\vec{s}' \sim p(\cdot \given \vec{s}, \vec{a})}\big[ V^\pi(\vec{s}') \big]
			\end{align*}
			between them. These are analogous to the finite-horizon case relations in \eqref{eq:valueQFuncRelations}, however in the last formula the expectation is multiplied with the discount factor. Also the maximization in the first equation was replaced with an expectation as now the policy might be suboptimal. In the finite case, only the optimal policy was used which equals using the maximum operator.

			\subsubsection{Value Iteration}
				Finding the optimal Q-function is again possible using value iteration with \( T \to \infty \) as shown in \autoref{alg:valueIterationInfiniteHorizon}. The only slight change (except for making the value function time-independent) is again that the expectation for computing the Q-function has to be multiplied with the discount factor in \autoref{algline:valueIterationInfiniteHorizon-qComputation}.

				\begin{algorithm}  \DontPrintSemicolon
					\( V^\ast(\vec{s}) \gets 0 \) for all \( \vec{s} \in \mathcal{S} \) \;
					\Repeat{convergence of \( V^\ast \)}{
						\tcp{Compute Q-Function for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\( Q^\ast(\vec{s}, \vec{a}) \gets r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\ast(\vec{s}') \)  \label{algline:valueIterationInfiniteHorizon-qComputation} \;

						\tcp{Compute V-Function for all \( \vec{s} \in \mathcal{S} \):}
						\( V^\ast(\vec{s}) \gets \max_{\vec{a}} \, Q^\ast(\vec{s}, \vec{a}) \) \;
					}
					\tcp{Return optimal policy for all \( \vec{s} \in \mathcal{S} \):}
					\Return \( \pi^\ast(\vec{s}) \gets \arg\max_{\vec{a}} \, Q^\ast(\vec{s}, \vec{a}) \) \;

					\caption{Value Iteration for Infinite-Horizon Problems}
					\label{alg:valueIterationInfiniteHorizon}
				\end{algorithm}
			% end

			\subsubsection{Policy Iteration}
				One major drawback of value iteration as introduced before is that a lot of redundant maximization operations have to be performed when calculating the Q-function values. This drawback can be overcome by \emph{policy iteration}, a similar approach for computing the optimal value and Q-function. It is composed of two steps that are after each other until convergence:
				\begin{enumerate}
					\item \eqmakebox[policyIteration][l]{\emph{Policy Evaluation:}} Estimation of the quality of the states and actions for the current policy.
					\item \eqmakebox[policyIteration][l]{\emph{Policy Improvement:}} Improve the policy by taking the actions with the highest quality.
				\end{enumerate}
				The process of policy evaluation is shown in \autoref{alg:policyEvaluation}. The main difference is that the value and Q-functions are not the optimal ones any more but the ones for the given policy \(\pi\). This results in the computation of the value function on \autoref{algline:policyEvaluationInfiniteHorizon-vComputation} to become an expectation (the policy is not optimal anymore and so is the Q-function, so using the maximum action would be wrong). The next step for policy iteration is to improve the policy by taking the actions with the highest quality. This corresponds to choosing the policy as
				\begin{equation*}
					\pi(\vec{a} \given \vec{s}) =
						\begin{cases}
							1 & \text{if } \vec{a} = \arg\max_{\vec{a}'} \, Q^\pi(\vec{s}, \vec{a}') \\
							0 & \text{otherwise}
						\end{cases}
				\end{equation*}
				where \( Q^\pi \) is the Q-function found via policy evaluation. Putting it all together, policy iteration is shown in \autoref{alg:policyIteration}.

				\begin{algorithm}  \DontPrintSemicolon
					\KwIn{Policy \(\pi\)}
					\( V^\pi(\vec{s}) \gets 0 \) for all \( \vec{s} \in \mathcal{S} \) \;
					\Repeat{convergence of \( V^\pi \)}{
						\tcp{Compute Q-Function for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\( Q^\pi(\vec{s}, \vec{a}) \gets r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\pi(\vec{s}') \) \;

						\tcp{Compute V-Function for all \( \vec{s} \in \mathcal{S} \):}
						\( V^\pi(\vec{s}) \gets \sum_{\vec{a}} \pi(\vec{a} \given \vec{s}) Q^\pi(\vec{s}, \vec{a}) \)  \label{algline:policyEvaluationInfiniteHorizon-vComputation} \;
					}
					\Return \( V^\pi \) and/or \( Q^\pi \) as needed \;

					\caption{Policy Evaluation for Infinite-Horizon Problems}
					\label{alg:policyEvaluation}
				\end{algorithm}

				\begin{algorithm}  \DontPrintSemicolon
					\KwIn{Policy \(\pi\)}
					\( V^\pi(\vec{s}) \gets 0 \) for all \( \vec{s} \in \mathcal{S} \) \;
					\Repeat{convergence of \( \pi \)}{
						\Repeat{convergence of \( V^\pi \)}{
							\tcp{Compute Q-Function for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
							\( Q^\pi(\vec{s}, \vec{a}) \gets r(\vec{s}, \vec{a}) + \gamma \sum_{\vec{s}'} p(\vec{s}' \given \vec{s}, \vec{a}) V^\pi(\vec{s}') \) \;

							\tcp{Compute V-Function for all \( \vec{s} \in \mathcal{S} \):}
							\( V^\pi(\vec{s}) \gets \sum_{\vec{a}} \pi(\vec{a} \given \vec{s}) Q^\pi(\vec{s}, \vec{a}) \) \;
						}
						\tcp{Improve policy for all \( \vec{s} \in \mathcal{S} \) and \( \vec{a} \in \mathcal{A} \):}
						\(
							\pi(\vec{a} \given \vec{s}) =
								\begin{cases}
									1 & \text{if } \vec{a} = \arg\max_{\vec{a}'} \, Q^\pi(\vec{s}, \vec{a}') \\
									0 & \text{otherwise}
								\end{cases}
						\)
					}
					\Return \( \pi \) \;

					\caption{Policy Iteration for Infinite-Horizon Problems}
					\label{alg:policyIteration}
				\end{algorithm}
			% end
		% end
	% end

	\section{Continuous State-Action Space} % 3b.1, 3b.2
		\todo{Content}

		\subsection{Modeling the System} % 3b.5, 3b.7, 3b.8, 3b.9, 3b.10, 3b.11
			\todo{Content}
		% end

		\subsection{Linear Quadratic Regulator (LQR)} % 3b.12, 3b.13, 3b.14, 3b.15
			\todo{Content}

			\subsubsection{Recap of Value Iteration} % 3b.17, 3b.18
				\todo{Content}
			% end

			\subsubsection{Solving the Optimal Control Problem} % 3b.19, 3b.20, 3b.21, 3b.22, 3b.23, 3b.24, 3b.25, 3b.26, 2.27
				\todo{Content}

				\paragraph{Computing the Expectation} % 3b.21
					\todo{Content}
				% end

				\paragraph{Computing the Maximization} % 3b.23
					\todo{Content}
				% end
			% end
		% end

		\subsection{Approximating Nonlinear Systems} % 3b.32, 3b.33, 3b.34, 3b.35, 3b.36
			\todo{Content}

			\subsubsection{Local Solutions by Linearization} % 3b.37, 3b.38, 3b.39
				\todo{Content}
			% end
		% end

		\subsection{Optimal Control with Learned Models} % 3b.42, 3b.43, 3b.44, 3b.45, 3b.46, 3b.48, 3b.51
			\todo{Content}

			\subsubsection{State-Of-The-Art Approaches} % 3b.52, 3b.55
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 3a.39, 3a.40, 3a.41, 3a.42, 3b.59
		\todo{Content}
	% end
% end

\chapter{Approximate Optimal Control} % {3a.41, 3a.42}
	\todo{Content}

	\section{Differential Dynamic Programming (DDP)} % 4a.1, 4a.2, 4a.3, 4a.12
		\todo{Content}

		\subsection{Locally Nonlinear LQR} % 4a.13, 4a.14
			\todo{Content}
		% end

		\subsection{Differential Dynamic Programming} % 4a.15, 4a.16, 4a.17
			\todo{Content}

			\subsubsection{Implementation Details} % 4a.18, 4a.19, 4a.20
				\todo{Content}
			% end
		% end

		\subsection{Iterative LQR} % 4a.22, 4a.23, 4a.24
			\todo{Content}
		% end

		\subsection{Stochastic DDP} % 4a.25, 4a.26
			\todo{Content}
		% end

		\subsection{Guided Policy Search} % 4a.27, 4a.28
			\todo{Content}
		% end
	% end

	\section{Approximate Dynamic Programming} % 4b.1, 4b.2, 4b.6
		\todo{Content}

		\subsection{Function Approximation} % 4b.7, 4b.8, 4b.9, 4b.10, 4b.11, 4b.12
			\todo{Content}
		% end

		\subsection{Approximate Value Iteration} % 4b.13, 4b.14
			\todo{Content}

			\subsubsection{Theoretical Analysis and Bellman Operator} % 4b.15, 4b.16, 4b.17
				\todo{Content}
			% end

			\subsubsection{Approximation Error and Performance Loss} % 4b.18, 4b.19, 4b.20
				\todo{Content}
			% end
		% end

		\subsection{Approximate Policy Iteration} % 4b.21, 4b.23, 4b.24, 4b.25
			\todo{Content}

			\subsubsection{Approximation Error and Performance Loss} % 4b.26
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 4a.30, 4b.28
		\todo{Content}
	% end
% end

\chapter{State Estimation} % 6.1, 6.2, 6.5, 6.6
	\todo{Content}

	\section{Kalman Filter as an Optimal Filter} % 6.8
		\todo{Content}

		\subsection{Observers} % 6.9, 6.10
			\todo{Content}
		% end

		\subsection{Optimal Observers} % 6.11, 6.12, 6.13, 6.14, 6.15, 6.16, 6.17, 6.18
			\todo{Content}
		% end

		\subsection{Geometric Perspective} % 6.19
			\todo{Content}
		% end
	% end

	\section{Kalman Filter as Bayesian Inference} % 6.20, 6.21, 6.22, 6.23, 6.24
		\todo{Content}
	% end

	\section{Partially Observed Optimal Control} % 6.27, 6.28, 6.29, 6.30
		\todo{Content}
	% end

	\section{Extended, Unscented and Particle Filter} % 6.32, 6.33
		\todo{Content}

		\subsection{Extended Kalman Filter (EKF)} % 6.34
			\todo{Content}
		% end

		\subsection{Cubature Kalman Filter (CKF)} % 6.35, 6.36, 6.37, 6.38
			\todo{Content}
		% end

		\subsection{Unscented Kalman Filter (UKF)} % 6.39
			\todo{Content}
		% end

		\subsection{Particle Filter / Sequential Monte Carlo (PF/SMC)} % 6.40, 6.41, 6.56
			\todo{Content}

			\subsubsection{Importance Sampling} % 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Sampling} % 6.48, 6.49, 6.50, 6.51, 6.52
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Resampling} % 6.53, 6.54, 6.55
				\todo{Content}
			% end
		% end

		\subsection{Examples} % N/A
			\todo{Content}

			\subsubsection{Approximate Message Passing} % 6.44, 6.45, 6.46, 6.47
				\todo{Content}
			% end

			\subsubsection{Pendulum} % 6.57, 6.58, 6.59, 6.60
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 6.62, 6.63
		\todo{Content}
	% end
% end

\chapter{Model Learning} % 7.1, 7.2
	\todo{Content}

	\section{Models in Robotics} % 7.4, 7.5, 7.6, 7.7, 7.8, 7.9
		\todo{Content}
	% end

	\section{Modling Assumptions: White, Black and Gray} % 7.10, 7.11
		\todo{Content}

		\subsection{White-Box Strategy} % 7.12
			\todo{Content}
		% end

		\subsection{Black-Box Strategy} % 7.13
			\todo{Content}
		% end

		\subsection{Gray-Box Strategy} % 7.14
			\todo{Content}
		% end
	% end

	\section{System Identification and Signal Processing} % 7.15, 7.16, 7.17
		\todo{Content}

		\subsection{Impulse Response} % 7.18, 7.19, 7.20, 7.21, 7.22, 7.23
			\todo{Content}
		% end

		\subsection{Step Response} % 7.24, 7.25
			\todo{Content}
		% end

		\subsection{Characterization of Dynamical Systems} % 7.26, 7.27
			\todo{Content}
		% end

		\subsection{Frequency Analysis} % 7.28, 7.29
			\todo{Content}
		% end

		\subsection{Ornstein-Uhlenbeck Process} % 7.30, 7.31
			\todo{Content}
		% end

		\subsection{Active Learning} % 7.32, 7.33, 7.34
			\todo{Content}
		% end
	% end

	\section{Learning Models} % 7.36, 7.37
		\todo{Content}

		\subsection{Linear Gaussian Dynamical Systems (LGDS)} % 7.38, 7.39, 7.40, 7.41, 7.42, 7.43, 7.44
			\todo{Content}
		% end
	% end

	\section{Case Studies} % 7.47
		\todo{Content}

		\subsection{Combining Rigid Body Dynamics and GPs} % 7.48, 7.49, 7.50
			\todo{Content}
		% end

		\subsection{Deep Lagrangian Networks} % 7.51, 7.52, 7.53, 7.54, 7.55
			\todo{Content}
		% end

		\subsection{The Differentiable Recusive Newton-Euler Algorithm} % 7.56, 7.57, 7.58, 5.59, 7.60, 7.61, 7.62, 7.63
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 7.65
		\todo{Content}
	% end
% end

\chapter{Policy Representations} % 8.1, 8.4
	\todo{Content}

	\section{Parametric Policies} % 8.3, 8.7, 8.8, 8.9
		\todo{Content}
	% end

	\section{Off-The-Shelf Policies} % 8.10, 8.16
		\todo{Content}

		\subsection{Linear Basis Functions} % 8.11, 8.12, 8.13
			\todo{Content}
		% end

		\subsection{Radial Basis Functions (RBFs)} % 8.14, 8.15
			\todo{Content}
		% end
	% end

	\section{Movement Primitives} % 8.17, 8.18, 8.20, 8.21, 8.22, 8.23, 8.24, 8.25
		\todo{Content}

		\subsection{Dynamic Movement Primitives (DMDs)} % 8.26, 8.27, 8.28
			\todo{Content}

			\subsubsection{Temporal Scaling} % 8.29, 8.30, 8.34
				\todo{Content}
			% end

			\subsubsection{Multiple Degrees of Freedom} % 8.33
				\todo{Content}
			% end

			\subsubsection{Imitation Learning} % 8.35
				\todo{Content}
			% end
		% end

		\subsection{Probabilistic Movement Primitives (ProMPs)} % 8.40, 8.41, 8.42, 8.43, 8.44, 8.45
			\todo{Content}

			\subsubsection{Conditioning} % 8.46
				\todo{Content}
			% end

			\subsubsection{Combination} % 8.47
				\todo{Content}
			% end
		% end

		\subsection{Time-Independent Stable Movement Primitives} % 8.51
			\todo{Content}

			\subsubsection{Nonlinear Stable Dynamical Systems} % 8.52, 8.53, 8.54
				\todo{Content}
			% end
		% end

		\subsection{Imitation Flows} % 8.55, 8.56, 8.57
			\todo{Content}
		% end

		\subsection{Libraries of Primitives} % 8.59, 8.60, 8.61
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 8.64
		\todo{Content}
	% end
% end

\chapter{Model-Based Reinforcement Learning} % 9.1, 9.2, 9.5, 9.6
	\todo{Content}

	\section{Differentiation of Model-Based and Model-Free} % 9.7, 9.8
		\todo{Content}

		\subsection{Sample Efficiency} % 9.9, 9.10, 9.11
			\todo{Content}
		% end
	% end

	\section{Domain Knowledge in Reinforcement Learning} % 9.13, 9.14
		\todo{Content}

		\subsection{Performance Bias} % 9.15, 9.16, 9.17, 9.22, 9.25
			\todo{Content}
		% end

		\subsection{Local Optima and Sample-Based Methods} % 9.18, 9.21
			\todo{Content}

			\subsubsection{The Cross-Entropy Method (CEM)} % 9.19
				\todo{Content}
			% end

			\subsubsection{(Model Predictive) Path Integral Control (MPPI)} % 9.20
				\todo{Content}
			% end
		% end

		\subsection{Numerical Sensitivity} % 9.23
			\todo{Content}

			\subsubsection{Backprop-Through-Time} % 9.24
				\todo{Content}
			% end
		% end

		\subsection{Catastrophic Model Errors} % 9.26
			\todo{Content}
		% end
	% end

	\section{Optimism and Pessimism in Reinforcement Learning} % 9.27, 9.28, 9.29
		\todo{Content}

		\subsection{Aleatoric and Epistemic Uncertainty} % 9.30, 9.31
			\todo{Content}
		% end

		\subsection{Optimism Under Uncertainty} % 9.31, 9.32, 9.33, 9.34, 9.35, 9.39
			\todo{Content}

			\subsubsection{Neural Linear Models} % 9.36
				\todo{Content}
			% end

			\subsubsection{Variational Inference} % 9.37
				\todo{Content}
			% end

			\subsubsection{Ensembles} % 9.38
				\todo{Content}
			% end
		% end
	% end

	\section{Replanning} % 9.40, 9.41, 9.42
		\todo{Content}
	% end

	\section{Case Studies} % 9.43
		\todo{Content}

		\subsection{Probabilistic Learning for Control (PILCO)} % 9.44, 9.45, 9.46
			\todo{Content}

		\subsection{Guided Policy Search (GPS)} % 9.48, 9.49, 9.50, 9.51, 9.52
			\todo{Content}
		% end

		\subsection{Model Predictive Control (MPC)} % 9.54, 9.56
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 9.58, 12.63
		\todo{Content}
	% end
% end

\chapter{Value Function Methods} % 11.1, 11.2
	\todo{Content}

	\section{Recap of Dynamic Programming} % 11.4, 11.5, 11.6, 11.7
		\todo{Content}
	% end

	\section{Temporal Differences} % 11.8, 11.9, 11.10
		\todo{Content}

		\subsection{Policy Evaluation} % 11.11, 11.12, 11.13
			\todo{Content}
		% end

		\subsection{Policy Improvement} % 11.14, 11.15
			\todo{Content}
		% end
	% end

	\section{Value Function Approximation} % 11.16, 11.17, 11.18, 11.21, 11.22, 11.23
		\todo{Content}
	% end

	\section{Batch Reinforcement Learning Methods} % 11.25, 11.26
		\todo{Content}

		\subsection{Least-Squares Temporal Differences (LSTD)} % 11.27, 11.28, 11.29, 11.30
			\todo{Content}
		% end

		\subsection{Fitted Q-Iteration} % 11.31, 11.32, 11.33
			\todo{Content}
		% end
	% end

	\section{Case Study: Robot Soccer} % 11.34, 11.35, 11.36, 11.37, 11.38, 11.39
		\todo{Content}
	% end

	\section{Wrap-Up} % 11.41, 12.64
		\todo{Content}
	% end
% end

\chapter{Policy Search} % N/A
	\todo{Content}

	\section{Categorization of Policy Search} % 10.6, 10.7, 10.8
		\todo{Content}

		\subsection{Episode- vs. Step-Based Evaluation} % 10.9, 10.10, 10.11
			\todo{Content}
		% end

		\subsection{Epsiode-Based Policy Search} % 10.12, 10.13, 10.14
			\todo{Content}
		% end

		\subsection{Exploration vs. Exploitation} % 10.15, 10.16, 10.17, 10.18, 10.19
			\todo{Content}
		% end
	% end

	\section{Policy Gradient Methods} % 10.1, 10.2, 10.5
		\todo{Content}

		\subsection{Policy Gradients} % 10.20, 10.21
			\todo{Content}

			\subsubsection{Gradient Computation} % N/A
				\todo{Content}

				\paragraph{Finite Differences} % 10.22
					\todo{Content}
				% end

				\paragraph{Likelihood Policy Gradients} % 10.23
					\todo{Content}
				% end

				\paragraph{Baselines} % 10.24
					\todo{Content}
				% end
			% end

			\subsubsection{Step-Based Policy Gradient Methods} % 10.25, 10.26
				\todo{Content}

				\paragraph{Likelihood Ratio Gradient} % 10.27, 10.28
					\todo{Content}
				% end

				\paragraph{Using the Rewards to Come} % 10.29
					\todo{Content}
				% end
			% end

			\subsubsection{Choosing the Step Size, Metrics in Standard Gradients} % 10.30, 10.31, 10.32, 10.33
				\todo{Content}
			% end
		% end

		\subsection{Relative Entropy and Natural Gradient} % 10.34, 10.35, 10.41, 10.42, 10.43
			\todo{Content}

			\subsubsection{Computing the Natural Gradient} % N/A
				\todo{Content}

				\paragraph{Episode-Based} % 10.44
					\todo{Content}
				% end

				\paragraph{Step-Based} % 10.45
					\todo{Content}
				% end
			% end

			\subsubsection{Compatible Function Approximation} % 10.46, 10.47, 10.48
				\todo{Content}

				\paragraph{Connection to Value Function Approximation} % 10.49, 10.50, 10.51
					\todo{Content}
				% end

				\paragraph{Episodic Natural Actor-Critic} % 10.52, 10.53, 10.54
					\todo{Content}
				% end
			% end
		% end
	% end

	\section{Probabilistic Policy Search} % 12.1, 12.2
		\todo{Content}

		\subsection{Success Matching Principle} % 12.5, 12.6, 12.7
			\todo{Content}
		% end

		\subsection{Weighted Maximum Likelihood} % 12.8, 12.9, 12.10, 12.11
			\todo{Content}

			\subsubsection{Difference to Policy Gradients} % 12.12
				\todo{Content}
			% end

			\subsubsection{Computing the Weights} % 12.13
				\todo{Content}

				\paragraph{Notes on Expectation Maximization} % 12.14
					\todo{Content}
				% end

				\paragraph{Notes on the Exponential Transformation} % 12.15
					\todo{Content}
				% end
			% end

			\subsubsection{Illustration and Results} % 12.16, 12.17, 12.19, 12.21
				\todo{Content}
			% end
		% end

		\subsection{Relative Entropy Policy Search (REPS)} % 12.22, 12.23
			\todo{Content}

			\subsubsection{Optimization Problem} % 12.24
				\todo{Content}

				\paragraph{Solving} % 12.25, 12.26
					\todo{Content}
				% end
			% end
		% end

		\subsection{REPS for Contextual Policy Search} % 12.28, 12.29, 12.30
			\todo{Content}

			\subsubsection{Optimization Problem} % 12.31
				\todo{Content}

				\paragraph{Solving} % 12.32, 12.33, 12.34, 12.35
					\todo{Content}
				% end
			% end

			\subsubsection{Contextual Policies with Weighted ML} % 12.36
				\todo{Content}
			% end
		% end

		\subsection{Learning Versatile Solutions} % 12.40, 12.41, 12.47
			\todo{Content}

			\subsubsection{Illustration} % 12.42, 12.43
				\todo{Content}
			% end

			\subsubsection{Hierarchy} % 12.44
				\todo{Content}

				\paragraph{Naive Hierarichal Approach} % 12.45, 12.46
					\todo{Content}
				% end

				\paragraph{Hierarichal REPS (HiREPS)} % 12.48, 12.49
					\todo{Content}
				% end
			% end
		% end

		\subsection{Sequencing Movement Primitives} % 12.54, 12.55, 12.56
			\todo{Content}

			\subsubsection{Sequential REPS} % 12.57, 12.65, 12.66
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 10.55, 10.56, 10.58, 12.60
		\todo{Content}
	% end
% end

\chapter{Imitation Learning: Behavioral Cloning and Inverse RL} % 13.1, 13.3, 13.4, 13.5, 13.8, 13.9, 13.10, 13.11, 13.12
	\todo{Content}

	\section{Distribution Matching} % 13.13
		\todo{Content}

		\subsection{Behavioral Cloning} % 13.14, 13.43
			\todo{Content}

			\subsubsection{Direct Behavioral Cloning} % 13.15, 13.16, 13.17, 13.18, 13.19, 13.20
				\todo{Content}
			% end

			\subsubsection{DAGGER: New Samples to Learn to Recover} % 13.21
				\todo{Content}
			% end

			\subsubsection{DART: Robustness in Imitation Learning} % 13.22
				\todo{Content}
			% end
		% end

		\subsection{Generative Adversarial Learning} % 13.23, 13.24, 13.25
			\todo{Content}
		% end
	% end

	\section{Inverse Reinforcement Learning} % 13.26, 13.27, 13.42, 13.44
		\todo{Content}

		\subsection{Basic Principle} % 13.46, 13.49, 13.55
			\todo{Content}

			\subsubsection{Feature-Based Reward Function} % 13.47, 13.48
				\todo{Content}
			% end

			\subsubsection{Constraint Generation} % 13.50, 13.51, 13.58
				\todo{Content}
			% end

			\subsubsection{Ill-Posed Problem} % 13.52, 13.54
				\todo{Content}
			% end

			\subsubsection{(Structured) Max. Margin Solution} % 13.53, 13.56
				\todo{Content}
			% end

			\subsubsection{Expert Suboptimality} % 13.57, 13.60
				\todo{Content}
			% end
		% end

		\subsection{Feature Matching by Max. Entropy} % 13.61
			\todo{Content}

			\subsubsection{Max. Entropy Approach to Inference} % 13.62
				\todo{Content}
			% end

			\subsubsection{Max. Entropy Approach to Inverse RL} % 13.63
				\todo{Content}
			% end

			\subsubsection{Maximum-Casual-Entropy Inverse RL} % 13.64, 13.65
				\todo{Content}
			% end
		% end

		\subsection{Reward-Parameterized Policies} % 13.66, 13.67
			\todo{Content}
		% end
	% end

	\section{Case Studies} % 13.68, 13.69
		\todo{Content}

		\subsection{Highway Driving} % 13.70
			\todo{Content}
		% end

		\subsection{Max. Margin} % 13.71, 13.72
			\todo{Content}
		% end

		\subsection{Parking Lot Navigation} % 13.73, 13.74, 13.75, 13.76, 13.77
			\todo{Content}
		% end

		\subsection{Human Path Planning} % 13.78, 13.79, 13.80, 13.81, 13.82, 13.83
			\todo{Content}
		% end

		\subsection{Goal Inference} % 13.84, 13.85
			\todo{Content}
		% end

		\subsection{Quadruped} % 13.86, 13.87, 13.88
			\todo{Content}
		% end

		\subsection{Extreme Helicopter Flight} % 13.89, 13.90, 13.91, 13.92
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 13.45, 13.94
		\todo{Content}
	% end
% end

\chapter{Bayesian Reinforcement Learning} % 14.1, 14.2, 14.15, 15.16, 14.17, 14.18
	\todo{Content}

	\section{Recap of Bayesian Methods} % 14.4, 14.5, 14.6, 14.7, 14.8, 14.9, 14.14
		\todo{Content}

		\subsection{Conjugate Prior} % 14.10, 14.11, 14.12, 14.13
			\todo{Content}
		% end
	% end

	\section{Bandits and Thompson Sampling} % 14.19, 14.20, 14.21
		\todo{Content}

		\subsection{Restaurant Selection} % 14.22, 14.23, 14.24, 14.25, 14.26, 14.27, 14.28, 14.29
			\todo{Content}
		% end
	% end

	\section{Model-Based Bayesian RL for Discrete MDPs} % 14.30, 14.31, 14.32, 14.33
		\todo{Content}

		\subsection{Belief} % 14.34, 14.35
			\todo{Content}
		% end

		\subsection{State Transition Model} % 14.36, 14.37
			\todo{Content}
		% end

		\subsection{Optimal Value Function for BAMPDs} % 14.38, 14.39
			\todo{Content}
		% end

		\subsection{Solving for the Optimal Value Function} % 14.40
			\todo{Content}
		% end
	% end

	\section{Continuous MDPs and Dual Control} % 14.41, 14.42, 14.43, 14.44
		\todo{Content}

		\subsection{One-Dimensional Linear Gaussian Dual Control} % 14.45, 14.46, 14.47, 14.48, 14.49, 14.50
			\todo{Content}
		% end

		\subsection{Practical Dual Control} % 14.51
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 14.53
		\todo{Content}
	% end
% end

\chapter{Outlook} % 15.1
	\todo{Content}

	\section{Recap} % 15.4, 15.5, 15.6, 15.7, 15.8, 15.9, 15.10, 15.11
		\todo{Content}
	% end

	\section{Open Research Question} % 15.13
		\todo{Content}
	% end
% end
