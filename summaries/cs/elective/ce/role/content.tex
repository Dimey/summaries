\lstset{language = Python}



\chapter{Introduction} % 1.1, 1.2, 1.7
	Most of the content in this summary, the ideas, the underlying structure and the image ideas are taken from the lecture \href{https://www.ias.informatik.tu-darmstadt.de/Teaching/RobotLearningLecture}{"Robot Learning"} by \href{https://www.ias.informatik.tu-darmstadt.de/Team/JanPeters}{Prof. Jan Peters}. It is really just a \emph{summary} of the contents of the lecture.

	\todo{Content}

	\section{History} % 1.8, 1.9, 1.10, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19, 1.20
		\todo{Content}

		\subsection{Deep Learning} % 1.21, 1.22, 1.23
			\todo{Content}
		% end

		\subsection{How Long Does Learning Take?} % 1.24, 1.25
			\todo{Content}
		% end

		\subsection{Inductive Biases} % 1.26, 1.27, 1.28
			\todo{Content}
		% end
	% end
% end

\chapter{Statistics, Linear Algebra and Calculus Refresher} % 5b.4
	\todo{Content}

	\section{Basics} % 5b.5, 5b.6, 5b.7, 5b.8
		\todo{Content}

		\subsection{Entropy} % 5b.9
			\todo{Content}
		% end

		\subsection{Gaussian Distribution and Properties} % 5b.17, 5b.18, 5b.19
			\todo{Content}
		% end
	% end

	\section{Kullback-Leibler Divergence} % 10.36, 10.37, 10.38
		\todo{Content}

		\subsection{Fisher Information Matrix} % 10.39, 10.40
			\todo{Content}
		% end
	% end

	\section{Monte-Carlo Integration and Gradient Estimation} % 5b.10
		\todo{Content}

		\subsection{Gradient Estimation} % 5b.11, 5b.12, 5b.13
			\todo{Content}
		% end
	% end

	\section{Linear Algebra and Calculus} % 5b.15
		\todo{Content}

		\subsection{Moore-Penrose Pseudo-Inverse} % 5b.16
			\todo{Content}
		% end
	% end
% end

\chapter{Robotics}
	This chapter covers the basics of robotics, covering modeling position, velocity, acceleration and forces as well as representing trajectories. Also the main concepts of linear and model-based control are covered.

	The definition of what a robot are quite diverse. The robotics institute of America defines a robot as follows: "A robot is a reprogrammable multi-functional manipulator designed to move material, parts, tools, or specialized devices through variable programmed motions for the performance of a variety of tasks." Another, rather inverse, definition is from G. Randelov: "A computer is just an amputee robot."

	\section{Modeling Robots}
		Modeling of a robot can be split into two separate categories: kinematics and dynamics. In \emph{kinematics}, only the geometric properties of the robot are modeled, e.g. the link length. In \emph{dynamics}, the forces acting on the links and joints are analyzed. There are two types of joints that can be used to model every joint out there: revolute and prismatic joints, whilst revolute joints are the most common ones. The displacement of a revolute joint is an angle (typically in radians), the displacement of a prismatic joint is a distance (typically in meters).

		The displacements of all joints in a robot is denoted with a vector \(\vec{q}\), the task space (e.g. the Cartesian coordinate system of the world) is denoted by \(\vec{x}\) and the state (i.e. the variables of the robot and the environment) is denoted by \(\vec{s}\). The set of places \(\vec{x}\) in the task space that the robot can reach is called the \emph{workspace}. That is, everything outside the workspace is not reachable.

		Actions that can be taken by the controlled are denoted by \(\vec{u}\) or \(\vec{a}\) and are most often in some way related to torques in the joints. A (control) policy \(\pi\) then maps the state \(\vec{s}\) onto some action \(\vec{u}\) to take, either in a deterministic or stochastic way:
		\begin{itemize}
			\item \eqmakebox[modelingRobotsPolicy][l]{Deterministic:} \( \vec{u} = \pi(\vec{s}) \)
			\item \eqmakebox[modelingRobotsPolicy][l]{Stochastic:}    \( \vec{u} \sim \pi(\vec{u} \given \vec{s}) \)
		\end{itemize}
		The complete system of a robot including the generation of the trajectory to perform as well as the controller is shown in \autoref{fig:robotsCompleteBlockDiagram}.

		\begin{figure}
			\centering
			\begin{tikzpicture}[block/.style = { draw, rectangle, minimum height = 1cm, minimum width = 2.5cm }]
				\node [block] (t) {\textbf{Trajectory}};
				\node [block, right = 2 of t] (c) {\textbf{Control}};
				\node [block, right = 2 of c] (d) {\textbf{Dynamics}};
				\node [block, right = 2 of d] (k) {\textbf{Kinematics}};

				\coordinate [right = 0 of t] (tr1);
				\coordinate [left = 0 of c] (cl1);
				\coordinate [right = 0 of c] (cr1);
				\coordinate [left = 0 of d] (dl1);
				\coordinate [right = 0 of d] (dr1);
				\coordinate [left = 0 of k] (kl1);

				\coordinate [below = 0 of c] (cb);
				\coordinate [left = 0.5 of cb] (cbl);
				\coordinate [below = 1.75 of cbl] (cblb);
				\coordinate [right = 0.5 of cb] (cbr);
				\coordinate [below = 0.75+0.125 of cbr] (cbrb);
				\coordinate [below = 0 of d] (db);
				\coordinate [below = 0.75+0.125 of db] (dbb);
				\coordinate [below = 0 of k] (kb);
				\coordinate [below = 1.75 of kb] (kbb);

				\draw [->] (tr1) -- node[above]{\( \vec{x}_d \), \( \dot{\vec{x}}_d \), \( \ddot{\vec{x}}_t \)} (cl1);
				\draw [->] (cr1) -- node[above]{\( \vec{u} \)} (dl1);
				\draw [->] (dr1) -- node[above]{\( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \)} (kl1);
				\draw [->] (db) -- (dbb) -- node[above]{\( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \)} (cbrb) -- (cbr);
				\draw [->] (kb) -- (kbb) -- node[above]{\( \vec{x} \), \( \dot{\vec{x}} \), \( \ddot{\vec{x}} \)} (cblb) -- (cbl);
			\end{tikzpicture}
			\caption{Block diagram of a complete system with the desired values \( \vec{x}_d \), \( \dot{\vec{x}}_d \), \( \ddot{\vec{x}}_t \), the joint angles/velocities/accelerations \( \vec{q} \), \( \dot{\vec{q}} \), \( \ddot{\vec{q}} \), the end-effector positions/velocities/accelerations \( \vec{x} \), \( \dot{\vec{x}} \), \( \ddot{\vec{x}} \), and the motor commands/torques \( \vec{u} \).}
			\label{fig:robotsCompleteBlockDiagram}
		\end{figure}

		\subsection{Kinematics}
			Forward kinematics tackle the problem to get the position of the end-effector given the current joint positions or, more general, the position of any point in reference to a set coordinate system. Forward kinematics therefore describe a mapping from joint- to task-space:
			\begin{equation*}
				\vec{x} = \vec{f}(\vec{q})
			\end{equation*}
			For simple robots, e.g. a two-dimensional robot with only few revolute joints or a robot with only prismatic joints, the forward kinematics can be found straightforwardly with geometric knowledge. For more complex robots, however, more sophisticated methods are needed to not waste a lot of time finding the forward kinematics model.

			\subsubsection{Rotations and Euler Angles}
				To model revolute joints, a decent modeling of rotations is needed. In two-dimensional settings, a rotary transformation around an arbitrary angle \(\alpha\) is described by the following matrix:
				\begin{equation*}
					\mat{R}(\alpha) =
						\begin{bmatrix}
							\cos\alpha & -\sin\alpha \\
							\sin\alpha &  \cos\alpha
						\end{bmatrix}
				\end{equation*}
				In three-dimensional settings, one transformation is needed for a rotation around one of the Cartesian axis. Letting \(x\), \(y\) and \(z\) be the first, second and third Cartesian axis, respectively, the corresponding rotation matrices around an angle \(\alpha\) are given as:
				\begin{align*}
					\mat{R}_x(\alpha) =
						\begin{bmatrix}
							1 & 0          & 0           \\
							0 & \cos\alpha & -\sin\alpha \\
							0 & \sin\alpha &  \cos\alpha
						\end{bmatrix}
					&&
					\mat{R}_y(\alpha) =
						\begin{bmatrix}
							 \cos\alpha & 0 & \sin\alpha \\
							 0          & 1 & 0          \\
							-\sin\alpha & 0 & \cos\alpha
						\end{bmatrix}
					&&
					\mat{R}_z(\alpha) =
						\begin{bmatrix}
							\cos\alpha & -\sin\alpha & 0 \\
							\sin\alpha &  \cos\alpha & 0 \\
							0          &  0          & 1
						\end{bmatrix}
				\end{align*}

				One of many methods of representing arbitrary rotations are \emph{Euler angles} which are parameterized by roll \(\psi\), pitch \(\theta\) and yaw \(\phi\). They transform from a coordinate system \(1\) into a coordinate system \(0\) as follows:
				\begin{equation*}
					\mat{R}_1^0 = \mat{R}_z(\phi) \mat{R}_y(\theta) \mat{R}_x(\psi)
				\end{equation*}
				Problems with Euler angles are that they are not unique, i.e. different Euler angles may describe the same rotation, and it is hard to quantify differences between Euler angles. An alternative for describing rotations are \emph{unit quaternions} and angle-axis formulations, where both a rotation axis as well as a rotation angle are given. Both of them solve the singularities with Euler angles and it is easier to computer differences of orientations.
			% end

			\subsubsection{Homogeneous Transformations}
				\emph{Homogeneous transformations} are a ways to represent combined translation and rotation transformations, e.g.
				\begin{equation*}
					\vec{p}^1 = \mat{R}_2^1 \vec{p}^2 + \vec{\delta}^1
					\quad\implies\quad
					\vec{p}^0 = \mat{R}_1^0 \vec{p}^1 + \vec{\delta}^0 = \mat{R}_1^0 \big( \mat{R}_2^1 \vec{p}^2 + \vec{\delta}^1 \big) + \vec{\delta}^0
				\end{equation*}
				into a single matrix-vector multiplication, making the computation less clumsy:
				\begin{equation*}
					\vec{p}^0 = \mat{R}_1^0 \vec{p}^1 + \vec{\delta}^0
					\quad\implies\quad
					\underbrace{\begin{bmatrix}
						\vec{p}^1 \\
						1
					\end{bmatrix}}_{\tilde{\vec{p}}^1}
					=
					\underbrace{\begin{bmatrix}
						\mat{R}_1^0 & \vec{\delta}^1 \\
						\vec{0}     & 1
					\end{bmatrix}}_{\mat{H}_1^0}
					\underbrace{\begin{bmatrix}
						\vec{p}^0 \\
						1
					\end{bmatrix}}_{\tilde{\vec{p}}^0}
				\end{equation*}
				Hence, multiple translation-rotation transformations can be stacked as follows:
				\begin{equation*}
					\tilde{\vec{p}}^0 = \mat{H}_1^0 \mat{H}_2^1 \cdots \mat{H}_n^{n - 1} \tilde{\vec{p}}^n
				\end{equation*}
			% end

			\subsubsection{Denavit-Hartenberg Convention}
				A common convention for describing the coordinate systems of a robot is the \emph{Denavit-Hartenberg convention} which describes the kinematics of a complete robot with only four parameters per joint. These four parameters are:
				\begin{itemize}
					\item \(\theta_i\), the angle between \(x_{i - 1}\) and \(x_i\), measured around \(z_{i - 1}\). Variable if \(i\) is a revolute joint.
					\item \(d_i\), the distance of the origin of \(S_{i - 1}\) along \(z_{i - 1}\) to the intersection with \(x_i\). Variable if \(i\) is a prismatic joint.
					\item \(a_i\), the distance between the intersection of \(z_{i - 1}\) and \(x_i\) along \(x_i\) towards the origin of \(S_i\).
					\item \(\alpha_i\), the angle between \(z_{i - 1}\) and \(z_i\), measured around \(x_i\).
				\end{itemize}
				Along with that, the following conditions have to hold for the coordinate systems \(S_i\):
				\begin{itemize}
					\item The origins of the coordinate systems lie on the movement axis.
					\item The \(z_{i - 1}\)-axis lies along the movement axis of the \(i\)-th joint.
					\item The \(x_i\)-axis is perpendicular to the \(z_{i - 1}\)-axis and point away from it.
					\item The \(x_i\)-axis and the \(z_{i - 1}\)-axis have an intersection.
				\end{itemize}
				Placing the coordinate system can be formulated as an algorithm, check out the foundations of robotics summary\footnote{\url{https://projects.frisp.org/documents/29} (German)} for more details.
			% end
		% end

		\subsection{Differential Forward Kinematics (Velocities and Accelerations)}
			Often it is necessary to get the velocities \(\dot{\vec{x}}\) and accelerations \(\ddot{\vec{x}}\}\) of the end-effector. These can be computed straightforwardly using the chain rule
			\begin{align*}
				\dot{\vec{x}} = \dv{t} \vec{f}(\vec{q}) = \dv{\vec{f}(\vec{q})}{\vec{q}} \dv{\vec{q}}{t} = \underbrace{\mat{J}(\vec{q})}_{\mathclap{\text{Jacobian}}} \dot{\vec{q}}
				&&
				\ddot{\vec{x}} = \dot{\mat{J}}(\vec{q}) \dot{\vec{q}} + \mat{J}(\vec{q}) \ddot{\vec{q}}
			\end{align*}
			where the end-effector velocities and accelerations are computed from the joint velocities and accelerations.

			\subsubsection{Singularities}
				In some cases, the Jacobian \( \mat{J}(\vec{q}) \) might get rank-deficient, i.e. \( \det \mat{J}(\vec{q}) = 0 \). In this case, infinite velocities in the joints are required to reach a desired end-effector velocities as the Jacobian is not invertible anymore. These positions are called \emph{singularities} of the robot and correspond to a loss in the degrees of freedom, e.g. when the robot stretches out its arm.
			% end

			\subsubsection{Computing the Jacobians}
				There are two main ways of computing the Jacobian:
				\begin{description}
					\item[Analytical] As before, the Jacobian is derived analytically and has the problem of singularities.
					\item[Geometric]  These are derived from geometric insights, maybe circumventing the "representational singularities".
				\end{description}
				Both of these ways are just different representations of the same concept.
			% end
		% end

		\subsection{Inverse Kinematics}
			The obvious next problem is how to get the required joint positions to reach a given end-effector pose\footnote{A pose refers to both the position and the orientation at the same time.}. Hence, the \emph{inverse kinematics} are a mapping from the task- to the joint-space:
			\begin{equation*}
				\vec{q} = \vec{f}^{-1}(\vec{x})
			\end{equation*}
			For really simple robots, this can again be solved from a geometric perspective. However, there is most likely more than one solution for a given end-effector position! With certain redundancies in the robot, it is even possible to get infinite solutions.

			An additional problem is that the inverse kinematics equations are often not solvable analytically, hence numerical methods have to be used. But those do not guarantee to find all solutions which might be necessary to move into an "optimal" joint configuration. However, for a lot of industrial robots, invertible solutions are possible due to intelligent joint and link design.
		% end

		\subsection{Differential Inverse Kinematics} % 2.96, 2.97
			\todo{Content}
		% end

		\subsection{Dynamics}
			For the dynamics of a robot, a forward model
			\begin{equation*}
				\ddot{\vec{q}} = \vec{f}(\vec{q}, \dot{\vec{q}}, \vec{u})
			\end{equation*}
			is wanted that gives the joint accelerations given the joint positions and velocities and torques/forces (in case of revolute/prismatic joints, respectively). Usually the dynamics are represented in the general form
			\begin{equation*}
				\vec{u} = \mat{M}(\vec{q}) \ddot{\vec{q}} + \vec{c}(\vec{q}, \dot{\vec{q}}) + \vec{g}(\vec{q})
			\end{equation*}
			with the motor commands \(\vec{u}\), the joint positions/velocities/accelerations \(\vec{q}\)/\(\dot{\vec{q}}\)/\(\ddot{\vec{q}}\), the mass matrix \(\mat{M}(\vec{q})\), the Coriolis and centripetal forces \(\vec{c}(\vec{q}, \dot{\vec{q}})\), and the gravity \(\vec{g}(\vec{q})\). This general form can be easily inverted to get the joint accelerations, as the mass matrix is always positive definite and hence invertible:
			\begin{equation*}
				\ddot{\vec{q}} = \mat{M}^{-1}(\vec{q}) \big( \vec{u} - \vec{c}(\vec{q}, \dot{\vec{q}}) - \vec{g}(\vec{q}) \big)
			\end{equation*}

			\subsubsection{Computing the Forces}
				To compute the rigid body forces, there are two central methods:
				\begin{enumerate}
					\item Newton-Euler Method
						\begin{itemize}
							\item Force-dissection-based approach.
							\item Can be formalized nicely, check out the foundations of robotics summary\footnote{\url{https://projects.frisp.org/documents/29} (German)} for more details.
						\end{itemize}
					\item Lagrangian Method
						\begin{itemize}
							\item Energy-based approach.
							\item Based on Lagrangian mechanics, check out the theoretical physics: classical mechanics summary\footnote{\url{https://projects.frisp.org/documents/31} (German)} for more details.
						\end{itemize}
				\end{enumerate}
				Other forces than rigid body forces, e.g. friction, are a lot harder to model as there is not general recipe for modeling them (friction is not so well understood).

				\paragraph{Newton-Euler Method} % 2.40
					\todo{Content}
				% end

				\paragraph{Lagrangian Methods} % 2.41, 2.42, 2.43
					\todo{Content}
				% end

				\paragraph{Comparison of Newton-Euler and Lagrangian} % 2.44
					\todo{Content}
				% end
			% end

			\subsubsection{General Forms} % 2.45, 2.47, 2.48, 2.49
				\todo{Content}
			% end
		% end
	% end

	\section{Representing Trajectories} % 2.50, 2.51, 2.52, 2.53
		\todo{Content}

		\subsection{Splines} % N/A
			\todo{Content}

			\subsubsection{Cubic Splines} % 2.54, 2.55, 2.56
				\todo{Content}
			% end

			\subsubsection{Quintic Splines} % 2.57, 2.58
				\todo{Content}
			% end
		% end

		\subsection{Alternatives} % 2.59
			\todo{Content}
		% end
	% end

	\section{Control in Joint Space} % 2.60, 2.62, 2.63, 2.64
		\todo{Content}

		\subsection{Linear Feedback Control} % 2.65, 2.66, 2.67, 2.68, 2.69, 2.70
			\todo{Content}

			\subsubsection{P-Controller} % 2.71
				\todo{Content}
			% end

			\subsubsection{PD-Controller} % 2.74, 2.80
				\todo{Content}

				\paragraph{With Gravity Compensation} % 2.76, 2.77
					\todo{Content}
				% end
			% end

			\subsubsection{PID-Controller} % 2.79
				\todo{Content}
			% end
		% end

		\subsection{Model-Based Control} % 2.81, 2.82, 2.83
			\todo{Content}
		% end

		\subsection{Feedforward Control} % 2.84, 2.85, 2.86
			\todo{Content}
		% end
	% end

	\section{Control in Task Space} % 2.87, 2.88, 2.89
		\todo{Content}

		\subsection{Differential Inverse Kinematics} % 2.97
			\todo{Content}
		% end

		\subsection{Jacobian Transpose} % 2.98, 2.99
			\todo{Content}
		% end

		\subsection{Jacobian Pseudo-Inverse} % 2.100, 2.101
			\todo{Content}
		% end

		\subsection{Task-Prioritization with Null-Space Movements} % 2.102
			\todo{Content}
		% end

		\subsection{More Advanced Solutions} % 2.103
			\todo{Content}
		% end

		\subsection{Singularities and Damped Pseudo-Inverse} % 2.104, 2.105
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 2.107
		\todo{Content}
	% end
% end

\chapter{Machine Learning Foundations} % 5b.1, 5b.2, 5b.21, 5b.22, 5b.23, 5b.24, 5b.25, 5b.26
	\todo{Content}

	\section{The Six Machine Learning Choices} % 5b.27, 5b.28, 5b.62
		\todo{Content}

		\subsection{Problem Class} % 5b.30, 5b.31, 5b.32, 5b.33, 5b.34, 5b.35
			\todo{Content}
		% end

		\subsection{Problem Assumptions} % 5b.36
			\todo{Content}
		% end

		\subsection{Evaluation} % 5b.37, 5b.38, 5b.39, 5b.40, 5b.41
			\todo{Content}
		% end

		\subsection{Model Type} % 5b.42
			\todo{Content}
		% end

		\subsection{Model Class Selection} % 5b.43, 5b.44, 5b.45
			\todo{Content}
		% end

		\subsection{Algorithm Realization} % 5b.46
			\todo{Content}
		% end

		\subsection{Example} % 5b.47, 5b.48, 5b.49, 5b.50, 5b.51, 5b.52, 5b.53, 5b.54, 5b.55, 5b.56, 5b.57, 5b.58, 5b.59, 5b.60, 5b.61
			\todo{Content}
		% end
	% end

	\section{Evaluation} % 5b.63, 5b.65
		\todo{Content}

		\subsection{Occams Razor} % 5b.66
			\todo{Content}
		% end

		\subsection{Bias and Variance} % 5b.67, 5b.68, 5b.69
			\todo{Content}
		% end

		\subsection{Model Selection} % 5b.70, 5b.71, 5b.72, 5b.73
			\todo{Content}
		% end
	% end

	\section{Frequentis vs. Bayesian Assumptions} % 5b.74, 5b.75
		\todo{Content}

		\subsection{Maximum Likelihood Estimation} % 5b.76, 5b.77, 5b.78, 5b.79
			\todo{Content}
		% end

		\subsection{Bayesian Thinking and Maximum A-Posteriori} % 5b.80, 5b.81, 5b.82
			\todo{Content}

			\subsubsection{Ridge Regression (Tikhonov Regularized Regression)} % 5b.83, 5b.84
				\todo{Content}
			% end

			\subsubsection{Predictions} % 5b.85, 5b.86, 5b.87
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Regression} % 5b.88, 5b.89, 5b.90, 5b.91, 5b.92
			\todo{Content}
		% end
	% end

	\section{Hand-Crafted Feature Construction} % 5b.93, 5b.94
		\todo{Content}

		\subsection{Discrete Inputs} % 5b.95, 5b.96
			\todo{Content}
		% end

		\subsection{Continuous Inputs} % 5b.97
			\todo{Content}

			\subsubsection{One-Hot} % 5b.98
				\todo{Content}
			% end

			\subsubsection{Radial Basis Functions (RBFs)} % 5b.99, 5b.100, 5b.101
				\todo{Content}
			% end
		% end
	% end

	\section{Automatic (Linear) Feature Construction} % 5b.102, 5b.103, 5b.104, 5b.105, 5b.106, 5b.107
		\todo{Content}

		\subsection{Respective Field Weighted Regression (RFWR)} % 5b.108, 5b.109, 5b.110
			\todo{Content}
		% end

		\subsection{Automatic Adaption of RFWR} % 5b.111, 5b.112, 5b.113
			\todo{Content}
		% end
	% end

	\section{Non-Parametric Approaches} % 5b.114, 5b.115, 5b.116, 5b.117
		\todo{Content}

		\subsection{Weighted Linear Regression} % 5b.118, 5b.119, 5b.120, 5b.121, 5b.122
			\todo{Content}
		% end

		\subsection{Locally Weighted Bayesian Linear Regression} % 5b.123
			\todo{Content}
		% end

		\subsection{Kernel Methods} % 5b.124, 5b.125, 5b.128
			\todo{Content}

			\subsubsection{Kernel Ridge Regression} % 5b.126, 5b.127
				\todo{Content}
			% end
		% end

		\subsection{Bayesian Kernel Regression: Gaussian Processes (GPs)} % 5b.129, 5b.130, 5b.131, 5b.136
			\todo{Content}

			\subsubsection{GP-Posterior} % 5b.132, 5b.133, 5b.134, 5b.135
				\todo{Content}
			% end
		% end
	% end

	\section{Neural Networks} % 5c.1, 5c.2, 5c.4, 5c.5, 5c.6, 5c.7, 5c.7, 5c.8, 5c.9
		\todo{Content}

		\subsection{Biology and Neuron Abstraction} % 5c.10, 5c.11, 5c.12, 5c.13
			\todo{Content}
		% end

		\subsection{Components of a Neural Network} % N/A
			\todo{Content}

			\subsubsection{Single- and Multi-Layer Networks} % 5c.14, 5c.15, 5c.16, 5c.17, 5c.18, 5c.19
				\todo{Content}
			% end

			\subsubsection{Topologies} % 5c.20, 5c.21
				\todo{Content}
			% end

			\subsubsection{Activation Functions} % 5c.22, 5c.23, 5c.24, 5c.25
				\todo{Content}
			% end

			\subsubsection{Output Neurons} % 5c.26
				\todo{Content}
			% end

			\subsubsection{Loss Functions} % 5c.27
				\todo{Content}
			% end
		% end

		\subsection{Forward- and Backpropagation} % 5c.28, 5c.29, 5c.30
			\todo{Content}

			\subsubsection{Forwardpropagation} % 5c.31, 5c.32
				\todo{Content}
			% end

			\subsubsection{Backpropagation} % 5c.33, 5c.34, 5c.35, 5c.37
				\todo{Content}

				\paragraph{Skip Connections} % 5c.36
					\todo{Content}
				% end
			% end

			\subsubsection{Finite Differences} % 5c.38, 5c.39
				\todo{Content}
			% end

			\subsubsection{Automatic Differentiation} % 5c.40
				\todo{Content}
			% end
		% end

		\subsection{Efficient Gradient Descent} % 5c.42, 5c.43, 5c.44, 5c.47
			\todo{Content}

			\subsubsection{Stochastic Gradient Descent} % 5c.45, 11.19, 11.20
				\todo{Content}
			% end

			\subsubsection{Mini-Batch Gradient Descent} % 5c.46
				\todo{Content}
			% end
		% end

		\subsection{Choosing the Learning Rate} % 5c.48, 5c.49, 5c.50, 5c.51
			\todo{Content}

			\subsubsection{Plateaus and Valleys} % 5c.52
				\todo{Content}
			% end

			\subsubsection{Adaptive Learning Rates} % N/A
				\todo{Content}

				\paragraph{Momentum} % 5c.53
					\todo{Content}
				% end

				\paragraph{Adadelta} % 5c.54
					\todo{Content}
				% end

				\paragraph{Adam} % 5c.55
					\todo{Content}
				% end
			% end
		% end

		\subsection{Choosing the Descent Direction} % N/A
			\todo{Content}

			\subsubsection{Hessian Approaches} % 5c.56
				\todo{Content}
			% end

			\subsubsection{Conjugate Gradient} % 5c.57
				\todo{Content}
			% end

			\subsubsection{Levenberg-Marquardt} % 5c.58
				\todo{Content}
			% end
		% end

		\subsection{Initialization of the Parameters} % 5c.59, 5c.60
			\todo{Content}
		% end

		\subsection{Overfitting} % 5c.61, 5c.62, 5c.63, 5c.64, 5c.65, 5c.66
			\todo{Content}

			\subsubsection{Weight Decay} % 5c.67
				\todo{Content}
			% end

			\subsubsection{Early Stopping} % 5c.68
				\todo{Content}
			% end

			\subsubsection{Input Noise Augmentation} % 6.69
				\todo{Content}
			% end

			\subsubsection{Dtopout} % 5c.70
				\todo{Content}
			% end

			\subsubsection{Batch Normalization} % 5c.71
				\todo{Content}
			% end
		% end

		\subsection{Theoretical Analysis} % 5c.72, 5c.73, 5c.75, 5c.76, 5c.77
			\todo{Content}

			\subsubsection{Universal Function Approximation Theorem} % 5c.74
				\todo{Content}
			% end
		% end

		\subsection{Network Architectures} % 5c.78, 5c.79, 5c.80
			\todo{Content}

			\subsubsection{Convolutional Neural Networks (CNNs)} % 5c.81, 5c.82, 5c.83, 5c.84, 5c.85, 5c.86
				\todo{Content}
			% end

			\subsubsection{Recurrent Neural Networks (RNNs)} % 5c.78
				\todo{Content}
			% end
		% end

		\subsection{Neural Networks in Robotics} % 5c.88, 5c.89
			\todo{Content}

			\subsubsection{Value Functions} % 5c.90, 5c.91
				\todo{Content}
			% end

			\subsubsection{Policies} % 5c.92, 5c.93
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 5b.138, 5c.95
		\todo{Content}
	% end
% end

\chapter{Optimal Control} % N/A
	\todo{Content}

	\section{Discrete State-Action Space} % 3a.1, 3a.2
		\todo{Content}

		\subsection{Foundations} % N/A
			\todo{Content}

			\subsubsection{Dynamic Programing and Principle of Optimality} % 3a.6, 3a.7
				\todo{Content}
			% end

			\subsubsection{Markov Decision Process and Policies} % 3a.9, 3a.10
				\todo{Content}
			% end
		% end

		\subsection{Finite-Horizon Optimal Control} % 3a.12, 3a.13
			\todo{Content}

			\subsubsection{Value and State-Action Value Functions} % 3a.14, 3a.15, 3a.17
				\todo{Content}
			% end

			\subsubsection{Value Iteration} % 3a.18, 3a.19
				\todo{Content}
			% end

			\subsubsection{Consequences of a Finite Time Horizon} % 3a.21
				\todo{Content}
			% end
		% end

		\subsection{Infinite-Horizon Optimal Control} % 3a.22, 3a.23, 3a.24, 3a.25
			\todo{Content}

			\subsubsection{Value and State-Action Value Functions} % 3a.30, 3a.31, 3a.32, 3a.33
				\todo{Content}
			% end

			\subsubsection{Value Iteration} % 3a.26
				\todo{Content}
			% end

			\subsubsection{Policy Iteration} % 3a.29, 3a.34, 3a.36, 3a.37
				\todo{Content}
			% end
		% end
	% end

	\section{Continuous State-Action Space} % 3b.1, 3b.2
		\todo{Content}

		\subsection{Modeling the System} % 3b.5, 3b.7, 3b.8, 3b.9, 3b.10, 3b.11
			\todo{Content}
		% end

		\subsection{Linear Quadratic Regulator (LQR)} % 3b.12, 3b.13, 3b.14, 3b.15
			\todo{Content}

			\subsubsection{Recap of Value Iteration} % 3b.17, 3b.18
				\todo{Content}
			% end

			\subsubsection{Solving the Optimal Control Problem} % 3b.19, 3b.20, 3b.21, 3b.22, 3b.23, 3b.24, 3b.25, 3b.26, 2.27
				\todo{Content}

				\paragraph{Computing the Expectation} % 3b.21
					\todo{Content}
				% end

				\paragraph{Computing the Maximization} % 3b.23
					\todo{Content}
				% end
			% end
		% end

		\subsection{Approximating Nonlinear Systems} % 3b.32, 3b.33, 3b.34, 3b.35, 3b.36
			\todo{Content}

			\subsubsection{Local Solutions by Linearization} % 3b.37, 3b.38, 3b.39
				\todo{Content}
			% end
		% end

		\subsection{Optimal Control with Learned Models} % 3b.42, 3b.43, 3b.44, 3b.45, 3b.46, 3b.48, 3b.51
			\todo{Content}

			\subsubsection{State-Of-The-Art Approaches} % 3b.52, 3b.55
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 3a.39, 3a.40, 3a.41, 3a.42, 3b.59
		\todo{Content}
	% end
% end

\chapter{Approximate Optimal Control} % N/A
	\todo{Content}

	\section{Differential Dynamic Programming (DDP)} % 4a.1, 4a.2, 4a.3, 4a.12
		\todo{Content}

		\subsection{Locally Nonlinear LQR} % 4a.13, 4a.14
			\todo{Content}
		% end

		\subsection{Differential Dynamic Programming} % 4a.15, 4a.16, 4a.17
			\todo{Content}

			\subsubsection{Implementation Details} % 4a.18, 4a.19, 4a.20
				\todo{Content}
			% end
		% end

		\subsection{Iterative LQR} % 4a.22, 4a.23, 4a.24
			\todo{Content}
		% end

		\subsection{Stochastic DDP} % 4a.25, 4a.26
			\todo{Content}
		% end

		\subsection{Guided Policy Search} % 4a.27, 4a.28
			\todo{Content}
		% end
	% end

	\section{Approximate Dynamic Programming} % 4b.1, 4b.2, 4b.6
		\todo{Content}

		\subsection{Function Approximation} % 4b.7, 4b.8, 4b.9, 4b.10, 4b.11, 4b.12
			\todo{Content}
		% end

		\subsection{Approximate Value Iteration} % 4b.13, 4b.14
			\todo{Content}

			\subsubsection{Theoretical Analysis and Bellman Operator} % 4b.15, 4b.16, 4b.17
				\todo{Content}
			% end

			\subsubsection{Approximation Error and Performance Loss} % 4b.18, 4b.19, 4b.20
				\todo{Content}
			% end
		% end

		\subsection{Approximate Policy Iteration} % 4b.21, 4b.23, 4b.24, 4b.25
			\todo{Content}

			\subsubsection{Approximation Error and Performance Loss} % 4b.26
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 4a.30, 4b.28
		\todo{Content}
	% end
% end

\chapter{State Estimation} % 6.1, 6.2, 6.5, 6.6
	\todo{Content}

	\section{Kalman Filter as an Optimal Filter} % 6.8
		\todo{Content}

		\subsection{Observers} % 6.9, 6.10
			\todo{Content}
		% end

		\subsection{Optimal Observers} % 6.11, 6.12, 6.13, 6.14, 6.15, 6.16, 6.17, 6.18
			\todo{Content}
		% end

		\subsection{Geometric Perspective} % 6.19
			\todo{Content}
		% end
	% end

	\section{Kalman Filter as Bayesian Inference} % 6.20, 6.21, 6.22, 6.23, 6.24
		\todo{Content}
	% end

	\section{Partially Observed Optimal Control} % 6.27, 6.28, 6.29, 6.30
		\todo{Content}
	% end

	\section{Extended, Unscented and Particle Filter} % 6.32, 6.33
		\todo{Content}

		\subsection{Extended Kalman Filter (EKF)} % 6.34
			\todo{Content}
		% end

		\subsection{Cubature Kalman Filter (CKF)} % 6.35, 6.36, 6.37, 6.38
			\todo{Content}
		% end

		\subsection{Unscented Kalman Filter (UKF)} % 6.39
			\todo{Content}
		% end

		\subsection{Particle Filter / Sequential Monte Carlo (PF/SMC)} % 6.40, 6.41, 6.56
			\todo{Content}

			\subsubsection{Importance Sampling} % 6.42, 6.43
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Sampling} % 6.48, 6.49, 6.50, 6.51, 6.52
				\todo{Content}
			% end

			\subsubsection{Sequential Importance Resampling} % 6.53, 6.54, 6.55
				\todo{Content}
			% end
		% end

		\subsection{Examples} % N/A
			\todo{Content}

			\subsubsection{Approximate Message Passing} % 6.44, 6.45, 6.46, 6.47
				\todo{Content}
			% end

			\subsubsection{Pendulum} % 6.57, 6.58, 6.59, 6.60
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 6.62, 6.63
		\todo{Content}
	% end
% end

\chapter{Model Learning} % 7.1, 7.2
	\todo{Content}

	\section{Models in Robotics} % 7.4, 7.5, 7.6, 7.7, 7.8, 7.9
		\todo{Content}
	% end

	\section{Modling Assumptions: White, Black and Gray} % 7.10, 7.11
		\todo{Content}

		\subsection{White-Box Strategy} % 7.12
			\todo{Content}
		% end

		\subsection{Black-Box Strategy} % 7.13
			\todo{Content}
		% end

		\subsection{Gray-Box Strategy} % 7.14
			\todo{Content}
		% end
	% end

	\section{System Identification and Signal Processing} % 7.15, 7.16, 7.17
		\todo{Content}

		\subsection{Impulse Response} % 7.18, 7.19, 7.20, 7.21, 7.22, 7.23
			\todo{Content}
		% end

		\subsection{Step Response} % 7.24, 7.25
			\todo{Content}
		% end

		\subsection{Characterization of Dynamical Systems} % 7.26, 7.27
			\todo{Content}
		% end

		\subsection{Frequency Analysis} % 7.28, 7.29
			\todo{Content}
		% end

		\subsection{Ornstein-Uhlenbeck Process} % 7.30, 7.31
			\todo{Content}
		% end

		\subsection{Active Learning} % 7.32, 7.33, 7.34
			\todo{Content}
		% end
	% end

	\section{Learning Models} % 7.36, 7.37
		\todo{Content}

		\subsection{Linear Gaussian Dynamical Systems (LGDS)} % 7.38, 7.39, 7.40, 7.41, 7.42, 7.43, 7.44
			\todo{Content}
		% end
	% end

	\section{Case Studies} % 7.47
		\todo{Content}

		\subsection{Combining Rigid Body Dynamics and GPs} % 7.48, 7.49, 7.50
			\todo{Content}
		% end

		\subsection{Deep Lagrangian Networks} % 7.51, 7.52, 7.53, 7.54, 7.55
			\todo{Content}
		% end

		\subsection{The Differentiable Recusive Newton-Euler Algorithm} % 7.56, 7.57, 7.58, 5.59, 7.60, 7.61, 7.62, 7.63
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 7.65
		\todo{Content}
	% end
% end

\chapter{Policy Representations} % 8.1, 8.4
	\todo{Content}

	\section{Parametric Policies} % 8.3, 8.7, 8.8, 8.9
		\todo{Content}
	% end

	\section{Off-The-Shelf Policies} % 8.10, 8.16
		\todo{Content}

		\subsection{Linear Basis Functions} % 8.11, 8.12, 8.13
			\todo{Content}
		% end

		\subsection{Radial Basis Functions (RBFs)} % 8.14, 8.15
			\todo{Content}
		% end
	% end

	\section{Movement Primitives} % 8.17, 8.18, 8.20, 8.21, 8.22, 8.23, 8.24, 8.25
		\todo{Content}

		\subsection{Dynamic Movement Primitives (DMDs)} % 8.26, 8.27, 8.28
			\todo{Content}

			\subsubsection{Temporal Scaling} % 8.29, 8.30, 8.34
				\todo{Content}
			% end

			\subsubsection{Multiple Degrees of Freedom} % 8.33
				\todo{Content}
			% end

			\subsubsection{Imitation Learning} % 8.35
				\todo{Content}
			% end
		% end

		\subsection{Probabilistic Movement Primitives (ProMPs)} % 8.40, 8.41, 8.42, 8.43, 8.44, 8.45
			\todo{Content}

			\subsubsection{Conditioning} % 8.46
				\todo{Content}
			% end

			\subsubsection{Combination} % 8.47
				\todo{Content}
			% end
		% end

		\subsection{Time-Independent Stable Movement Primitives} % 8.51
			\todo{Content}

			\subsubsection{Nonlinear Stable Dynamical Systems} % 8.52, 8.53, 8.54
				\todo{Content}
			% end
		% end

		\subsection{Imitation Flows} % 8.55, 8.56, 8.57
			\todo{Content}
		% end

		\subsection{Libraries of Primitives} % 8.59, 8.60, 8.61
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 8.64
		\todo{Content}
	% end
% end

\chapter{Model-Based Reinforcement Learning} % 9.1, 9.2, 9.5, 9.6
	\todo{Content}

	\section{Differentiation of Model-Based and Model-Free} % 9.7, 9.8
		\todo{Content}

		\subsection{Sample Efficiency} % 9.9, 9.10, 9.11
			\todo{Content}
		% end
	% end

	\section{Domain Knowledge in Reinforcement Learning} % 9.13, 9.14
		\todo{Content}

		\subsection{Performance Bias} % 9.15, 9.16, 9.17, 9.22, 9.25
			\todo{Content}
		% end

		\subsection{Local Optima and Sample-Based Methods} % 9.18, 9.21
			\todo{Content}

			\subsubsection{The Cross-Entropy Method (CEM)} % 9.19
				\todo{Content}
			% end

			\subsubsection{(Model Predictive) Path Integral Control (MPPI)} % 9.20
				\todo{Content}
			% end
		% end

		\subsection{Numerical Sensitivity} % 9.23
			\todo{Content}

			\subsubsection{Backprop-Through-Time} % 9.24
				\todo{Content}
			% end
		% end

		\subsection{Catastrophic Model Errors} % 9.26
			\todo{Content}
		% end
	% end

	\section{Optimism and Pessimism in Reinforcement Learning} % 9.27, 9.28, 9.29
		\todo{Content}

		\subsection{Aleatoric and Epistemic Uncertainty} % 9.30, 9.31
			\todo{Content}
		% end

		\subsection{Optimism Under Uncertainty} % 9.31, 9.32, 9.33, 9.34, 9.35, 9.39
			\todo{Content}

			\subsubsection{Neural Linear Models} % 9.36
				\todo{Content}
			% end

			\subsubsection{Variational Inference} % 9.37
				\todo{Content}
			% end

			\subsubsection{Ensembles} % 9.38
				\todo{Content}
			% end
		% end
	% end

	\section{Replanning} % 9.40, 9.41, 9.42
		\todo{Content}
	% end

	\section{Case Studies} % 9.43
		\todo{Content}

		\subsection{Probabilistic Learning for Control (PILCO)} % 9.44, 9.45, 9.46
			\todo{Content}

		\subsection{Guided Policy Search (GPS)} % 9.48, 9.49, 9.50, 9.51, 9.52
			\todo{Content}
		% end

		\subsection{Model Predictive Control (MPC)} % 9.54, 9.56
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 9.58, 12.63
		\todo{Content}
	% end
% end

\chapter{Value Function Methods} % 11.1, 11.2
	\todo{Content}

	\section{Recap of Dynamic Programming} % 11.4, 11.5, 11.6, 11.7
		\todo{Content}
	% end

	\section{Temporal Differences} % 11.8, 11.9, 11.10
		\todo{Content}

		\subsection{Policy Evaluation} % 11.11, 11.12, 11.13
			\todo{Content}
		% end

		\subsection{Policy Improvement} % 11.14, 11.15
			\todo{Content}
		% end
	% end

	\section{Value Function Approximation} % 11.16, 11.17, 11.18, 11.21, 11.22, 11.23
		\todo{Content}
	% end

	\section{Batch Reinforcement Learning Methods} % 11.25, 11.26
		\todo{Content}

		\subsection{Least-Squares Temporal Differences (LSTD)} % 11.27, 11.28, 11.29, 11.30
			\todo{Content}
		% end

		\subsection{Fitted Q-Iteration} % 11.31, 11.32, 11.33
			\todo{Content}
		% end
	% end

	\section{Case Study: Robot Soccer} % 11.34, 11.35, 11.36, 11.37, 11.38, 11.39
		\todo{Content}
	% end

	\section{Wrap-Up} % 11.41, 12.64
		\todo{Content}
	% end
% end

\chapter{Policy Search} % N/A
	\todo{Content}

	\section{Categorization of Policy Search} % 10.6, 10.7, 10.8
		\todo{Content}

		\subsection{Episode- vs. Step-Based Evaluation} % 10.9, 10.10, 10.11
			\todo{Content}
		% end

		\subsection{Epsiode-Based Policy Search} % 10.12, 10.13, 10.14
			\todo{Content}
		% end

		\subsection{Exploration vs. Exploitation} % 10.15, 10.16, 10.17, 10.18, 10.19
			\todo{Content}
		% end
	% end

	\section{Policy Gradient Methods} % 10.1, 10.2, 10.5
		\todo{Content}

		\subsection{Policy Gradients} % 10.20, 10.21
			\todo{Content}

			\subsubsection{Gradient Computation} % N/A
				\todo{Content}

				\paragraph{Finite Differences} % 10.22
					\todo{Content}
				% end

				\paragraph{Likelihood Policy Gradients} % 10.23
					\todo{Content}
				% end

				\paragraph{Baselines} % 10.24
					\todo{Content}
				% end
			% end

			\subsubsection{Step-Based Policy Gradient Methods} % 10.25, 10.26
				\todo{Content}

				\paragraph{Likelihood Ratio Gradient} % 10.27, 10.28
					\todo{Content}
				% end

				\paragraph{Using the Rewards to Come} % 10.29
					\todo{Content}
				% end
			% end

			\subsubsection{Choosing the Step Size, Metrics in Standard Gradients} % 10.30, 10.31, 10.32, 10.33
				\todo{Content}
			% end
		% end

		\subsection{Relative Entropy and Natural Gradient} % 10.34, 10.35, 10.41, 10.42, 10.43
			\todo{Content}

			\subsubsection{Computing the Natural Gradient} % N/A
				\todo{Content}

				\paragraph{Episode-Based} % 10.44
					\todo{Content}
				% end

				\paragraph{Step-Based} % 10.45
					\todo{Content}
				% end
			% end

			\subsubsection{Compatible Function Approximation} % 10.46, 10.47, 10.48
				\todo{Content}

				\paragraph{Connection to Value Function Approximation} % 10.49, 10.50, 10.51
					\todo{Content}
				% end

				\paragraph{Episodic Natural Actor-Critic} % 10.52, 10.53, 10.54
					\todo{Content}
				% end
			% end
		% end
	% end

	\section{Probabilistic Policy Search} % 12.1, 12.2
		\todo{Content}

		\subsection{Success Matching Principle} % 12.5, 12.6, 12.7
			\todo{Content}
		% end

		\subsection{Weighted Maximum Likelihood} % 12.8, 12.9, 12.10, 12.11
			\todo{Content}

			\subsubsection{Difference to Policy Gradients} % 12.12
				\todo{Content}
			% end

			\subsubsection{Computing the Weights} % 12.13
				\todo{Content}

				\paragraph{Notes on Expectation Maximization} % 12.14
					\todo{Content}
				% end

				\paragraph{Notes on the Exponential Transformation} % 12.15
					\todo{Content}
				% end
			% end

			\subsubsection{Illustration and Results} % 12.16, 12.17, 12.19, 12.21
				\todo{Content}
			% end
		% end

		\subsection{Relative Entropy Policy Search (REPS)} % 12.22, 12.23
			\todo{Content}

			\subsubsection{Optimization Problem} % 12.24
				\todo{Content}

				\paragraph{Solving} % 12.25, 12.26
					\todo{Content}
				% end
			% end
		% end

		\subsection{REPS for Contextual Policy Search} % 12.28, 12.29, 12.30
			\todo{Content}

			\subsubsection{Optimization Problem} % 12.31
				\todo{Content}

				\paragraph{Solving} % 12.32, 12.33, 12.34, 12.35
					\todo{Content}
				% end
			% end

			\subsubsection{Contextual Policies with Weighted ML} % 12.36
				\todo{Content}
			% end
		% end

		\subsection{Learning Versatile Solutions} % 12.40, 12.41, 12.47
			\todo{Content}

			\subsubsection{Illustration} % 12.42, 12.43
				\todo{Content}
			% end

			\subsubsection{Hierarchy} % 12.44
				\todo{Content}

				\paragraph{Naive Hierarichal Approach} % 12.45, 12.46
					\todo{Content}
				% end

				\paragraph{Hierarichal REPS (HiREPS)} % 12.48, 12.49
					\todo{Content}
				% end
			% end
		% end

		\subsection{Sequencing Movement Primitives} % 12.54, 12.55, 12.56
			\todo{Content}

			\subsubsection{Sequential REPS} % 12.57, 12.65, 12.66
				\todo{Content}
			% end
		% end
	% end

	\section{Wrap-Up} % 10.55, 10.56, 10.58, 12.60
		\todo{Content}
	% end
% end

\chapter{Imitation Learning: Behavioral Cloning and Inverse RL} % 13.1, 13.3, 13.4, 13.5, 13.8, 13.9, 13.10, 13.11, 13.12
	\todo{Content}

	\section{Distribution Matching} % 13.13
		\todo{Content}

		\subsection{Behavioral Cloning} % 13.14, 13.43
			\todo{Content}

			\subsubsection{Direct Behavioral Cloning} % 13.15, 13.16, 13.17, 13.18, 13.19, 13.20
				\todo{Content}
			% end

			\subsubsection{DAGGER: New Samples to Learn to Recover} % 13.21
				\todo{Content}
			% end

			\subsubsection{DART: Robustness in Imitation Learning} % 13.22
				\todo{Content}
			% end
		% end

		\subsection{Generative Adversarial Learning} % 13.23, 13.24, 13.25
			\todo{Content}
		% end
	% end

	\section{Inverse Reinforcement Learning} % 13.26, 13.27, 13.42, 13.44
		\todo{Content}

		\subsection{Basic Principle} % 13.46, 13.49, 13.55
			\todo{Content}

			\subsubsection{Feature-Based Reward Function} % 13.47, 13.48
				\todo{Content}
			% end

			\subsubsection{Constraint Generation} % 13.50, 13.51, 13.58
				\todo{Content}
			% end

			\subsubsection{Ill-Posed Problem} % 13.52, 13.54
				\todo{Content}
			% end

			\subsubsection{(Structured) Max. Margin Solution} % 13.53, 13.56
				\todo{Content}
			% end

			\subsubsection{Expert Suboptimality} % 13.57, 13.60
				\todo{Content}
			% end
		% end

		\subsection{Feature Matching by Max. Entropy} % 13.61
			\todo{Content}

			\subsubsection{Max. Entropy Approach to Inference} % 13.62
				\todo{Content}
			% end

			\subsubsection{Max. Entropy Approach to Inverse RL} % 13.63
				\todo{Content}
			% end

			\subsubsection{Maximum-Casual-Entropy Inverse RL} % 13.64, 13.65
				\todo{Content}
			% end
		% end

		\subsection{Reward-Parameterized Policies} % 13.66, 13.67
			\todo{Content}
		% end
	% end

	\section{Case Studies} % 13.68, 13.69
		\todo{Content}

		\subsection{Highway Driving} % 13.70
			\todo{Content}
		% end

		\subsection{Max. Margin} % 13.71, 13.72
			\todo{Content}
		% end

		\subsection{Parking Lot Navigation} % 13.73, 13.74, 13.75, 13.76, 13.77
			\todo{Content}
		% end

		\subsection{Human Path Planning} % 13.78, 13.79, 13.80, 13.81, 13.82, 13.83
			\todo{Content}
		% end

		\subsection{Goal Inference} % 13.84, 13.85
			\todo{Content}
		% end

		\subsection{Quadruped} % 13.86, 13.87, 13.88
			\todo{Content}
		% end

		\subsection{Extreme Helicopter Flight} % 13.89, 13.90, 13.91, 13.92
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 13.45, 13.94
		\todo{Content}
	% end
% end

\chapter{Bayesian Reinforcement Learning} % 14.1, 14.2, 14.15, 15.16, 14.17, 14.18
	\todo{Content}

	\section{Recap of Bayesian Methods} % 14.4, 14.5, 14.6, 14.7, 14.8, 14.9, 14.14
		\todo{Content}

		\subsection{Conjugate Prior} % 14.10, 14.11, 14.12, 14.13
			\todo{Content}
		% end
	% end

	\section{Bandits and Thompson Sampling} % 14.19, 14.20, 14.21
		\todo{Content}

		\subsection{Restaurant Selection} % 14.22, 14.23, 14.24, 14.25, 14.26, 14.27, 14.28, 14.29
			\todo{Content}
		% end
	% end

	\section{Model-Based Bayesian RL for Discrete MDPs} % 14.30, 14.31, 14.32, 14.33
		\todo{Content}

		\subsection{Belief} % 14.34, 14.35
			\todo{Content}
		% end

		\subsection{State Transition Model} % 14.36, 14.37
			\todo{Content}
		% end

		\subsection{Optimal Value Function for BAMPDs} % 14.38, 14.39
			\todo{Content}
		% end

		\subsection{Solving for the Optimal Value Function} % 14.40
			\todo{Content}
		% end
	% end

	\section{Continuous MDPs and Dual Control} % 14.41, 14.42, 14.43, 14.44
		\todo{Content}

		\subsection{One-Dimensional Linear Gaussian Dual Control} % 14.45, 14.46, 14.47, 14.48, 14.49, 14.50
			\todo{Content}
		% end

		\subsection{Practical Dual Control} % 14.51
			\todo{Content}
		% end
	% end

	\section{Wrap-Up} % 14.53
		\todo{Content}
	% end
% end

\chapter{Outlook} % 15.1
	\todo{Content}

	\section{Recap} % 15.4, 15.5, 15.6, 15.7, 15.8, 15.9, 15.10, 15.11
		\todo{Content}
	% end

	\section{Open Research Question} % 15.13
		\todo{Content}
	% end
% end
