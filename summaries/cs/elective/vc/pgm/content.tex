\chapter{Introduction} % 1.1, 1.2, 1.28, 1.29, 1.30, 1.38
    \todo{Content}

    \section{Examples} % 1.3, ..., 1.27
        \todo{Content}
    % end

    \section{Fundamental Questions} % 1.36, 1.37
        \todo{Content}
    % end
% end

\chapter{Foundations} % N/A
    \todo{Content}

    \section{Probability Theory} % 1.40
        \todo{Content}

        \subsection{(Conditional) Independence} % 1.57, 1.58, 1.59, 1.60
            \todo{Content}

            \subsubsection{Monty Hall Problem} % 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67
                \todo{Content}
            % end
        % end

        \subsection{Inference} % 1.70, 1.71, 1.72
            \todo{Content}

            \subsubsection{Information Theory} % 1.73, 1.74, 1.75
                \todo{Content}
            % end
        % end

        \subsection{Potentials} % 4.19, 4.20, 4.21, 4.22, 4.23
            \todo{Content}
        % end
    % end

    \section{Machine Learning} % N/A
        \todo{Content}

        \subsection{(Document) Classification} % 2.11, 2.12, 2.13, 2.14, 2.15, 2.16
            \todo{Content}
        % end
    % end
% end

\chapter{Bayesian Networks} % 1.79, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.31, 2.32
    \todo{Content}

    \section{The Naive Bayes Model} % 2.8, 2.9
        \todo{Content}

        \subsection{Classification} % 2.20, 2.21, 2.22, 2.27
            \todo{Content}
        % end

        \subsection{Maximum Likelihood Parameter Estimation} % 2.23, 2.24, 2.25, 2.26
            \todo{Content}
        % end

        \subsection{Application} % 2.28, 2.29
            \todo{Content}
        % end
    % end

    \section{Definition and Independence Assumptions} % 2.36, 2.37, 2.38, 2.39, 2.40, 2.46, 2.58
        \todo{Content}

        \subsection{Local Markov Assumption} % 2.41, 2.42, 2.45
            \todo{Content}
        % end

        \subsection{"Explaining Away" / Berkson's Paradox} % 2.43, 2.44
            \todo{Content}
        % end

        \subsection{Representation Theorem} % 2.47, 2.48, 2.49, 2.50, 2.51, 2.52, 2.53, 2.54
            \todo{Content}
        % end

        \subsection{Building a Bayesian Network} % 2.55, 2.56, 2.57
            \todo{Content}
        % end
    % end

    \section{Encoded Independencies} % 2.59, 3.2, 3.3, 3.28, 3.29
        \todo{Content}

        \subsection{Dependency Structures} % 3.4
            \todo{Content}
        % end

        \subsection{d-Separation} % N/A
            \todo{Content}

            \subsubsection{(Active) Trails} % 3.5, 3.6
                \todo{Content}
            % end

            \subsubsection{Independencies} % 3.9
                \todo{Content}
            % end

            \subsubsection{Soundness} % 3.15, 3.16
                \todo{Content}
            % end

            \subsubsection{Completeness} % 3.18, 3.20
                \todo{Content}
            % end
        % end

        \subsection{Faithful Distributions} % 3.17
            \todo{Content}
        % end

        \subsection{Context-Specific Independence (CSI)} % 3.21, 3.22, 3.23, 3.27
            \todo{Content}

            \subsubsection{Tree CPD} % 3.24
                \todo{Content}
            % end

            \subsubsection{Determinism} % 3.25, 3.26
                \todo{Content}
            % end
        % end

        \subsection{The Bayes' Ball Algorithm} % 3.19
            \todo{Content}
        % end
    % end
% end

\chapter{Inference} % 3.30, 3.31, 3.32, 3.33
    \todo{Content}

    \section{Chain Models} % 3.34, 3.35, 3.36, 3.37
        \todo{Content}
    % end

    \section{Variable Elimination} % 3.38, 3.39, 3.40, 3.41, 3.42, 3.43, 3.44, 3.45, 3.46, 3.54, 3.57
        \todo{Content}

        \subsection{Evidence} % 3.47, 3.48, 3.49, 3.50, 3.51, 3.52, 3.53
            \todo{Content}
        % end

        \subsection{Complexity} % 3.55, 3.56
            \todo{Content}
        % end

        \subsection{VE for Potentials} % 4.24, 4.25
            \todo{Content}
        % end
    % end

    \section{Abductive Inference} % 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.12, 4.17, 4.58
        \todo{Content}

        \subsection{Consistency} % 4.8
            \todo{Content}
        % end

        \subsection{Finding Most Probable Explanations (MPEs)} % 4.9, 4.10, 4.11
            \todo{Content}
        % end
    % end

    \section{Complexity of Conditional Queries} % 4.13, 4.14, 4.15, 4.16, 4.17
        \todo{Content}
    % end

    \section{Moralizing} % 4.26, 4.27, 4.28, 4.29, 4.30, 4.31, 4.32
        \todo{Content}
    % end

    \section{Variable Elimination in Moral Graphs} % 4.18, 4.33, 4.34, 4.35, 4.59
        \todo{Content}

        \subsection{Perfect Elimination Sequences} % 4.36, 4.37, 4.38, 4.39, 4.40, 4.41, 4.42
            \todo{Content}
        % end

        \subsection{Complexity} % 4.43, 4.44, 4.45
            \todo{Content}
        % end

        \subsection{Induced Graph} % 4.46, 4.47, 4.48
            \todo{Content}
        % end

        \subsection{Induced Treewidth} % 4.49
            \todo{Content}
        % end

        \subsection{Elimination on Trees} % 4.50, 4.51, 4.52
            \todo{Content}

            \subsubsection{Polytrees} % 4.53
                \todo{Content}
            % end
        % end

        \subsection{General Networks} % 4.54, 4.55, 4.56, 4.57
            \todo{Content}
        % end
    % end
% end

\chapter{Markov Random Fields} % 4.60, 4.61, 4.62, 4.63, 4.64, 4.99
    \todo{Content}

    \section{Bayesian Networks as MRFs} % 4.65, 4.66, 4.67, 4.68, 4.69, 4.70
        \todo{Content}
    % end

    \section{Triangulated Graphs} % 4.71, 4.72, 4.73, 4.74, 4.75
        \todo{Content}
    % end

    \section{Join Trees} % 4.76, 4.77, 4.78, 4.79, 4.80, 4.81, 4.82, 4.83, 4.84, 4.85, 4.86
        \todo{Content}
    % end

    \section{Junction Trees} % 4.87, 4.88, 4.89, 4.90
        \todo{Content}

        \subsection{Collecting Evidence} % 4.91, 4.92, 4.93, 4.94, 4.95, 4.96
            \todo{Content}
        % end

        \subsection{Distributing Evidence} % 4.97
            \todo{Content}
        % end
    % end

    \section{Non-Triangulated Graphs} % 4.98
        \todo{Content}
    % end
% end

\chapter{Learning} % 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.22, 5.28, 5.38, 6.58
    \todo{Content}

    \section{Complete and Incomplete Data Sets} % 5.9, 5.10, 5.11, 5.12
        \todo{Content}

        \subsection{Hidden Variables} % 5.12, 5.13, 5.14, 5.15, 5.16, 5.17, 5.18, 5.19, 5.20, 5.21
            \todo{Content}
        % end
    % end

    \section{Parameter Estimation} % 5.23, 5.55, 6.3
        \todo{Content}

        \subsection{Known Structure, Complete Data} % 5.29
            \todo{Content}

            \subsubsection{Maximum Likelihood} % 5.24, 5.25, 5.26, 5.27, 5.30
                \todo{Content}
            % end

            \subsubsection{Decomposability of the Likelihood} % 5.31, 5.32
                \todo{Content}
            % end

            \subsubsection{Likelihood for (Conditional) Bi- and Multinomials} % 5.33, 5.34, 5.35, 5.36, 5.37
                \todo{Content}
            % end
        % end

        \subsection{Known Structure, Incomplete Data (Expectation-Maximization)} % 5.39, 5.44, 5.48, 5.49, 5.53, 6.1
            \todo{Content}

            \subsubsection{EM Idea} % 5.40, 5.41, 5.42, 5.45
                \todo{Content}
            % end

            \subsubsection{Complete-Data Likelihood} % 5.43
                \todo{Content}
            % end

            \subsubsection{EM for (Conditional) Multinomials} % 5.46, 5.47
                \todo{Content}
            % end

            \subsubsection{Monotonicity} % 5.50
                \todo{Content}
            % end
        % end

        \subsection{Gradient Ascent} % 5.54
            \todo{Content}
        % end

        \subsection{Bayesian Parameter Estimation} % 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 6.10
            \todo{Content}

            \subsubsection{Laplace Estimation} % 6.11, 6.12, 6.13, 6.16, 6.17
                \todo{Content}
            % end

            \subsubsection{Bayesian Prediction} % 6.18, 6.19, 6.24
                \todo{Content}
            % end

            \subsubsection{Conjugate Priors} % 6.20, 6.21, 6.25
                \todo{Content}

                \paragraph{Binomial Prior} % 6.22, 6.23
                    \todo{Content}
                % end

                \paragraph{Dirichlet Prior} % 6.26, 6.27, 6.28, 6.29, 6.30
                    \todo{Content}
                % end
            % end

            \subsubsection{Bayesian Networks and Bayesian Prediction} % 6.31, 6.32
                \todo{Content}
            % end
        % end

        \subsection{Summary} % 6.33, 6.34, 6.35
            \todo{Content}
        % end
    % end

    \section{Structure Learning / Model Selection} % 6.36, 6.37, 6.38, 6.39, 6.40, 6.57
        \todo{Content}

        \subsection{Minimal I-Maps} % 6.41, 6.42, 6.43
            \todo{Content}
        % end

        \subsection{Perfect Maps (P-Maps)} % 6.44, 6.45
            \todo{Content}
        % end

        \subsection{I-Equivalence} % 6.46, 6.47
            \todo{Content}

            \subsubsection{Skeleton and Immoralities} % 6.48, 6.49, 6.50, 6.51
                \todo{Content}
            % end
        % end

        \subsection{Obtaining a P-Map} % 6.52
            \todo{Content}

            \subsubsection{Identifying the Skeleton} % 6.53, 6.54
                \todo{Content}
            % end

            \subsubsection{Identifying Immoralities} % 6.55
                \todo{Content}
            % end

            \subsubsection{From Immoralities to Structures} % 6.56
                \todo{Content}
            % end
        % end

        \subsection{Accurate Structures} % 6.59
            \todo{Content}
        % end

        \subsection{Learning} % 6.60, 6.61
            \todo{Content}

            \subsubsection{Constrained-Based} % 6.62, 6.63
                \todo{Content}
            % end

            \subsubsection{Score-Based} % 6.64
                \todo{Content}

                \paragraph{Likelihood Score} % 6.65
                    \todo{Content}
                % end

                \paragraph{Bayesian Score and Bayesian Information Criterion} % 6.66, 6.67, 6.68, 6.69, 6.70
                    \todo{Content}
                % end
            % end
        % end

        \subsection{Structure Search as Optimization} % 6.71
            \todo{Content}

            \subsubsection{Learning Trees (Complete Data)} % 6.72, 6.73
                \todo{Content}
            % end

            \subsubsection{Heuristic (Local) Search} % 6.74, 6.75, 6.76, 6.77, 6.78, 6.79, 6.80, 6.81, 6.82, 6.83
                \todo{Content}
            % end
        % end

        \subsection{Structural EM} % 6.84, 6.85, 6.86, 6.87, 6.88
            \todo{Content}
        % end

        \subsection{Summary} % 6.89
            \todo{Content}
        % end
    % end
% end

\chapter{Dynamic Bayesian Networks} % 7.1, 7.2, 7.3, 7.48
    \todo{Content}

    \section{Hidden Markov Models} % 7.4, 7.14, 7.15, 7.16
        \todo{Content}
    % end

    \section{Inference} % 7.17
        \todo{Content}

        \subsection{Decoding} % 7.18, 7.19, 7.20, 7.21, 7.22, 7.36
            \todo{Content}

            \subsubsection{Forward Pass} % 7.23, 7.24, 7.25, 7.26, 7.27, 7.28, 7.29, 7.30, 7.31, 7.32
                \todo{Content}
            % end

            \subsubsection{Backward Pass} % 7.33, 7.34, 7.35
                \todo{Content}
            % end
        % end

        \subsection{Best State Sequence} % 7.37, 7.38, 7.39
            \todo{Content}

            \subsubsection{Viterbi Algorithm} % 7.40, 7.41, 7.42
                \todo{Content}
            % end
        % end

        \subsection{Parameter Estimation} % 7.43, 7.44, 7.45, 7.46, 7.47
            \todo{Content}
        % end
    % end

    \section{State Estimation (Kalman Filter)} % 7.49, 7.50, 7.51, 7.52
        \todo{Content}

        \subsection{Recursive Bayesian Updating} % 7.53, 7.54
            \todo{Content}
        % end

        \subsection{(Modeling) Actions} % 7.55, 7.56, 7.57, 7.58
            \todo{Content}
        % end

        \subsection{Bayes Filter} % 7.62, 7.63, 7.64, 7.65, 7.66
            \todo{Content}
        % end

        \subsection{Discrete-Time Kalman Filter} % 7.69, 7.70, 7.71
            \todo{Content}

            \subsubsection{Dynamics and Observations} % 7.72, 7.74
                \todo{Content}
            % end

            \subsubsection{Belief Update: Prediction} % 7.73
                \todo{Content}
            % end

            \subsubsection{Belief Update: Correction} % 7.75
                \todo{Content}
            % end
        % end
    % end

    \section{General Dynamic Bayesian Networks} % 7.77, 7.78, 7.79
        \todo{Content}

        \subsection{Exact Inference} % 7.80, 7.81
            \todo{Content}
        % end

        \subsection{Tractable, Approximate Inference} % 7.82
            \todo{Content}

            \subsubsection{Assumed Density Filtering} % 7.83, 7.84, 7.85
                \todo{Content}
            % end
        % end
    % end
% end

\chapter{Approximate Inference} % 8.1, 8.2
    \todo{Content}

    \section{Message Passing} % 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 8.10, 8.11
        \todo{Content}

        \subsection{Sum-Product Belief Propagation} % 8.12, 8.13, 8.14, 8.15, 8.16, 8.17, 8.18, 8.19, 8.20, 8.21
            \todo{Content}
        % end

        \subsection{(Acyclic) Belief Propagation as Dynamic Programming} % 8.22, 8.23, 8.24, 8.25, 8.26, 8.27, 8.28, 8.29, 8.30, 8.31, 8.32, 8.33, 8.34, 8.35, 8.36
            \todo{Content}
        % end

        \subsection{Loopy Belief Propagation} % 8.37, 8.38, 8.39, 8.40, 8.41
            \todo{Content}
        % end
    % end

    \section{Sampling} % 8.42, 8.43, 8.44, 8.45, 8.46, 8.47, 8.48
        \todo{Content}

        \subsection{Forward Sampling (Without Evidence)} % 8.49, 8.50, 8.51, 8.52
            \todo{Content}
        % end

        \subsection{Forward Sampling (With Evidence)} % 8.53, 8.54, 8.55, 8.56, 8.57
            \todo{Content}
        % end

        \subsection{Gibbs Sampling} % 8.58, 8.59, 8.60, 8.61, 8.62, 8.63, 8.64, 8.65, 8.66, 8.67, 8.68
            \todo{Content}

            \subsubsection{Burn-In} % 8.69
                \todo{Content}
            % end

            \subsubsection{Irreducibility, Aperiodicity, and Ergodicity} % 8.71, 8.72, 8.73
                \todo{Content}
            % end

            \subsubsection{Convergence} % 8.70, 8.74, 8.75
                \todo{Content}
            % end

            \subsubsection{Performance} % 8.77
                \todo{Content}
            % end

            \subsubsection{Speeding Convergence} % 8.77
                \todo{Content}

                \paragraph{Skipping Samples} % 8.78
                    \todo{Content}
                % end

                \paragraph{Randomized Variable Order} % 8.79
                    \todo{Content}
                % end

                \paragraph{Blocking} % 8.80
                    \todo{Content}
                % end

                \paragraph{Rao-Blackwellization} % 8.81, 8.82
                    \todo{Content}
                % end

                \paragraph{Multiple Chains} % 8.83
                    \todo{Content}
                % end
            % end
        % end

        \subsection{Likelihood Weighting} % 8.84, 8.85, 8.86, 8.87, 8.88
            \todo{Content}
        % end
    % end
% end

\chapter{Tractable Probabilistic Models} % 9.1
    \todo{Content}

    \section{Deep Learning} % 9.2, 9.3, 9.4
        \todo{Content}
    % end

    \section{Probabilistic Circuits} % 9.5, 9.6, 9.7, 9.8, 9.9, 9.10
        \todo{Content}
    % end

    \section{Sum-Product Networks} % 9.11, 9.12, 9.58, 9.59
        \todo{Content}

        \subsection{Inference} % 9.13, 9.14, 9.15, 9.16
            \todo{Content}
        % end

        \subsection{Learning} % 9.16, 9.17
            \todo{Content}

            \subsubsection{Directly Learning SPNs} % 9.24, 9.25, 9.26, 9.27
                \todo{Content}
            % end
        % end

        \subsection{Inference on Devices} % 9.29
            \todo{Content}
        % end
    % end
% end

\chapter{Deep Generative Models} % 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7, 10.8
    \todo{Content}

    \section{Likelihood-Based} % 10.9
        \todo{Content}

        \subsection{Autoregressive Generatie Models} % 10.10
            \todo{Content}

            \subsubsection{Learning and Inference} % 10.11
                \todo{Content}
            % end

            \subsubsection{Parametrization} % 10.12, 10.13, 10.14
                \todo{Content}
            % end
        % end

        \subsection{Variational Auto-Encoders} % 10.15, 10.16, 10.17
            \todo{Content}

            \subsubsection{Inference as Optimization} % 10.18, 10.19
                \todo{Content}
            % end

            \subsubsection{Variational Bayes} % 10.20, 10.21
                \todo{Content}
            % end

            \subsubsection{Learning and Inference} % 10.22
                \todo{Content}
            % end

            \subsubsection{Open Questions} % 10.24
                \todo{Content}
            % end
        % end

        \subsection{Normalizing Flows} % 10.25, 10.26, 10.27, 10.28, 10.29
            \todo{Content}

            \subsubsection{Learning and Inference} % 10.30
                \todo{Content}
            % end
        % end
    % end

    \section{Likelihood-Free} % 10.32
        \todo{Content}

        \subsection{Generative Adversarial Networks} % 10.33, 10.34, 10.35
            \todo{Content}

            \subsubsection{Inference} % 10.36
                \todo{Content}
            % end
        % end
    % end

    \section{Applications in Scientific Discovery} % 10.39, 10.40, 10.41, 10.42, 10.43
        \todo{Content}
    % end
% end
