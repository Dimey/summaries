% !TeX spellcheck = en_US


\chapter{Introduction}
	This summary of the course "Deep Learning: Architectures and Methods" held at the TU Darmstadt covers a lot of topics in the wast field of deep learning and neural network architectures. The first part focuses on fully connected and convolutional networks and in the end some more advanced architectures are touched, namely recurrent, long short-term memory and transformer networks. Knowledge of basic machine learning taxonomy like "features", "training data" and "supervised learning" is assumed to be known as well as basic mathematical knowledge.

	The goal of deep architectures is learning a feature hierarchy where higher-level features are built on top of lower-level features. This is done by stacking multiple linear layers (perceptrons) on top of each other with nonlinearities between them to get a larger support. While it is, due to the universal function approximation theorem, not necessary to have multiple layers (one hidden layer is enough to approximate any sufficiently well-behaving function), it has been shown empirically that multiple layers improve the performance. In the last century the extreme computational power available leveraged the success of deep learning. This success was so extraordinary that deep learning became a hype that everyone jumped onto.

	This first chapter covers some milestones and history as well as the reason for the extreme success of deep learning.

	\section{Milestones}
		The first successful application of deep learning was in 1989 with LeNet to recognize zip codes. Image detection and classification was develop a lot further since that and in 2012, AlexNet broke the human prediction error on ImageNet, a collection of (then) \SI{200}{GB} labeled images. But deep learning is not only useful for image classification: In 2013, DeepMind beat the best human players on basic Arcade games and in 2016 AlphaGo defeated the world champion in the game Go. All of this was possible due to the massive power of deep neural networks and reinforcement learning.
	% end

	\section{The (Surprising?) Success of Deep Neural Networks}
		It is interesting that deep neural networks are so successful as they are built from extremely simple blocks. Also, the algorithms used for training these networks are extremely basic gradient descent descendants that only find local minima. In other words: they are greedy algorithms which are even more limited in what they can do than the network itself (if a problem has a greedy solution, it is usually regarded as an "easy" problem). But still they are extremely powerful!

		This is partially due to that hierarchical representations as induced by neural networks, are ubiquitous in artificial intelligence and information representation in general (e.g., images are hierarchical as well as natural language). Also, it seems like most learning problem are actually (relatively) easy and they have a gradient descent path towards a good model.
	% end
% end

\chapter{Optimization}
	\label{c:optimization}

	This chapter covers basic (numerical) optimization techniques that are used in machine and deep learning. Hence, it mainly focuses in gradient descent variants and less on, for example, sequential quadratic programming or trust region methods. A first method when thinking of optimization is a random search: instead of using sophisticated update rules for the parameters, they are sampled randomly. Obviously, this does not yield robust results and most often does not even yield any useful results. One advantage nevertheless is that random search does not get stuck in local minima and is, with infinite time, capable of finding the global optimum.

	\section{Gradient Descent}
		The basic idea of gradient descent (GD) is to follow the slope of the curve, i.e. the gradient\footnote{In this case the gradient refers to the vector of partial derivatives of a function w.r.t. all parameters.}. The gradient always points in the direction of the steepest \emph{ascent}, so its negative direction points towards the steepest descent. Intuitively, GD works like standing on a hill (the function that is being optimized) and always walking into the direction where the slope is the steepest. One might find the valley that way, but it is also possible to get stuck in a small hole. Formally the update equation for GD is given as
		\begin{equation}
			\vec{\theta}^{(k + 1)} = \vec{\theta}^{(k)} - \alpha \cdot \grad_{\vec{\theta}} L\bigl( \vec{\theta}^{(k)} \bigr),
		\end{equation}
		where \(\vec{\theta}\) are the parameters and \(L\) is the function to be minimized.

		\subsection{Evaluating the Gradient}
			As GD needs the values of the gradient, it has to be evaluated. There are three ways of approaching this: numerical evaluation, analytical derivatives or automatic differentiation.

			Numerical evaluation is extraordinarily simple, but is only approximate and slow to evaluate (when using forward difference quotients, the function has to be evaluated two times for each parameter).

			Analytical derivatives is hard and time-consuming, but exact and fast. But it is also error-prone as both the original function and the derivative have to be both implemented and derived. To verify that an analytical gradient is correct one method used in practice is to also evaluate the gradient numerically and checking if the results are roughly equal.

			As neural networks usually have lots of parameters and are fairly complex computational structures, neither of these methods are practical. Hence, automatic differentiation is used. In automatic differentiation, a computational graph is built and then the gradients are propagated backwards through it. This is called backpropagation and is covered in more detail in \autoref{c:backpropagation}.
		% end

		\subsection{Mini-Batch and Stochastic Gradient Descent}
			In mini-batch GD, only a small portion (common sizes are \num{32}, \num{64}, or \num{128} samples) of the training set is used for computing the gradient. This technique is used as it might not be possible to compute the whole gradient due to memory limitations. This makes the gradient noisy, but the progress is still good on average. It is also called \emph{stochastic} GD as the gradient gets replaced by a Monte-Carlo estimate of the expectation of the gradient.
		% end
	% end

	\section{Newton's Method and L-BFGS}
		Instead of walking along the gradient, \emph{Newton's method} leverages further knowledge from calculus and uses not only the gradient but the Hessian, too. The update equation is
		\begin{equation}
			\vec{\theta}^{(k + 1)} = \vec{\theta}^{(k)} - \alpha \cdot \Bigl(\!\mat{H}_L\bigl( \vec{\theta}^{(k)} \bigr)\!\Bigr)^{-1} \, \grad_{\vec{\theta}} L\bigl( \vec{\theta}^{(k)} \bigr),
		\end{equation}
		where \(\mat{H}_L\) is the Hessian. Newton's method is capable of finding the minima of quadratic functions in one step and also converges fast for other problems. But due to its dependence on the Hessian, which is costly to compute, and due to the inverse of the said, it is rarely used in deep learning. The inversion has a time complexity of \( \mathcal{O}(d^3) \), where \(d\) are the number of parameters. As neural networks usually have millions of parameters, inverting the matrix is not practical. Also, matrix inversion is numerically unstable and the Hessian might even be singular.

		Arising from these problems, the L-BFGS algorithm was developed which directly approximates the inverse of the Hessian. Hence, neither calculating nor inverting it is necessary.
	% end

	\section{Convergence}
		A method is said to converge \emph{quadratically} if the error \( \epsilon_k = x_\ast - x \) depends quadratically on the previous error:
		\begin{equation}
			\epsilon_{k + 1} = \mu \epsilon_k^2,\qquad \epsilon_k \in \mathcal{O}\bigl(\mu^{2^k}\bigr)
		\end{equation}
		Analogously, a method converges \emph{linearly} if the error depends linearly on the previous error:
		\begin{equation}
			\epsilon_{k + 1} \leq \mu \epsilon_k,\qquad \epsilon_k \in \mathcal{O}(\mu^n)
		\end{equation}
		For SGD, when the learning rate is changed to \(1/n\), the convergence is \( \epsilon_k \in \mathcal{O}(1/n) \). So SGD is terrible compared to other methods, but still it is used in practice as one million iterations or so are still enough for reaching single point precision in the error and one million iterations are okay to compute.
	% end

	\section{Momentum}
		When using SGD, the trajectory along the algorithm converges is very jittery on steep locations as the gradient is large and huge steps are made. On the other hand, only very slow progress is made at locations where the gradient is small. One approach for tackling this problem is \emph{momentum}. The formal definition is
		\begin{align}
			\vec{v}^{(k + 1)} &= \mu \vec{v}^{(k)} - \alpha \cdot \grad_{\vec{\theta}} L\bigl(\vec{\theta}^{(k)}\bigr) \\
			\vec{\theta}^{(k + 1)} &= \vec{\theta}^{(k)} + \vec{v}^{(k + 1)}
		\end{align}
		with a "friction" coefficient \(\mu\) which is usually \num{0.5}, \num{0.9}, or \num{0.99} or annealed over time (e.g., from \num{0.5} to \num{0.99}). The intuition is that the gradient can build up velocity along shallow regions of the loss which gets damped in steep regions. This reduces the jittering at steep regions by reducing the size of the gradients and ramps up the speed on shallow regions.

		The momentum is usually initialized with zeros.

		\subsection{Nesterov Momentum}
			A variant of the vanilla momentum is \emph{Nesterov momentum}, where the update is compute "one step ahead" by following the old momentum:
			\begin{align}
				\vec{v}^{(k + 1)} &= \mu \vec{v}^{(k)} - \alpha \cdot \grad_{\vec{\theta}} L\bigl( \vec{\theta}^{(k)} + \mu \vec{v}^{(k)} \bigr) \\
				\vec{\theta}^{(k + 1)} &= \vec{\theta}^{(k)} + \vec{v}^{(k + 1)}
			\end{align}
			Notice the change in the top equation when calculating the gradient. But this calculation is slightly inconvenient as usually the forward pass is computed for the current parameter settings anyway, e.g., for computing the predication accuracy. This inconvenience can be circumvented by a slight change of variables. Setting \( \vec{\phi}^{(k)} = \vec{\theta}^{(k)} + \mu \vec{v}^{(k)} \) and rearranging the formulas a bit, the update equation becomes:
			\begin{align}
				\vec{v}^{(k + 1)} &= \mu \vec{v}^{(k)} - \alpha \cdot \grad_{\vec{\theta}} L\bigl(\vec{\phi}^{(k)}\bigr) \\
				\vec{\phi}^{(k + 1)} &= \vec{\phi}^{(k)} - \mu \vec{v}^{(k)} + (1 + \mu) \vec{v}^{(k + 1)}
			\end{align}
			The disadvantage of this change of variables is that now the previous value of the momentum has to be kept in memory, but this is mainly an implementation pitfall one has to care for.
		% end

		\subsection{AdaGrad}
			Another variant of SGD is \emph{AdaGrad} which changes the update in a ways such that the gradients are normalized by the square-root of gradient norms. This reduces the learning rate when gradients are high and increases it when gradients are small. Hence, the algorithms moves faster in shallow and slower in steep regions:
			\begin{align}
				s^{(k + 1)} &= s^{(k)} + \Big\lVert \grad_{\vec{\theta}} L\bigl(\vec{\theta}^{(k)}\bigr) \Big\rVert_2^2 \\
				\vec{\theta}^{(k + 1)} &= \vec{\theta}^{(k)} - \frac{\alpha}{\sqrt{s^{(k + 1)}} + \epsilon} \grad_{\vec{\theta}} L\bigl(\vec{\theta}^{(k)}\bigr)
			\end{align}
			The "gradient cache" \(s\) is initialized with zero and \(\epsilon\) is a small number added to the denominator to prevent divisions by zero. Problems of AdaGrad are that the first step can be far off and that the learning rate vanishes over time due to \(s\) building up large values (as norms can never be negative), causing learning to stagnate.
		% end

		\subsection{RMSProp}
			\emph{RMSProp} tackles the problem of AdaGrad that the learning rate vanishes by adding a decay rate \(\eta \in [0, 1)\) to it:
			\begin{align}
				s^{(k + 1)} &= s^{(k)} \beta s^{(k)} + (1 - \beta) \big\lVert \grad_{\vec{\theta}} L\bigl(\vec{\theta}^{(k)}\bigr) \big\rVert_2^2 \\
				\vec{\theta}^{(k + 1)} &= \vec{\theta}^{(k)} - \frac{\alpha}{\sqrt{s^{(k + 1)}} + \epsilon} \cdot \grad_{\vec{\theta}} L\bigl(\vec{\theta}^{(k)}\bigr)
			\end{align}
			With a decay of \( \beta = 0 \), RMSProp is equivalent to AdaGrad. The decay rate should be set to something high like \( \beta = 0.9 \) as small learning rates cause exploding updates in the first steps. Like for AdaGrad, the first few steps can be quite far off.
		% end

		\subsection{Adam}
			\emph{Adam} is the state-of-the-art optimizer used nearly every time for training neural networks. It combines the idea of momentum with RMSProp
			\begin{align}
				\vec{v}^{(k + 1)} &= \beta_1 \vec{v}^{(k)} + (1 - \beta_1) \cdot \grad_{\vec{\theta}} L\bigl(\vec{\theta}^{(k)}\bigr) \\
				s^{(k + 1)} &= \beta_2 s^{(k)} + (1 - \beta_2) \big\lVert \grad_{\vec{\theta}} L\bigl(\vec{\theta}^{(k)}\bigr) \big\rVert_2^2 \\
				\hat{\vec{v}}^{(k + 1)} &= \frac{\vec{v}^{(k + 1)}}{1 - \beta_1^{k + 1}} \\
				\hat{s}^{(k + 1)} &= \frac{s^{(k + 1)}}{1 - \beta_2^{k + 1}} \\
				\vec{\theta}^{(k + 1)} &= \vec{\theta}^{(k)} - \frac{\alpha}{\sqrt{\hat{s}^{(k + 1)}} + \epsilon} \vec{v}^{(k + 1)}
			\end{align}
			where the hatted variables are a bias correction that compensates for the zero-initialization of \(\vec{v}\) and \(s\) and only really affects the process in the first few iterations where \(k\) i small.
		% end
	% end

	\section{Gradient Clipping}
		In gradient descent, big gradients cause divergence while small gradients cause slow convergence. As divergence is much worse and gradients can explode pretty fast, a technique called \emph{gradient clipping} can be used. Gradient clipping limits the magnitude of each gradient
		\begin{equation}
			\hat{g}_i = \min\big( g_\mathrm{max},\, \max(-g_\mathrm{max},\, g_i) \big)
		\end{equation}
		such that \( \lvert \hat{g}_i \rvert \leq g_\mathrm{max} \). Then a decreasing learning rate is used to converge to an optimum. Standard gradient clipping limits the larges gradient dimensions while others max be very small.

		An alternative, \emph{extreme} gradient clipping which is used in AdaGrad and RMSProp scale all dimensions by the inverse standard deviation such that all dimensions have unit standard deviation. An even more extreme approach are one-bit gradients where all gradient dimensions are clipped such that only their sign is left. This actually works on some problems! But usually, gradient clipping is not used this aggressive.
	% end

	\section{Gradient Noise}
		In stochastic gradient descent, the gradient resulting from the Monte Carlo approximations is noisy, but SGD still makes good progress on average. This gives the idea of whether it might be good to add a little noise to the gradients? Experiments have shown that this is actually the case, reducing overfitting in complex models. Usually the noise is additive
		\begin{equation}
			\hat{g}_i^{(k)} = g_i^{(k)} + \epsilon_t^{(k)},\quad \epsilon_t^{(k)} \sim \mathcal{N}(0, \sigma_k^2)
		\end{equation}
		with variance
		\begin{equation}
			\sigma_k^2 = \frac{\eta}{(1 + k) \gamma}
		\end{equation}
		where \( \eta \in \{ 0.01, 0.3, 1 \} \) and \( \gamma = 0.55 \). This way the noise added to the gradients is reduced over time, producing more stable gradients towards the end.

		Gradient noise raises some interesting theoretical questions as it turns the model parameters into a Bayesian inference task where the noise magnitude controls the temperature of the distribution (higher noise \(\to\) higher temperature). Cf. momentum which reduces the level of detail.
	% end

	\section{Learning Rate}
		All of the above optimizers share at least one hyperparameter: the learning rate \(\alpha\). This hyperparameter is usually the one with the most influence on the learning process. If the learning rate is too small, no learning happens. But if it is too high, the parameters blow up and the loss rises indefinitely. A good learning rate is somewhere im between. Tuning of the learning rate and hyperparameter search will also be covered in \autoref{subsec:hyperparameterOpt}.

		As usually most learning happens in the beginning and later on fine-tuning happens, it can be useful to decay the learning rate over time, i.e., make \(\alpha\) \(k\)-dependent: \(\alpha_k\). Two options are exponential decay
		\begin{equation}
			\alpha_k = \alpha_0 e^{-\eta k}
		\end{equation}
		with an initial learning rate \(\alpha_0\) and a decay rate \(\eta\). Another option is \(1/k\)-decay
		\begin{equation}
			\alpha_k = \frac{\alpha_0}{1 + \eta k},
		\end{equation}
		again with an initial learning rate \(\alpha_0\) and a decay rate \(\eta\). Other options are for example step-based decay (e.g., every \(n\) iterations, go done by a factor) or even manual decay.
	% end
% end

\chapter{Backpropagation}
	\label{c:backpropagation}

	As already discussed in \autoref{c:optimization}, the gradient of the loss function has to be computed in order to update the parameters of a neural network. The best method for doing this is automatic differentiation which is both fast (compared to numerical evaluation) and not as error-prone as analytical derivatives which have to be both implemented and derived by hand. The heart of automatic differentiation is building a computational graph and applying the chain rule
	\begin{equation}
		\dv{x} f\bigl( g(x) \bigr) = \pdv{f\bigl( g(x) \bigr)}{g(x)} \dv{g(x)}{x}
	\end{equation}
	multiple times. As an example \autoref{fig:compGraph} shows the computational graph for \( \bigl( (x + y) z \bigr)^2 \), including a forward pass (for computing the output) as well as a backward pass (for computing the derivatives). Computing the derivative goes as follows: start with one on the output (this is the derivative of the output w.r.t. the output). Then subsequently compute the local derivatives for each node with respect to the node the edge is coming from. This local derivative is then multiplied with the \emph{upstream gradient}, i.e., the value of the derivative coming in from the right. This yields the derivative of the output w.r.t. the node the edge the current number is computed for is coming from.

	This exhibits some interesting properties of the gradient flow and the patterns several gates induce. First of all, an add gate distributes the gradient between the incoming paths:
	\begin{align}
		\pdv{x} (x + y) &= 1 &
		\pdv{y} (y + y) &= 1
	\end{align}
	A product gate "switches" the gradient as the first downstream gradient gets multiplied by the value of the second flow:
	\begin{align}
		\pdv{x} (xy) &= y &
		\pdv{y} (xy) &= y
	\end{align}
	Another interesting gate is the max gate which "routes" the gradient depending on the input values, i.e., the gradient of which the flow has the smaller variable vanishes:
	\begin{align}
		\pdv{x} \max(x, y) &=
			\begin{cases}
				1 & \text{if } x > y \\
				0 & \text{if } x < y
			\end{cases} &
		\pdv{y} \max(x, y) &=
			\begin{cases}
				0 & \text{if } x > y \\
				0 & \text{if } x < y
			\end{cases}
	\end{align}
	An important property here is that the derivative for \(x = y\) is undefined as the maximum is not differentiable for \(x = y\). In practice, however, exact equality is rarely the case so this is not a real problem. Similarly a ReLU \( \max(0, x) \) works like a gradient switch where it can only pass through if the value was positive during the forward pass.

	In is also possible go forward through the network. This is called \emph{forward differentiation} opposed to \emph{backward differentiation}. In forward differentiation, the derivative \( \pdv*{x}{y} \) is computed, in backward differentiation the derivative \( \pdv*{y}{x} \) is computed. As neural networks usually have a scalar loss as the output, backward differentiation is trivial and easy to calculate.

	In real-world application, the backward pass would be computed in a batched/vectorized fashion for multiple input/output data and weights at once. This can be implemented efficiently using Einstein summation. \autoref{lst:forwardBackwardPass} shows a forward- and backward-pass through a multi-layer perceptron with two hidden layers and sigmoid nonlinearities after each hidden layer and no output nonlinearity.

	\begin{figure}
		\centering
		\begin{tikzpicture}[->, comp/.style = { draw, circle, minimum width = 0.8cm, minimum height = 0.8cm, inner sep = 0 }]
			\node (x) {\(x\)};
			\node [below = 1 of x] (y) {\(y\)};
			\node [below = 1 of y] (z) {\(z\)};
			\node [comp, right = 3 of x, label = above:{\(a\)}] (c1) {\(+\)};
			\node [comp, right = 3 of c1, label = above:{\(b\)}] (c2) {\(\ast\)};
			\node [comp, right = 3.5 of c2, label = above:{\(c\)}] (c3) {\(\cdot^2\)};
			\coordinate [right = 2 of c3] (c4);

			\draw (x) -- node[above, sloped, near start]{\(2\)} node[below, sloped, near start]{\(160\)} (c1);
			\draw (y) -- node[above, sloped, near start]{\(3\)} node[below, sloped, near start]{\(160\)} (c1);
			\draw (z) -- node[above, sloped]{\(4\)} node[below, sloped]{\( \pdv{c}{z} = \pdv{c}{b} \pdv{b}{c} = 40 \cdot a = 40 \cdot 5 = 200 \)} (c2);
			\draw (c1) -- node[above, sloped, near start]{\(5\)} node[below, sloped, near start]{\(160\)} (c2);
			\draw (c2) -- node[above, sloped]{\(20\)} node[below, sloped]{\(\pdv{c}{b} = \pdv{c}{c} \pdv{c}{b} = 1 \cdot 40\)} (c3);
			\draw (c3) -- node[above, sloped]{\(400\)} node[below, sloped]{\(\pdv{c}{c} = 1\)} (c4);
		\end{tikzpicture}
		\caption{Computational graph for \( \bigl( (x + y) z \bigr)^2 \). The numbers above the edges represent the forward pass, the numbers below the edges the backward pass (the derivatives). For some edges the application of the chain rule is written out explicitly. All other edges get computed analogous.}
		\label{fig:compGraph}
	\end{figure}

	\begin{lstlisting}[
			language = Python,
			caption = {Forward- and backward-pass for a multi-layer perceptron.},
			label = lst:forwardBackwardPass
		]
# W1, W2, W3, b1, b2, b3 are the weights/biases of the layers.
hid_1 = sigmoid(W1 @ X + b1[:, np.newaxis])
hid_2 = sigmoid(W2 @ hid_1 + b2[:, np.newaxis])
outputs = W3 @ hid_2 + b3[:, np.newaxis]

dL  = deriv_squared_loss(outputs, targets)
dW3 = np.einsum('ib,jb->ij', dL, hid_2)
db3 = np.einsum('ib->i', dL)

dS2 = deriv_sigmoid(W2 @ hid_1 + b2[:, np.newaxis])
dL  = np.einsum('kb,ki,ib->ib', dL, W3, dS2)
dW2 = np.einsum('ib,jb->ij', dL, hid_1)
db2 = np.einsum('ib->i', dL)

dS1 = deriv_sigmoid(W1 @ X + b1[:, np.newaxis])
dL  = np.einsum('kb,ki,ib->ib', dL, W2, dS1)
dW1 = np.einsum('ib,jb->ij', dL, X)
db1 = np.einsum('ib->i', dL)
	\end{lstlisting}

	\section{Activation Functions}
		\label{sec:activationFunctions}

		The \emph{activation function} is the nonlinearity behind a linear layer of a neural network, enriching the prediction power of the network. The big problem with activation functions is that---as neural networks lack interpretability---it is not really known what nonlinearities are good for which task. This section discusses some of the most popular activation functions and their respective pros and cons.

		\subsubsection{Sigmoid}
			One of the first activation functions was \emph{sigmoid} which squashes all input numbers into a range from zero to one:
			\begin{align}
				\sigma : \R \to (0, 1) : x \mapsto \frac{1}{1 + e^{-x}} &&
				\sigma'(x) = \sigma(x) \bigl( 1 - \sigma(x) \bigr)
			\end{align}
			Sigmoid is best for learning logical inputs, i.e., functions with binary inputs and is used for control signals in LSTM networks. But they can kill gradients for values \( \lvert x \rvert \lg 0 \) as the derivative is near zero for values far away from the center. Sigmoid is also not good for image networks (better use ReLU) and is not zero-centered. The latter is a problem because the sigmoid itself requires zero-centered input data for producing non-trivial (constant) results.

			Additionally, an always-positive input to a neuron causes the gradients to always be all-positive or all-negative. this leads to a zigzag path through the parameter space (much like axial iteration) which is not near an optimal optimization trajectory. This is also why the input data shall always be zero-centered beforehand (i.e., the mean of the input data should be zero).
		% end

		\subsubsection{Hyperbolic Tangent (Tanh)}
			The shape of the \emph{hyperbolic tangent} (tanh) activation function is similar, but the numbers are squashed into a range from plus to minus one:
			\begin{align}
				\tanh : \R \to (-1, 1) : x \mapsto \frac{e^x - e^{-x}}{e^x + e^{-x}} &&
				\tanh'(x) = 1 - \tanh^2(x)
			\end{align}
			Much like the sigmoid, gradients are killed for \(x\) values away from zero. But as the output is zero-centered, the tanh activation function is better suited for most problems as opposed to the sigmoid. It is also used for bounded, but signed, values in LSTM networks. But they are not as good as sigmoid for binary functions.
		% end

		\subsubsection{Rectified Linear Unit (ReLU)}
			The \emph{Rectified Linear Unit} (ReLU) is (at the moment) the go-to activation function for most problems:
			\begin{align}
				\mathrm{ReLU} : \R \to [0, \infty) : x \mapsto \max(0, x) &&
				\mathrm{ReLU}'(x) =
					\begin{cases}
						1 & \text{if } x > 0 \\
						0 & \text{if } x < 0
					\end{cases}
			\end{align}
			It saturates on half of the real axis and kills the gradient there, but it does never kill the gradient on the other half and is thus better suited for most problems than sigmoid or tanh. This is also visible empirically: networks using ReLU activations converge around six times faster than networks using sigmoid/tanh activations. But it is also not suitable for logical functions or for control in recurrent networks. Also, the output is not zero-centered. Another (theoretical) problem is that the gradient at \(x = 0\) is not defined. In practice this is not a big problem as values are almost never exactly equal to zero. If they are, one usually chooses zero or one as a gradient value, it does not really matter.

			To prevent the gradients from dying (a saturated ReLU is often called a \emph{dead ReLU}), the network is usually initialized with a slightly positive bias (e.g., \num{0.01}).
		% end

		\subsubsection{Leaky and Parametric ReLU}
			An alternative to the vanilla ReLU are the \emph{leaky} and \emph{parametric} ReLU. The parametric ReLU is given as
			\begin{align}
				\mathrm{PReLU}_\alpha : \R \to (-\infty, \infty) : x \mapsto \max(\alpha x, x) &&
				\mathrm{PReLU}_\alpha'(x) =
					\begin{cases}
						1      & \text{if } x > 0 \\
						\alpha & \text{if } x < 0
					\end{cases}
			\end{align}
			where for the leaky ReLU the parameter is fixed to \( \alpha = 0.01 \). This parameter is also learnable. Compared to all activation functions discussed before, the leaky and parametric ReLU do not saturate and have most advantages of the ReLU (e.g., faster convergence than sigmoid/tanh). Also the outputs are closer to zero-mean compared to a vanilla ReLU. But it is still not differentiable at \( x = 0 \).
		% end

		\subsubsection{Exponential Linear Unit (ELU)}
			The \emph{exponential linear unit} (ELU) combines the pros of a ReLU with differentiability at \(x = 0\):
			\begin{align}
				\mathrm{ELU} : \R \to (-\alpha, \infty) : x \mapsto
					\begin{cases}
						x                          & \text{if } x > 0    \\
						\alpha \big( e^x - 1 \big) & \text{if } x \leq 0
					\end{cases} &&
				\mathrm{ELU}'(x) =
					\begin{cases}
						\alpha e^x & \text{if } x > 0 \\
						1          & \text{if } x < 0 \\
						1          & \text{if } x = 0 \text{ and } \alpha = 1
					\end{cases}
			\end{align}
			As can be seen from the derivative, if \(\alpha = 1\) is chosen, the function is continuously differentiable everywhere. Otherwise it is not differentiable at \(x = 0\). The value of \(\alpha\) defines the value of the ELU as \(x \to -\infty\).
		% end

		\subsubsection{Maxout Neuron}
			The \emph{maxout neuron} is a special kind of neuron that combines the weighting and bias with the activation function:
			\begin{align}
				\max\bigl( \vec{w}_1^T \vec{x} + b_1,\, \vec{w}_2^T \vec{x} + b_2 \bigr)
			\end{align}
			It generalizes the parametric ReLU and works in the linear regime, and hence does not saturate. But it has the problem that it doubles the number of parameters as two weight matrices and biases have to be used.
		% end

		\subsubsection{In Practice\dots}
			\dots everything should be tried, usually ReLU. But also try leaky ReLU, Maxout and ELU. For binary (logical) functions, use sigmoid and also try out tanh (but do not hope for much).
		% end
	% end

	\section{Regularization}
		The number of layers and neurons per layer has an extreme impact on the performance and capacity of a network. In general more layers work better over more neurons per layer and more layers/neurons in general work better than less. But large networks can lead to overfitting when there are a lot more parameters then there is data! But instead of regularizing the network using the network size, it is better to use a strong regularization on the weights instead, e.g., L1- or L2-regularization. In these regularization methods, the sum of the absolute (L1) or squared (L2) weights is added to the loss with a penalty factor \(\lambda\) (usually \num{0.1}, \num{0.01}, or \num{0.001}). This keeps the parameters small and thus reduces model complexity.
	% end
% end

\chapter{Training Neural Networks}
	This chapter covers the complete learning process of neural networks. Some quirks and tricks for data pre-processing and normalization are covered as well as hyperparameter optimization and regularization.

	% TODO: Training Neural Networks: History; 4.24, 4.25, 4.26, 4.27, 4.28

	\section{Data Pre-Processing: Normalization}
		As already seen during the discussion of activation functions in \autoref{sec:activationFunctions}, it is desirable to have zero-centered input data, i.e., data with a zero mean. Because of that it is common (and recommended) to first \emph{normalize} the input data before training by subtracting the mean. Additionally the data is also often \emph{standardized}, meaning the normalized data is divided by the standard deviation of all samples to get a variance of one in every direction. This way the data does not fluctuate much along all axis which eases the training. It is also common to \emph{whiten} the data by applying a PCA (Principal Component Analysis) transformation to it, producing a unit Gaussian and decorrelating the features by rotating the data (such that the covariance matrix is the identity matrix).

		For images, however, it is common to not standardize or whiten the data but to just subtract the mean (e.g. the mean image in AlexNet of the per-channel mean in VGGNet). This is due to the numbers of an image being bounded by nature, e.g. to the interval from zero to one. Hence different variances along the different axis might even encode information that shall not be removed.

		During test time, the test input data has to be shifted and scaled the same way the training data was, based on the mean/standard deviation/PCA matrix/\dots that was computed from the training data. So the mean is not re-computed during test time!
	% end

	\section{Weight Initialization}
		An important factor of training neural networks is the initialization of the weights and biases in the neurons. If, for example, all weights are initialized with zeros, all downstream gradients vanish as they get multiplied with the weights. Hence, no learning happens. Besides the methods proposed below, initialization remains an open topic in neural network research and is not well understood. What is well understood, however, is that initialization plays a really important role and more often than expected is the key factor for the success of a neural network\footnote{Jonathan Frankle and Michael Carbin (2018): "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}.

		\paragraph{Small or Big Random Numbers}
			Another idea is to initialize all weights by sampling from a Gaussian distribution with a small standard deviation, e.g. \( \sigma = 0.01 \). This works fine for small networks, but for deep networks this initialization causes the mean and standard deviation of the activations in each layer to collapse quickly to zero. This is due to multiple small numbers getting multiplied together a lot of times. But activations with (near) zero values cause the gradient of the weights to vanish! Like for an initialization with all zeros, this causes no learning to happen.

			So small numbers are not the key to success. On the other hand, using a variance of \( \sigma = 1 \) for generating the initial weights does not work out either because the activation functions saturate quickly, yielding vanishing gradients. Hence again, no learning happens. For ReLU, this can also lead to exploding values as ReLU is not bounded.

			\autoref{fig:nnInitSmallRandom} shows the activation statistics of random data propagating through a ten-layer neural network with \num{300} neurons each. It can be seen that the mean and standard deviation saturate quickly, characterized by the histograms getting sharper. \autoref{fig:nnInitBigRandom} shows the same kind of plot, but for the big standard deviation. In this case the ReLU activation function is not shown in the mean/standard deviation plots because it blows up exponentially. But for sigmoid and tanh it can be seen that they saturate, too, causing the gradients to vanish.

			\begin{figure}
				\centering
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-small.pdf}
					\caption{Mean and standard deviation in each layer.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-small-Sigmoid-hist.pdf}
					\caption{Histogram of the values in each layer for the sigmoid activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-small-Tanh-hist.pdf}
					\caption{Histogram of the values in each layer for the tanh activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-small-ReLU-hist.pdf}
					\caption{Histogram of the values in each layer for the ReLU activation function.}
				\end{subfigure}
				\caption{Activation statistics for weights initialized by a Gaussian with small variance. Layer zero is the input layer. For all activation functions it can be seen that the mean saturates quickly and the standard deviation vanishes.}
				\label{fig:nnInitSmallRandom}
			\end{figure}

			\begin{figure}
				\centering
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-big.pdf}
					\caption{Mean and standard deviation in each layer.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-big-Sigmoid-hist.pdf}
					\caption{Histogram of the values in each layer for the sigmoid activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-big-Tanh-hist.pdf}
					\caption{Histogram of the values in each layer for the tanh activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-normal-big-ReLU-hist.pdf}
					\caption{Histogram of the values in each layer for the ReLU activation function.}
				\end{subfigure}
				\caption{Activation statistics for weights initialized by a Gaussian with big variance. Layer zero is the input layer. For all activation functions it can be seen that the neurons saturates quickly. The standard deviation is not zero, but this is caused by the sigmoid and tanh activation to have two saturation values.}
				\label{fig:nnInitBigRandom}
			\end{figure}
		% end

		\paragraph{Xavier Initialization}
			\emph{Xavier initialization} also uses normally distributed weights, but the standard deviation is reciprocally proportional to the square-root of the number of neurons in the previous layer (called \emph{fan in}): \( \sigma = 1/\sqrt{\textit{fan-in}} \). This is a reasonable activation and works well when using sigmoid or tanh activations. But when using ReLU activations, the gradients vanish again as nearly all activations get saturated in the left half of the ReLU (where it is flat). Instead, He et al. (2015) proposed to divide the fan in by two to get a better initialization, i.e., \( \sigma = 1/\sqrt{\textit{fan-in} / 2} \). This works reasonably well also for ReLU activations.

			\autoref{fig:nnInitXavier} shows the activation statistics like before. For tanh the mean approaches zero, but this is a good thing as zero-centered data reduces saturation in the next layer as long as the histogram is sufficiently wide. But it does not work good for ReLU which saturates quickly. The alternative Xavier initialization proposed by He et al. is shown in \autoref{fig:nnInitHe}, yielding similar results for sigmoid and tanh. But even for the ReLU, the histogram is not as sharp as for vanilla Xavier initialization.

			\begin{figure}
				\centering
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-xavier.pdf}
					\caption{Mean and standard deviation in each layer.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-xavier-Sigmoid-hist.pdf}
					\caption{Histogram of the values in each layer for the sigmoid activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-xavier-Tanh-hist.pdf}
					\caption{Histogram of the values in each layer for the tanh activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-xavier-ReLU-hist.pdf}
					\caption{Histogram of the values in each layer for the ReLU activation function.}
				\end{subfigure}
				\caption{Activation statistics for weights initialized using Xavier initialization. Layer zero is the input layer. Except for the ReLU activation, Xavier initialization works well and the neurons do not saturate.}
				\label{fig:nnInitXavier}
			\end{figure}

			\begin{figure}
				\centering
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-he.pdf}
					\caption{Mean and standard deviation in each layer.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-he-Sigmoid-hist.pdf}
					\caption{Histogram of the values in each layer for the sigmoid activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-he-Tanh-hist.pdf}
					\caption{Histogram of the values in each layer for the tanh activation function.}
				\end{subfigure}
				\begin{subfigure}{\linewidth}
					\includegraphics[width=\linewidth]{tmp-nn-init-he-ReLU-hist.pdf}
					\caption{Histogram of the values in each layer for the ReLU activation function.}
				\end{subfigure}
				\caption{Activation statistics for weights initialized using the modified Xavier initialization by He et al. Layer zero is the input layer. It can be seen that the histograms in the layers are sufficiently wide, enabling learning. Also the ReLU does not saturate.}
				\label{fig:nnInitHe}
			\end{figure}
		% end

		\subsection{Batch Normalization}
			As already seen, unit Gaussian data is best as an input for most activation functions. The idea of batch normalization to just make the data unit Gaussian by subtracting the mean and dividing by the (empirical) standard deviation at each layer. This is done by averaging the incoming data along each dimension and computing the standard deviation for each dimension. Then element-wise subtraction and division is used for normalize and standardize the data. As the network then learns weights and biases on this data, it is theoretically possible that the networks learns to recover the original data by learning the values of the mean and standard deviation. Hence, no prediction power is taken away from the network by adding a batch normalization layer\footnote{Note that this is just a semi-theoretical argument to support batch normalization. In practice, the neural network nearly never learns the mean and standard deviation}.

			Advantages of batch normalization is that it improves the gradient flow through the network, allows higher learning rates by high-quality gradients, and reduces the strong dependence on initialization. It also kind of works as a form of regularization and maybe reduces the need for dropout.

			During test time, the mean and standard deviation that were computed during training are used to normalizing/standardizing the data---the test data is not used to compute the mean/standard deviation again.
		% end
	% end

	\section{Designing a Network and Hyperparameter Optimization}
		This section covers the basic steps of designing a neural network, i.e., choosing the architecture, the loss, etc. The first step is to pre-process the data (normalization) as this is usually useful. The next step is to choose the architecture. This heavily depends on the task and prior intuition and knowledge are extremely helpful for settings up a good initial network. Furthermore, double check that the loss is reasonable: do not use cross-entropy for regression or quadratic loss for classification and remember to disable any regularization tricks. When enabling regularization, the loss should go up---that can be checked, too.

		The next step is to train the network on a small portion of the training data to see if the model can overfit. If the model is not able to overfit on a very small dataset, its capacity is not high enough and the neural network has to be made larger. If that works, switch to training on the complete dataset and tuning of the learning rate. If the loss is not going down, the learning rate is usually too low. If the loss is exploding, the learning rate is usually too high. A principled way of finding a good learning rate is to use cross-validation on several learning rates that are magnitudes apart (a learning rate of \(0.0002\) usually performs similar to \(0.0001\), so try out \(0.1\), \(0.01\), \(0.001\), \dots). This boils down to hyperparameter optimization as the learning rate is a hyperparameter.

		\section{Hyperparameter Optimization}
			\label{subsec:hyperparameterOpt}

			\emph{Hyperparameter optimization} is a big topic in designing neural networks and deep learning in general. It usually boils down to trying out different hyperparameter settings in a principled way. As a first step, a coarse search for only a few epochs should be done to get a rough idea what parameter settings work. Afterwards, a finer search for more iterations is executed to get more precise parameters. To speed up this process it is often advisable to stop the learning if the loss gets more than three times as high as the original loss. Also, the update ratio, i.e., the ratio between the norm of the old weights and the norm of the new weights \( \lVert \vec{\theta}^{(k)} \rVert_2 / \lVert \vec{\theta}^{(k + 1)} \rVert_2 \), should be around \num{0.001} or so.

			For the coarse search it is best to search in log-space. This is especially true for the learning rate which is the most important parameter. Other hyperparameters to try are for example the network architecture itself, the decay schedule of the learning rate and its update type, regularization (L2/dropout strength), etc. When looking at the accuracy, there should also be a small gap between the training and test accuracy. If the gap is too big, the model overfitted and the regularization strength should be increased. If these is no gap, the model capacity is most likely too low and the network should be enlarged.

			\subsection{Grid vs. Random Search}
				When evaluating multiple parameters at once, it is advisable to not evaluate them on a grid, but to sample each from a uniform distribution. It is often the case that one hyperparameter (e.g. the learning rate) has a higher influence on the result than another (e.g. the initialization of the bias). If one uses a grid, a lot of redundant computations will be performed for a single parameter. When using random values, this happens less frequent. This is illustrated in \autoref{fig:gridRandomSearch}.

				\begin{figure}
					\centering
					\begin{subfigure}{0.49\linewidth}
						\centering
						\begin{tikzpicture}[
									hp/.style = {
										draw,
										circle,
										fill = black,
										inner sep = 0,
										minimum width = 5pt,
										minimum height = 5pt
									},
									lv/.style = {
										draw,
										rectangle,
										fill = black,
										inner sep = 0,
										minimum width = 1pt,
										minimum height = 5pt
									},
									lh/.style = {
										draw,
										rectangle,
										fill = black,
										inner sep = 0,
										minimum width = 5pt,
										minimum height = 1pt
									}
								]
							\coordinate (a) at (-1.75, -1.75);
							\coordinate (b) at (+1.75, -1.75);
							\coordinate (c) at (+1.75, +1.75);
							\coordinate (d) at (-1.75, +1.75);

							\node [hp] at (-1, -1) {};
							\node [hp] at ( 0, -1) {};
							\node [hp] at (+1, -1) {};
							\node [hp] at (-1,  0) {};
							\node [hp] at ( 0,  0) {};
							\node [hp] at (+1,  0) {};
							\node [hp] at (-1, +1) {};
							\node [hp] at ( 0, +1) {};
							\node [hp] at (+1, +1) {};

							\node [lv] at (-1, +1.75) {};
							\node [lv] at ( 0, +1.75) {};
							\node [lv] at (+1, +1.75) {};

							\node [lh] at (-1.75, -1) {};
							\node [lh] at (-1.75,  0) {};
							\node [lh] at (-1.75, +1) {};

							\draw (a) -- (b) -- (c) -- (d) -- cycle;

							\path (c) -- node[above, sloped]{Important Param.} (d);
							\path (a) -- node[above, sloped]{Unimportant Param.} (d);
						\end{tikzpicture}
						\caption{Grid Search}
					\end{subfigure}
					~
					\begin{subfigure}{0.49\linewidth}
						\centering
						\begin{tikzpicture}[
									hp/.style = {
										draw,
										circle,
										fill = black,
										inner sep = 0,
										minimum width = 5pt,
										minimum height = 5pt
									},
									lv/.style = {
										draw,
										rectangle,
										fill = black,
										inner sep = 0,
										minimum width = 1pt,
										minimum height = 5pt
									},
									lh/.style = {
										draw,
										rectangle,
										fill = black,
										inner sep = 0,
										minimum width = 5pt,
										minimum height = 1pt
									}
								]
							\coordinate (a) at (-1.75, -1.75);
							\coordinate (b) at (+1.75, -1.75);
							\coordinate (c) at (+1.75, +1.75);
							\coordinate (d) at (-1.75, +1.75);

							\node [hp] at (+1.43, +1.27) {};
							\node [hp] at (+0.73, +1.49) {};
							\node [hp] at (+0.11, -1.32) {};
							\node [hp] at (-1.17, -0.50) {};
							\node [hp] at (-1.10, +0.43) {};
							\node [hp] at (+0.97, -0.77) {};
							\node [hp] at (-0.68, -0.40) {};
							\node [hp] at (+0.81, +0.49) {};
							\node [hp] at (-0.05, -0.33) {};

							\node [lv] at (+1.43, +1.75) {};
							\node [lv] at (+0.73, +1.75) {};
							\node [lv] at (+0.11, +1.75) {};
							\node [lv] at (-1.17, +1.75) {};
							\node [lv] at (-1.10, +1.75) {};
							\node [lv] at (+0.97, +1.75) {};
							\node [lv] at (-0.68, +1.75) {};
							\node [lv] at (+0.81, +1.75) {};
							\node [lv] at (-0.05, +1.75) {};

							\node [lh] at (-1.75, +1.27) {};
							\node [lh] at (-1.75, +1.49) {};
							\node [lh] at (-1.75, -1.32) {};
							\node [lh] at (-1.75, -0.50) {};
							\node [lh] at (-1.75, +0.43) {};
							\node [lh] at (-1.75, -0.77) {};
							\node [lh] at (-1.75, -0.40) {};
							\node [lh] at (-1.75, +0.49) {};
							\node [lh] at (-1.75, -0.33) {};

							\draw (a) -- (b) -- (c) -- (d) -- cycle;

							\path (c) -- node[above, sloped]{Important Param.} (d);
							\path (a) -- node[above, sloped]{Unimportant Param.} (d);
						\end{tikzpicture}
						\caption{Random Search}
					\end{subfigure}
					\caption{Comparison of grid search (left) and random search (right). It can be seen that for random search, more (nine) values of the important parameter are tried while when using grid search, only three values are tested.}
					\label{fig:gridRandomSearch}
				\end{figure}
			% end
		% end
	% end

	\section{Ensembles}
		\emph{Ensembles} leverage multiple simpler models for constructing a single big model with strong prediction power. There are two basic types for ensembles: \emph{Bagging} (Bootstrap Aggregation) and \emph{Boosting}. In bagging, multiple models are trained on samples of the data and a majority vote (for classification) or average (for regression) is used for prediction. In \emph{boosting}, the learners are ordered and training takes place one after another. Then each learner tries to reduce the residual error of the previous trainers on the examples that where misclassified. Bagging usually reduces the variance whereas boosting reduces the bias of the model (and also probably the variance, but this is not true for every model). In both cases, the resulting prediction is a sum or vote of the base learner predictions.

		Usually bagging yields around \SI{2}{\percent} of extra accuracy, so models can be made better but one can not fix a broken model using bagging. Hence, neural networks are fairly often used with bagging. But they are only rarely used with boosting. The reason for this is that deep neural networks usually model global effects and wide networks model local effects, whereas boosting models from global to local. This means that the first models of a boosting sequence models the global effects and later models are used for fine-tuning locally. So boosting with the same network architecture does not yield good results because the shift from global to local is not representable. An open questions is if it is possible to move from deep to wide networks the further the process of boosting gets. The problem that arises in this case is that it is hard to detect whether the next model in a boosting process models global or local properties.

		A common disadvantage of bagging and boosting is of course that it takes longer to train compared to a single model. One trick to circumvent this in bagging is to average multiple model checkpoints, i.e., the weights of a network at different iterations, rather than completely different models.
	% end

	\section{Dropout Regularization}
		As neural networks have lots of parameters, sometimes more than there is training data, overfitting is a big problem in deep learning. One regularization technique for tackling this is \emph{dropout}. The core concept of dropout is to randomly set some neurons to zero with probability \(p\) for each neuron during the forward pass, disabling them for the prediction. The idea is that this forces the network to learn redundant representations, making it more robust towards changes in the inputs. Another interpretation is that dropout trains a large ensemble of models that share parameters.

		During test time, it would be ideal to integrate out all the noise, but in practice this is obviously intractable. Instead, the result will be approximated using Monte Carlo integration techniques: calculate multiple forward passes using different (random) dropout masks and average all predictions. But in fact this can be done (approximately) with a single forward pass with all neurons being active! To compensate for the inflation caused by roughly \(1/p\) more neurons being active during the fully activated forward pass, all activations have to be scaled by \(p\).

		As scaling the activations at test time reveals details about the training process\footnote{Take, for example, a model that is deployed on lots on end-user devices (like a face detection model). If the developer finds a better way of training that improves the model, but uses dropout, the end-user model would have to be changed just because the training process changed.}, it is also possible to scale the activations by \(1/p\) in the training forward pass. This is called \emph{inverted dropout}.
	% end
% end

\chapter{Convolutional Neural Networks} % 6.1, 6.2, 6.3, 6.64, 6.65, 6.66, 6.67, 6.68, 6.170
	\todo{Content}

	\section{Biology} % N/A
		\todo{Content}

		\subsection{Retinal Receptive Fields} % 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 6.10, 6.11, 6.12, 6.13, 6.14
			\todo{Content}
		% end

		\subsection{Cortical Receptive Fields} % 6.15, 6.16, 6.17, 6.18, 6.19, 6.20, 6.21, 6.22, 6.23, 6.24, 6.25, 6.26, 6.27, 6.28
			\todo{Content}
		% end
	% end

	\section{Convolutions} % 6.29, 6.30, 6.31, 6.32, 6.33, 6.34, 6.35, 6.36, 6.37, 6.38
		\todo{Content}

		\subsection{Examples} % 6.39, 6.40, 6.41, 6.42, 6.43, 6.44, 6.45, 6.46, 6.47, 6.48, 6.49, 6.50, 6.51, 6.52, 6.53, 6.54, 6.55, 6.56, 6.57, 6.58, 6.59, 6.60, 6.61, 6.62, 6.63
			\todo{Content}
		% end
	% end

	\section{Convolutional Layers} % 6.66, 6.67, 6.68, 6.69, 6.70, 6.71, 6.72, 6.73, 6.74, 6.75, 6.76, 6.77, 6.78, 6.79, 6.80, 6.81, 6.82, 6.83, 6.84, 6.85, 6.86, 6.120, 6.121
		\todo{Content}

		\subsection{Activation Volume} % 6.87, 6.88, 6.89, 6.90, 6.91, 6.92, 6.93, 6.94, 6.95, 6.96, 6.115, 6.122
			\todo{Content}
		% end

		\subsection{Zero-Padding and Stride} % 6.100, 6.101, 6.102, 6.103, 6.104, 6.105, 6.106, 6.107, 6.108, 6.109, 6.110, 6.111, 6.112, 6.113, 6.114
			\todo{Content}
		% end

		\subsection{Neuron View} % 6.123, 6.124, 6.125, 6.126
			\todo{Content}
		% end
	% end

	\section{Pooling} % 6.127, 6.128, 6.129, 6.130, 6.131
		\todo{Content}
	% end

	\section{Examples} % N/A
		\todo{Content}

		\paragraph{LeNet-5} % 6.133, 6.134, 7.11
			\todo{Content}
		% end

		\paragraph{AlexNet} % 6.136, 6.137, 6.138, 6.139, 6.140, 6.141, 6.142, 6.143, 7.12, 7.13
			\todo{Content}
		% end
	% end

	\section{Transfer Learning} % 6.144, 6.145, 6.146, 6.147, 6.148, 6.149, 6.150, 6.151
		\todo{Content}

		\subsection{Examples} % N/A
			\todo{Content}

			\paragraph{VGGNet} % 6.152, 6.153, 6.154, 6.155
				\todo{Content}
			% end

			\paragraph{GoogLeNet} % 6.156, 6.157
				\todo{Content}
			% end

			\paragraph{ResNet} % 6.158, 6.159, 6.160, 6.161, 6.162, 6.163, 6.164, 6.165, 6.166, 6.167, 7.26, 7.27, 7.28, 7.32, 7.34
				\todo{Content}
			% end

			\paragraph{AlphaGo} % 6.168, 6.169
				\todo{Content}
			% end
		% end
	% end
% end

\chapter{Computer Vision Tasks} % 7.36, 7.37, 7.38, 7.118
	\todo{Content}

	\section{Classification + Localization} % 7.39, 7.40, 7.63
		\todo{Content}

		\subsection{Localization as Regression} % 7.41, 7.42, 7.43, 7.44, 7.45, 7.46, 7.47
			\todo{Content}

			\subsubsection{Localizing Multiple Objects} % 7.48
				\todo{Content}
			% end

			\subsubsection{Human Pose Estimation} % 7.49
				\todo{Content}
			% end
		% end

		\subsection{Sliding Window} % 7.50, 7.51, 7.52, 7.53, 7.54, 7.55, 7.56, 7.57, 7.58, 7.59, 7.60, 7.61, 7.62
			\todo{Content}
		% end
	% end

	\section{Object Detection} % 7.65, 7.87, 7.88, 7.117
		\todo{Content}

		\subsection{Detection as Regression} % 7.66, 7.67, 7.68
			\todo{Content}
		% end

		\subsection{Detection as Classification} % 7.69, 7.70, 7.71, 7.72, 7.73, 7.76
			\todo{Content}

			\subsubsection{Deformable Parts Model (DPM)} % 7.74, 7.75
				\todo{Content}
			% end

			\subsubsection{Region Proposals} % 7.77, 7.78, 7.79, 7.80
				\todo{Content}
			% end
		% end

		\subsection{R-CNN} % 7.81, 7.82, 7.83, 7.84, 7.85, 7.86, 7.89, 7.90, 7.91, 7.92, 7.93
			\todo{Content}
		% end

		\subsection{Fast R-CNN} % 7.94, 7.95, 7.96, 7.98, 7.98, 7.99, 7.100, .101, 7.102, 7.103, 7.104, 7.105, 7.106
			\todo{Content}
		% end

		\subsection{Faster R-CNN} % 7.107, 7.108, 7.109, 7.110, 7.111, 7.112, 7.113
			\todo{Content}
		% end

		\subsection{YOLO: You Only Look Once} % 7.114, 7.115
			\todo{Content}
		% end
	% end
% end

\chapter{Recurrent Neural Networks} % 8.1, 8.4, 8.5, 8.23, 8.24, 8.25, 8.26, 8.27, 8.28, 8.29, 8.98
	\todo{Content}

	\section{Unrolling and Backprop-Through-Time} % 8.6, 8.7, 8.8, 8.9, 8.10, 8.11, 8.12, 8.13, 8.14, 8.15, 8.16, 8.17, 8.18, 8.19, 8.20, 8.21, 8.22
		\todo{Content}
	% end

	\section{Vanilla RNN} % 8.30, 8.31, 8.32, 8.33, 8.34
		\todo{Content}

		\subsection{Example: Character-Level Language Model} % 8.35, 8.36, 8.37, 8.38, 8.47, 8.48, 8.52, 8.53, 8.54, 8.55, 8.56, 8.57, 8.58, 8.59, 8.60, 8.61
			\todo{Content}

			\subsubsection{Interpretable Cells} % 8.62, 8.63, 8.64, 8.65, 8.66, 8.67
				\todo{Content}
			% end
		% end

		\subsection{Image Captioning} % 8.68, 8.69, 8.70, 8.71, 8.72, 8.73, 8.74, 8.75, 8.76, 8.77, 8.78, 8.79, 8.80, 8.81, 8.82, 8.83
			\todo{Content}
		% end
	% end

	\section{Long Short-Term Memory (LSTM)} % 8.84, 8.85, 8.86, 8.87, 8.88, 8.89, 8.90, 8.91, 8.92, 8.93
		\todo{Content}

		\subsection{Gradient Flow Dynamics} % 8.94, 8.95, 8.96
			\todo{Content}
		% end

		\subsection{Variants and Friends} % 8.97
			\todo{Content}
		% end
	% end
% end

\chapter{Generative Models} % 9b.1, 9b.3, 9b.4, 9b.5, 9b.6, 9b.7, 9b.8, 9b.9, 9b.10, 9b.11, 9b.12, 9b.13, 9b.14, 9b.15, 9b.16, 9b.17, 9b.18, 9b.19, 9b.20, 9b.131, 9b.132, 9b.133, 9a.3, 9a.4, 9a.5, 9a.6, 9a.7, 9a.8
	\todo{Content}

	\section{Fully Visible Belief Network} % 9b.22, 9b.23, 9b.24, 9a.9
		\todo{Content}
	% end

	\section{WaveNet} % 9a.10
		\todo{Content}
	% end

	\section{Change of Variables} % 9a.11
		\todo{Content}
	% end

	\section{Boltzmann Machine} % 9a.13
		\todo{Content}
	% end

	\section{PixelRNN and PixelCNN} % 9b.21, 9b.25, 9b.26, 9b.27, 9b.28, 9b.29, 9b.30, 9b.31, 9b.32, 9b.33
		\todo{Content}
	% end

	\section{Variational Auto-Encoder (VAE)} % 9b.34, 9b.35, 9b.36, 9b.97, 9a.12
		\todo{Content}

		\subsection{Auto-Encoder} % 9b.37, 9b.38, 9b.39, 9b.40, 9b.41, 9b.42, 9b.43, 9b.44, 9b.45, 9b.46, 9b.47, 9b.48, 9b.49
			\todo{Content}
		% end

		\subsection{Model} % 9b.50, 9b.51, 9b.52, 9b.53, 9b.54, 9b.55, 9b.56, 9b.57, 9b.58, 9b.59, 9b.60, 9b.61
			\todo{Content}
		% end

		\subsection{Intractability} % 9b.62, 9b.63, 9b.64, 9b.65, 9b.66, 9b.67, 9b.68
			\todo{Content}
		% end

		\subsection{Encoder, Decoder, and Evidence Lower Bound (ELBO)} % 9b.69, 9b.70, 9b.71, 9b.72, 9b.73, 9b.74, 9b.75, 9b.76, 9b.77, 9b.78, 9b.79, 9b.80, 9b.81, 9b.82
			\todo{Content}
		% end

		\subsection{Training Procedure} % 9b.83, 9b.84, 9b.85, 9b.86, 9b.87, 9b.88, 9b.89, 9b.90
			\todo{Content}
		% end

		\subsection{Generating Data} % 9b.91, 9b.92, 9b.93, 9b.94, 9b.95, 9b.96
			\todo{Content}
		% end
	% end

	\section{Generative Adversarial Networks (GANs)} % , 9b.114, 9b.115, 9b.130, 9a.1, 9a.2, 9a.14, 9a.15, 9a.16
		\todo{Content}

		\subsection{Two-Player Game} % 9b.104, 9b.105, 9b.106, 9b.107, 9b.108, 9b.109
			\todo{Content}

			\subsubsection{Optimization Problems} % 9b.110, 9b.111, 9b.112
				\todo{Content}
			% end
		% end

		\subsection{Convolutional Architectures} % 9b.118, 9b.119, 9b.120, 9b.121
			\todo{Content}

			\subsubsection{Interpretability} % 9b.122, 9b.123, 9b.124, 9b.125, 9b.126
				\todo{Content}
			% end
		% end
	% end

	\section{Generative Adversarial Networks (GANs)} % 9a.14, 9a.15, 9a.16, 9a.33, 9a.50, 9b.98, 9b.99, 9b.100, 9b.101, 9b.102, 9b.103, 9b.130
		\todo{Content}

		\subsection{Training Procedure} % 9a.17, 9a.23, 9a.34, 9b.104, 9b.105, 9b.113, 9b.114, 9b.115
			\todo{Content}

			\subsubsection{Minimax, Non-Saturating, and Maximum Likelihood Games} % 9a.18, 9a.19, 9a.20, 9a.21, 9b.106, 9b.107, 9b.108, 9b.109
				\todo{Content}
			% end

			\subsubsection{Discriminator Strategy} % 9a.22
				\todo{Content}
			% end

			\subsubsection{Mode Collapse} % 9a.29, 9b.110, 9b.111, 9b.112
				\todo{Content}
			% end
		% end

		\subsection{Convolutional Architectures} % 9b.118, 9b.119, 9b.120, 9b.121
			\todo{Content}
		% end

		\subsection{Vector Space Arithmetic} % 9a.28, 9b.122, 9b.123, 9b.124, 9b.125, 9b.126
			\todo{Content}
		% end
	% end

	\section{Optimization and Games} % 9a.37, 9a.38
		\todo{Content}

		\subsection{Nash Equilibrium} % 9a.39
			\todo{Content}
		% end

		\subsection{Well-Studies Cases} % 9a.40
			\todo{Content}

			\subsubsection{Continuous Minimax Game} % 9a.41
				\todo{Content}
			% end

			\subsubsection{Local Differential Nash Equilibria} % 9a.42
				\todo{Content}
			% end

			\subsubsection{Gradient Descent Convergence} % 9a.43, 9a.44
				\todo{Content}
			% end
		% end

		\subsection{Heuristics} % 9a.45, 9a.46
			\todo{Content}
		% end

		\subsection{Other Games in AI} % 9a.47
			\todo{Content}
		% end
	% end
% end

\chapter{Probabilistic Graphical Models} % 10b.6, 10b.7, 10b.8, 10b.9, 10b.10, 10b.11, 10b.12, 10b.13, 10b.14, 10b.15, 10b.16, 10b.17, 10b.18, 10b.19, 10a.1, 10a.2, 10a.3, 10a.6, 10a.13, 10a.14, 10a.22, 10a.23, 10a.24, 10a.69
	\todo{Content}

	\section{(Conditional) Independency} % 10a.15, 10a.16
		\todo{Content}
	% end

	\section{Tractability vs. Expressiveness} % 10b.60, 10b.61, 10b.62, 10b.63
		\todo{Content}

		\subsection{Inference and Queries} % 10a.19
			\todo{Content}

			\paragraph{Complete Evidence Queries (EVI)} % 10b.20, 10b.21, 10b.22
				\todo{Content}
			% end

			\paragraph{Marginal Queries (MAR)} % 10b.31, 10b.32, 10b.33
				\todo{Content}
			% end

			\paragraph{Conditional Queries (CON)} % 10b.34, 10b.35, 10b.36
				\todo{Content}
			% end

			\paragraph{Maximum A-Posteriori (MAP)} % 10b.46, 10b.47, 10b.48, 10b.49
				\todo{Content}
			% end

			\paragraph{Marginal MAP (MMAP)} % 10b.50, 10b.51, 10b.52, 10b.53
				\todo{Content}
			% end

			\paragraph{Advanced Queries} % 10b.54, 10b.55, 10b.56, 10b.57, 10b.58
				\todo{Content}
			% end
		% end

		\subsection{Models} % N/A
			\todo{Content}

			\subsubsection{Generative Adversarial Networks} % 10b.23,, 10b.24
				\todo{Content}
			% end

			\subsubsection{Variational Autoencoders} % 10b.25, 10b.26
				\todo{Content}
			% end

			\subsubsection{Probabilistic Graphical Models: Markov and Bayes Networks} % 10b.27, 10b.28, 10b.29, 10b.30, 10b.37, 10b.38, 10a.7, 10a.17, 10a.18, 10a.19, 10a.25
				\todo{Content}

				\paragraph{Variable Elimination} % 10a.20, 10a.21
					\todo{Content}
				% end
			% end

			\subsubsection{Low-Tree-Width PGMs: Trees} % 10b.39, 10b.40, 10b.41
				\todo{Content}
			% end

			\subsubsection{Mixtures} % 10b.42, 10b.43, 10b.44, 10b.45
				\todo{Content}
			% end

			\subsubsection{Fully Factorized Models} % 10b.59
				\todo{Content}
			% end
		% end
	% end

	\section{Probabilistic Circuits} % 10b.64, 10b.65, 10b.66, 10b.67, 10b.68, 10b.69, 10b.70, 10b.71, 10b.72, 10b.73, 10b.74, 10b.75, 10b.76, 10b.77, 10b.78, 10b.79, 10b.80, 10b.81, 10b.82, 10b.104, 10b.126
		\todo{Content}

		\subsection{Ensuring Tractability} % 10b.83
			\todo{Content}

			\subsubsection{Decomposability and Smoothness} % 10b.84, 10b.85
				\todo{Content}

				\paragraph{Tractable MAR/CON} % 10b.86, 10b.87, 10b.88, 10b.89
					\todo{Content}
				% end
			% end

			\subsubsection{Determinism} % 10b.90
				\todo{Content}

				\paragraph{Tractable MAP} % 10b.91, 10b.92, 10b.93, 10b.94, 10b.95, 10b.96, 10b.97, 10b.98
					\todo{Content}
				% end

				\paragraph{Approximate MAP} % 10b.99
					\todo{Content}
				% end
			% end

			\subsubsection{Structured Decomposability} % 10b.100, 10b.101, 10b.102, 10b.103
				\todo{Content}
			% end
		% end

		\subsection{Logical Circuits} % 10b.105, 10b.106, 10b.107, 10b.108
			\todo{Content}

			\subsubsection{Weighted Model Counting (WMC)} % 10b.109
				\todo{Content}
			% end

			\subsubsection{From Trees to Circuits} % 10b.110, 10b.111, 10b.112, 10b.113, 10b.114, 10b.115, 10b.116, 10b.117
				\todo{Content}
			% end

			\subsubsection{Low-Tree-Width PGMs} % 10b.118
				\todo{Content}
			% end

			\subsubsection{Arithmetic Circuits (ACs)} % 10b.119
				\todo{Content}
			% end

			\subsubsection{Sum-Product Networks (SPNs)} % 10b.120, 10a.26, 10a.27, 10a.28, 10a.29, 10a.30, 10a.31, 10a.32, 10a.33, 10a.53, 10a.54, 10a.55, 10a.56, 10a.57
				\todo{Content}

				\paragraph{Semantics} % 10a.34
					\todo{Content}
				% end

				\paragraph{Linear Inference} % 10a.35, 10a.36, 10a.37, 10a.38
					\todo{Content}
				% end

				\paragraph{Image Completion} % 10a.40, 10a.41, 10a.42, 10a.43, 10a.44
					\todo{Content}
				% end

				\paragraph{Variants} % 10a.45, 10a.46, 10a.47
					\todo{Content}
				% end

				\paragraph{Symbolic Evaluation} % 10a.51, 10a.52
					\todo{Content}
				% end
			% end

			\subsubsection{Cutset Networks (CNets)} % 10b.121, 10b.122, 10b.123
				\todo{Content}
			% end

			\subsubsection{Probabilistic Sentential Decision Diagrams} % 10b.124
				\todo{Content}
			% end
		% end

		\subsection{Expressiveness} % 10b.127, 10b.128
			\todo{Content}
		% end
	% end

	\section{Building Circuits} % 10b.129, 10b.130, 10b.131, 10b.132, 10b.133, 10b.134, 10b.135, 10b.136, 10a.39
		\todo{Content}

		\subsection{Hard/Soft Parameter Updating: Gradient Descent and EM} % 10b.137, 10b.138
			\todo{Content}
		% end

		\subsection{(Bayesian) Parameter Learning} % 10b.139, 10b.140, 10b.141
			\todo{Content}
		% end

		\subsection{Structure Learning} % 10b.142
			\todo{Content}

			\subsubsection{LearnSPN} % 10b.143, 10b.144, 10b.145, 10b.146
				\todo{Content}

				\paragraph{ID-SPN} % 10b.148
					\todo{Content}
				% end

				\paragraph{Other Variants} % 10b.147, 10b.149
					\todo{Content}
				% end
			% end

			\subsubsection{Cut(e)set Network} % 10b.150
				\todo{Content}
			% end

			\subsubsection{PSDD Structure Learning} % 10b.151, 10b.152
				\todo{Content}
			% end

			\subsubsection{LearnPSDD} % 10b.153
				\todo{Content}
			% end

			\subsubsection{Learning Logistic Circuits} % 10b.154, 10b.155
				\todo{Content}
			% end

			\subsubsection{Bayesian Structure Learning} % 10b.156
				\todo{Content}
			% end

			\subsubsection{Automatic Bayesian Density Analysis (ABDA)} % 10b.157, 10b.158
				\todo{Content}
			% end

			\subsubsection{Bayesian SPNs} % 10b.159, 10b.160
				\todo{Content}
			% end

			\subsubsection{Randomized Structure Learning: RAT-SPNs} % 10b.161, 19b.162
				\todo{Content}
			% end

			\subsubsection{Extremely Randomized CNets: XCNets} % 10b.166, 10b.167
				\todo{Content}
			% end

			\subsubsection{Learning (Tree-)SPNs} % 10a.47, 10a.48, 10a.49, 10a.50
				\todo{Content}
			% end
		% end

		\subsection{Ensembles of Probabilistic Circuits} % 10b.163, 10b.164, 10b.165
			\todo{Content}
		% end

		\subsection{Online Learning} % 10b.168
			\todo{Content}
		% end

		\subsection{Knowledge Compilation} % 10b.169, 10b.170
			\todo{Content}
		% end

		\subsection{Hybridizing TPMs with Intractable Models} % 10b.171
			\todo{Content}

			\subsubsection{Sum-Product Graphical Model (SPGM)} % 10b.172
				\todo{Content}
			% end

			\subsubsection{Sum-Product Variational Auto-Encoder (SPVAE)} % 10b.173
				\todo{Content}
			% end
		% end
	% end

	\section{Applications} % 10b.174, 10b.175, 10b.176, 10b.177, 10b.181, 10b.188, 10b.191
		\todo{Content}

		\paragraph{Computer Vision} % 10b.178, 10a.5
			\todo{Content}
		% end

		\paragraph{Image Segmentation} % 10b.179
			\todo{Content}
		% end

		\paragraph{Scene Understanding: Su-PAIR} % 10b.180
			\todo{Content}
		% end

		\paragraph{Activity Recognition} % 10b.182
			\todo{Content}
		% end

		\paragraph{Speec Reconstriction and Extension} % 10b.183, 10a.4
			\todo{Content}
		% end

		\paragraph{Sequence Labeling} % 10b.184
			\todo{Content}
		% end

		\paragraph{Robotics} % 10b.185
			\todo{Content}
		% end

		\paragraph{SOP: Preference Learning} % 10b.186
			\todo{Content}
		% end

		\paragraph{SOP: Routing} % 10b.187
			\todo{Content}
		% end

		\paragraph{Probabilistic Programming} % 10b.189
			\todo{Content}
		% end

		\paragraph{And more\dots} % 10b.190
			\todo{Content}
		% end
	% end

	\section{Takeaways and Open Challenges} % 10b.192, 10b.193, 10b.194, 10b.195
		\todo{Content}
	% end
% end

\chapter{Natural Language Processing} % 11.1, 11.2, 11.82, 11.84
	\todo{Content}

	\section{Text Semantics} % 11.3, 11.4, 11.39
		\todo{Content}

		\subsection{Propositional Semantics} % 11.5, 11.6, 11.7, 11.8, 11.9, 11.10, 11.11, 11.12, 11.13, 11.14
			\todo{Content}

			\subsubsection{Vector Embeddings and Similarity} % 11.15, 11.16, 11.17
				\todo{Content}
			% end

			\subsubsection{Latent Semantic Analysis} % 11.18, 11.19, 11.20, 11.21, 11.22, 11.23
				\todo{Content}
			% end
		% end

		\subsection{Word2Vec} % 11.24, 11.25, 11.26, 11.27, 11.28, 11.29, 11.30
			\todo{Content}

			\subsubsection{Learned Relations} % 11.31, 11.32, 11.33, 11.34, 11.35, 11.36, 11.37, 11.38
				\todo{Content}
			% end
		% end

		\subsection{Skip-Thought Vectors} % 11.40, 11.41, 11.42, 11.43
			\todo{Content}

			\subsubsection{Sentence Similarity and Relatedness} % 11.44, 11.45, 11.46, 11.47
				\todo{Content}
			% end
		% end

		\subsection{Siamese Models} % 11.48, 11.49, 11.50, 11.51, 11.52
			\todo{Content}

			\subsubsection{Semantic Entailment} % 11.53, 11.54
				\todo{Content}
			% end
		% end
	% end

	\section{Translation Models} % 11.55
		\todo{Content}

		\subsection{Sequence-to-Sequence RNNs} % 11.56, 11.57
			\todo{Content}

			\subsubsection{Bleu Scores for Translation} % 11.58, 11.59, 11.60, 11.61, 11.62, 11.63, 11.64, 11.65, 11.66
				\todo{Content}
			% end

			\subsubsection{Sequence-to-Sequence Model Translation} % 11.67, 11.68, 11.69, 11.70
				\todo{Content}
			% end
		% end

		\subsection{State-of-the-Art Neural Machine Translation} % 11.71, 11.72
			\todo{Content}

			\subsubsection{Parsing} % 11.73, 11.74, 11.75
				\todo{Content}
			% end

			\subsubsection{Sequence-to-Sequence Parser} % 11.76, 11.77, 11.78, 11.79
				\todo{Content}
			% end

			\subsubsection{Neural Entity-Relation Extraction} % 11.80, 11.81, 11.82
				\todo{Content}
			% end
		% end
	% end

	\section{Attention Models} % 12.1, 12.3, 12.4, 12.5, 12.6, 13.2, 13.3, 13.4, 13.5
		\todo{Content}

		\subsection{Hard Attention for Recognition} % 12.7, 12.8, 12.9
			\todo{Content}
		% end

		\subsection{Soft Attention for Translation} % 12.10, 12.11, 12.12, 12.13, 12.14, 12.15, 12.16, 12.17, 12.18, 12.19, 12.20, 12.21, 12.22, 12.23
			\todo{Content}
		% end

		\subsection{Global and Local Attention Model} % 12.24, 12.25, 12.26
			\todo{Content}
		% end

		\subsection{Soft Attention for Captioning} % 12.27, 12.28, 12.29, 12.30, 12.31, 12.35
			\todo{Content}

			\subsubsection{Soft Attention for Video} % 12.32, 12.33, 12.34
				\todo{Content}
			% end
		% end

		\subsection{Attending to Arbitrary Regions} % 12.36, 12.37
			\todo{Content}

			\subsubsection{DRAW} % 12.38, 12.39
				\todo{Content}
			% end

			\subsubsection{Spatial Transformer Networks} % 12.40, 12.41, 12.42, 12.43, 12.44
				\todo{Content}
			% end
		% end

		\subsection{Takeaways} % 12.45, 12.46
			\todo{Content}
		% end
	% end

	\section{Transformer Networks} % 13.1, 13.6, 13.7, 13.8, 13.9, 13.10, 13.11
		\todo{Content}

		\subsection{GPT and GPT-2} % 13.12
			\todo{Content}
		% end

		\subsection{BERT (Bidirectional Encoder Representation from Transformers)} % 13.13
			\todo{Content}
		% end
	% end
% end
