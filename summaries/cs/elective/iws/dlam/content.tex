% !TeX spellcheck = en_US


\chapter{Introduction}
	This summary of the course "Deep Learning: Architectures and Methods" held at the TU Darmstadt covers a lot of topics in the wast field of deep learning and neural network architectures. The first part focuses on fully connected and convolutional networks and in the end some more advanced architectures are touched, namely recurrent, long short-term memory and transformer networks. Knowledge of basic machine learning taxonomy like "features", "training data" and "supervised learning" is assumed to be known as well as basic mathematical knowledge.

	The goal of deep architectures is learning a feature hierarchy where higher-level features are built on top of lower-level features. This is done by stacking multiple linear layers (perceptrons) on top of each other with nonlinearities between them to get a larger support. While it is, due to the universal function approximation theorem, not necessary to have multiple layers (one hidden layer is enough to approximate any sufficiently well-behaving function), it has been shown empirically that multiple layers improve the performance. In the last century the extreme computational power available leveraged the success of deep learning. This success was so extraordinary that deep learning became a hype that everyone jumped onto.

	This first chapter covers some milestones and history as well as the reason for the extreme success of deep learning.

	\section{Milestones}
		The first successful application of deep learning was in 1989 with LeNet to recognize zip codes. Image detection and classification was develop a lot further since that and in 2012, AlexNet broke the human prediction error on ImageNet, a collection of (then) \SI{200}{GB} labeled images. But deep learning is not only useful for image classification: In 2013, DeepMind beat the best human players on basic Arcade games and in 2016 AlphaGo defeated the world champion in the game Go. All of this was possible due to the massive power of deep neural networks and reinforcement learning.
	% end

	\section{The (Surprising?) Success of Deep Neural Networks}
		It is interesting that deep neural networks are so successful as they are built from extremely simple blocks. Also, the algorithms used for training these networks are extremely basic gradient descent descendants that only find local minima. In other words: they are greedy algorithms which are even more limited in what they can do than the network itself (if a problem has a greedy solution, it is usually regarded as an "easy" problem). But still they are extremely powerful!

		This is partially due to that hierarchical representations as induced by neural networks, are ubiquitous in artificial intelligence and information representation in general (e.g., images are hierarchical as well as natural language). Also, it seems like most learning problem are actually (relatively) easy and they have a gradient descent path towards a good model.
	% end
% end

\chapter{Basics} % N/A
	\todo{Content}

	\section{Linear Classifier} % 2.4, 2.5, 2.6, 2.7
		\todo{Content}
	% end

	\section{Random Variables, Distributions, and Bayes Rule} % 10a.8, 10a.9, 10a.10, 10a.11, 10a.12
		\todo{Content}
	% end

	\section{Bias and Variance, Loss Functions, and Weight Regularization} % 2.8, 2.9, 2.10, 2.11
		\todo{Content}
	% end
% end

\chapter{Optimization} % 2.1, 2.12, 2.13, 2.14, 2.81
	\todo{Content}

	\section{Gradient Descent} % 2.17, 2.33, 2.34
		\todo{Content}

		\subsection{Evaluating the Gradient} % 2.25, 2.26, 2.27, 2.28, 2.29, 2.30, 2.32, 3.4
			\todo{Content}
		% end

		\subsection{Mini-Batch and Stochastic Gradient Descent} % 2.35, 2.36, 2.37, 2.38, 2.39, 2.40, 2.41, 4.2, 5.40
			\todo{Content}
		% end
	% end

	\section{Newton's Method and L-BFGS} % 2.42, 2.43, 2.44, 2.45, 2.46, 2.47, 2.48, 2.49, 2.50
		\todo{Content}
	% end

	\section{Convergence} % 2.51, 2.52, 2.53, 2.54
		\todo{Content}
	% end

	\section{Momentum} % 2.56, 2.57, 2.58, 2.59, 2.60
		\todo{Content}

		\subsection{Nesterov Momentum} % 2.61, 2.62, 2.63, 2.64, 2.65, 2.66, 2.67
			\todo{Content}
		% end

		\subsection{AdaGrad} % 2.68, 2.69, 2.70
			\todo{Content}
		% end

		\subsection{RMSProp} % 2.71, 2.72, 2.73, 2.74
			\todo{Content}
		% end

		\subsection{Adam} % 2.75, 2.76, 2.77, 2.78
			\todo{Content}
		% end
	% end

	\section{Learning Rate} % 2.79, 2.80
		\todo{Content}
	% end
% end

\chapter{Backpropagation} % 3.1, 3.2, 3.4, 3.5, 3.61, 4.3, 4.4
	\todo{Content}

	\section{Differentiating a Computational Graph} % 3.7, 3.8, 3.9, 3.10, 3.11, 3.12, 3.13, 3.14, 3.15, 3.16, 3.17, 3.18, 3.19, 3.20, 3.21, 3.22, 3.23, 3.24, 3.25, 3.26, 3.27, 3.28, 3.29, 3.30, 3.31, 3.32, 3.33, 3.34, 3.35, 3.36, 3.37, 3.38, 3.39, 3.40, 3.41
		\todo{Content}
	% end

	\section{Patterns in the Backward Flow} % 3.42, 3.43
		\todo{Content}
	% end

	\section{Forward and Backward Differentiation} % 3.47, 3.48, 3.49, 4.5, 4.6, 4.7
		\todo{Content}
	% end

	\section{Vectorized Operations} % 3.55, 3.56, 3.57, 3.58, 3.59
		\todo{Content}
	% end

	\section{Neural Networks} % 3.62, 3.63, 3.64, 3.65, 3.66, 3.67, 3.83, 3.84
		\todo{Content}

		\subsection{Brain Analogies} % 3.72, 3.73, 3.74, 3.75, 3.76
			\todo{Content}
		% end

		\subsection{Activation Functions} % 3.77, 4.30, 4.31, 4.32, 4.46
			\todo{Content}

			\subsubsection{Sigmoid} % 4.33, 4.34, 4.35
				\todo{Content}
			% end

			\subsubsection{Tanh} % 4.36
				\todo{Content}
			% end

			\subsubsection{Rectified Linear Unit (ReLU)} % 4.37, 4.38, 4.39, 4.40, 4.41
				\todo{Content}
			% end

			\subsubsection{Leaky and Parametric ReLU} % 4.42, 4.43
				\todo{Content}
			% end

			\subsubsection{Exponential Linear Unit (ELU)} % 4.44
				\todo{Content}
			% end

			\subsubsection{Maxout} % 4.45
				\todo{Content}
			% end
		% end

		\subsection{Regularization} % 3.81, 3.82
			\todo{Content}
		% end
	% end
% end

\chapter{Training Neural Networks} % 4.1, 4.29, 4.101, 4.102, 5.1, 5.2, 5.3, 5.48
	\todo{Content}

	\section{History} % 4.24, 4.25, 4.26, 4.27, 4.28
		\todo{Content}
	% end

	\section{Data Pre-Processing} % 4.47, 4.48, 4.49, 4.50
		\todo{Content}
	% end

	\section{Weight Initialization} % 4.51, 4.52, 4.53, 4.54, 4.63
		\todo{Content}

		\subsection{Activation Statistics} % 4.55, 4.56, 4.57, 4.58, 4.59, 4.60, 4.61, 4.62
			\todo{Content}
		% end

		\subsection{Batch Normalization} % 4.64, 4.65, 4.66, 4.67, 4.68, 4.69
			\todo{Content}
		% end
	% end

	\section{Babysitting the Learning Process} % 4.70, 4.71, 4.72, 4.73, 4.74, 4.75, 4.76, 4.77, 4.78, 4.79, 4.80, 4.81, 4.82, 4.82
		\todo{Content}
	% end

	\section{Hyperparameter Optimization} % 4.84, 4.85, 4.86, 4.87, 4.88, 4.89, 4.90, 4.91, 4.92, 4.93, 4.94, 4.95, 4.96, 4.97, 4.98, 4.99, 4.100
		\todo{Content}
	% end

	\section{Ensemble Learning} % 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 5.10, 5.11, 5.12, 5.13, 5.14, 5.15, 5.16, 5.17
		\todo{Content}
	% end

	\section{Dropout Regularization} % 5.18, 5.19, 5.20, 5.21, 5.22, 5.23, 5.30
		\todo{Content}

		\subsection{At Test Time} % 5.24, 5.25, 5.26, 5.27, 5.28, 5.29
			\todo{Content}
		% end

		\subsection{Inverted Dropout} % 5.31
			\todo{Content}
		% end
	% end

	\section{Gradient Clipping} % 5.32, 5.33, 5.34, 5.35, 5.36, 5.39
		\todo{Content}

		\subsection{Extreme Gradient Clipping} % 5.37
			\todo{Content}
		% end

		\subsection{One-Bit Gradient} % 5.38
			\todo{Content}
		% end
	% end

	\section{Gradient Noise} % 5.41, 5.42, 5.43, 5.44, 5.45, 5.46, 5.47
		\todo{Content}
	% end
% end

\chapter{Convolutional Neural Networks} % 6.1, 6.2, 6.3, 6.64, 6.65, 6.66, 6.67, 6.68, 6.170
	\todo{Content}

	\section{Biology} % N/A
		\todo{Content}

		\subsection{Retinal Receptive Fields} % 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 6.10, 6.11, 6.12, 6.13, 6.14
			\todo{Content}
		% end

		\subsection{Cortical Receptive Fields} % 6.15, 6.16, 6.17, 6.18, 6.19, 6.20, 6.21, 6.22, 6.23, 6.24, 6.25, 6.26, 6.27, 6.28
			\todo{Content}
		% end
	% end

	\section{Convolutions} % 6.29, 6.30, 6.31, 6.32, 6.33, 6.34, 6.35, 6.36, 6.37, 6.38
		\todo{Content}

		\subsection{Examples} % 6.39, 6.40, 6.41, 6.42, 6.43, 6.44, 6.45, 6.46, 6.47, 6.48, 6.49, 6.50, 6.51, 6.52, 6.53, 6.54, 6.55, 6.56, 6.57, 6.58, 6.59, 6.60, 6.61, 6.62, 6.63
			\todo{Content}
		% end
	% end

	\section{Convolutional Layers} % 6.66, 6.67, 6.68, 6.69, 6.70, 6.71, 6.72, 6.73, 6.74, 6.75, 6.76, 6.77, 6.78, 6.79, 6.80, 6.81, 6.82, 6.83, 6.84, 6.85, 6.86, 6.120, 6.121
		\todo{Content}

		\subsection{Activation Volume} % 6.87, 6.88, 6.89, 6.90, 6.91, 6.92, 6.93, 6.94, 6.95, 6.96, 6.115, 6.122
			\todo{Content}
		% end

		\subsection{Zero-Padding and Stride} % 6.100, 6.101, 6.102, 6.103, 6.104, 6.105, 6.106, 6.107, 6.108, 6.109, 6.110, 6.111, 6.112, 6.113, 6.114
			\todo{Content}
		% end

		\subsection{Neuron View} % 6.123, 6.124, 6.125, 6.126
			\todo{Content}
		% end
	% end

	\section{Pooling} % 6.127, 6.128, 6.129, 6.130, 6.131
		\todo{Content}
	% end

	\section{Examples} % N/A
		\todo{Content}

		\paragraph{LeNet-5} % 6.133, 6.134, 7.11
			\todo{Content}
		% end

		\paragraph{AlexNet} % 6.136, 6.137, 6.138, 6.139, 6.140, 6.141, 6.142, 6.143, 7.12, 7.13
			\todo{Content}
		% end
	% end

	\section{Transfer Learning} % 6.144, 6.145, 6.146, 6.147, 6.148, 6.149, 6.150, 6.151
		\todo{Content}

		\subsection{Examples} % N/A
			\todo{Content}

			\paragraph{VGGNet} % 6.152, 6.153, 6.154, 6.155
				\todo{Content}
			% end

			\paragraph{GoogLeNet} % 6.156, 6.157
				\todo{Content}
			% end

			\paragraph{ResNet} % 6.158, 6.159, 6.160, 6.161, 6.162, 6.163, 6.164, 6.165, 6.166, 6.167, 7.26, 7.27, 7.28, 7.32, 7.34
				\todo{Content}
			% end

			\paragraph{AlphaGo} % 6.168, 6.169
				\todo{Content}
			% end
		% end
	% end
% end

\chapter{Computer Vision Tasks} % 7.36, 7.37, 7.38, 7.118
	\todo{Content}

	\section{Classification + Localization} % 7.39, 7.40, 7.63
		\todo{Content}

		\subsection{Localization as Regression} % 7.41, 7.42, 7.43, 7.44, 7.45, 7.46, 7.47
			\todo{Content}

			\subsubsection{Localizing Multiple Objects} % 7.48
				\todo{Content}
			% end

			\subsubsection{Human Pose Estimation} % 7.49
				\todo{Content}
			% end
		% end

		\subsection{Sliding Window} % 7.50, 7.51, 7.52, 7.53, 7.54, 7.55, 7.56, 7.57, 7.58, 7.59, 7.60, 7.61, 7.62
			\todo{Content}
		% end
	% end

	\section{Object Detection} % 7.65, 7.87, 7.88, 7.117
		\todo{Content}

		\subsection{Detection as Regression} % 7.66, 7.67, 7.68
			\todo{Content}
		% end

		\subsection{Detection as Classification} % 7.69, 7.70, 7.71, 7.72, 7.73, 7.76
			\todo{Content}

			\subsubsection{Deformable Parts Model (DPM)} % 7.74, 7.75
				\todo{Content}
			% end

			\subsubsection{Region Proposals} % 7.77, 7.78, 7.79, 7.80
				\todo{Content}
			% end
		% end

		\subsection{R-CNN} % 7.81, 7.82, 7.83, 7.84, 7.85, 7.86, 7.89, 7.90, 7.91, 7.92, 7.93
			\todo{Content}
		% end

		\subsection{Fast R-CNN} % 7.94, 7.95, 7.96, 7.98, 7.98, 7.99, 7.100, .101, 7.102, 7.103, 7.104, 7.105, 7.106
			\todo{Content}
		% end

		\subsection{Faster R-CNN} % 7.107, 7.108, 7.109, 7.110, 7.111, 7.112, 7.113
			\todo{Content}
		% end

		\subsection{YOLO: You Only Look Once} % 7.114, 7.115
			\todo{Content}
		% end
	% end
% end

\chapter{Recurrent Neural Networks} % 8.1, 8.4, 8.5, 8.23, 8.24, 8.25, 8.26, 8.27, 8.28, 8.29, 8.98
	\todo{Content}

	\section{Unrolling and Backprop-Through-Time} % 8.6, 8.7, 8.8, 8.9, 8.10, 8.11, 8.12, 8.13, 8.14, 8.15, 8.16, 8.17, 8.18, 8.19, 8.20, 8.21, 8.22
		\todo{Content}
	% end

	\section{Vanilla RNN} % 8.30, 8.31, 8.32, 8.33, 8.34
		\todo{Content}

		\subsection{Example: Character-Level Language Model} % 8.35, 8.36, 8.37, 8.38, 8.47, 8.48, 8.52, 8.53, 8.54, 8.55, 8.56, 8.57, 8.58, 8.59, 8.60, 8.61
			\todo{Content}

			\subsubsection{Interpretable Cells} % 8.62, 8.63, 8.64, 8.65, 8.66, 8.67
				\todo{Content}
			% end
		% end

		\subsection{Image Captioning} % 8.68, 8.69, 8.70, 8.71, 8.72, 8.73, 8.74, 8.75, 8.76, 8.77, 8.78, 8.79, 8.80, 8.81, 8.82, 8.83
			\todo{Content}
		% end
	% end

	\section{Long Short-Term Memory (LSTM)} % 8.84, 8.85, 8.86, 8.87, 8.88, 8.89, 8.90, 8.91, 8.92, 8.93
		\todo{Content}

		\subsection{Gradient Flow Dynamics} % 8.94, 8.95, 8.96
			\todo{Content}
		% end

		\subsection{Variants and Friends} % 8.97
			\todo{Content}
		% end
	% end
% end

\chapter{Generative Models} % 9b.1, 9b.3, 9b.4, 9b.5, 9b.6, 9b.7, 9b.8, 9b.9, 9b.10, 9b.11, 9b.12, 9b.13, 9b.14, 9b.15, 9b.16, 9b.17, 9b.18, 9b.19, 9b.20, 9b.131, 9b.132, 9b.133, 9a.3, 9a.4, 9a.5, 9a.6, 9a.7, 9a.8
	\todo{Content}

	\section{Fully Visible Belief Network} % 9b.22, 9b.23, 9b.24, 9a.9
		\todo{Content}
	% end

	\section{WaveNet} % 9a.10
		\todo{Content}
	% end

	\section{Change of Variables} % 9a.11
		\todo{Content}
	% end

	\section{Boltzmann Machine} % 9a.13
		\todo{Content}
	% end

	\section{PixelRNN and PixelCNN} % 9b.21, 9b.25, 9b.26, 9b.27, 9b.28, 9b.29, 9b.30, 9b.31, 9b.32, 9b.33
		\todo{Content}
	% end

	\section{Variational Auto-Encoder (VAE)} % 9b.34, 9b.35, 9b.36, 9b.97, 9a.12
		\todo{Content}

		\subsection{Auto-Encoder} % 9b.37, 9b.38, 9b.39, 9b.40, 9b.41, 9b.42, 9b.43, 9b.44, 9b.45, 9b.46, 9b.47, 9b.48, 9b.49
			\todo{Content}
		% end

		\subsection{Model} % 9b.50, 9b.51, 9b.52, 9b.53, 9b.54, 9b.55, 9b.56, 9b.57, 9b.58, 9b.59, 9b.60, 9b.61
			\todo{Content}
		% end

		\subsection{Intractability} % 9b.62, 9b.63, 9b.64, 9b.65, 9b.66, 9b.67, 9b.68
			\todo{Content}
		% end

		\subsection{Encoder, Decoder, and Evidence Lower Bound (ELBO)} % 9b.69, 9b.70, 9b.71, 9b.72, 9b.73, 9b.74, 9b.75, 9b.76, 9b.77, 9b.78, 9b.79, 9b.80, 9b.81, 9b.82
			\todo{Content}
		% end

		\subsection{Training Procedure} % 9b.83, 9b.84, 9b.85, 9b.86, 9b.87, 9b.88, 9b.89, 9b.90
			\todo{Content}
		% end

		\subsection{Generating Data} % 9b.91, 9b.92, 9b.93, 9b.94, 9b.95, 9b.96
			\todo{Content}
		% end
	% end

	\section{Generative Adversarial Networks (GANs)} % , 9b.114, 9b.115, 9b.130, 9a.1, 9a.2, 9a.14, 9a.15, 9a.16
		\todo{Content}

		\subsection{Two-Player Game} % 9b.104, 9b.105, 9b.106, 9b.107, 9b.108, 9b.109
			\todo{Content}

			\subsubsection{Optimization Problems} % 9b.110, 9b.111, 9b.112
				\todo{Content}
			% end
		% end

		\subsection{Convolutional Architectures} % 9b.118, 9b.119, 9b.120, 9b.121
			\todo{Content}

			\subsubsection{Interpretability} % 9b.122, 9b.123, 9b.124, 9b.125, 9b.126
				\todo{Content}
			% end
		% end
	% end

	\section{Generative Adversarial Networks (GANs)} % 9a.14, 9a.15, 9a.16, 9a.33, 9a.50, 9b.98, 9b.99, 9b.100, 9b.101, 9b.102, 9b.103, 9b.130
		\todo{Content}

		\subsection{Training Procedure} % 9a.17, 9a.23, 9a.34, 9b.104, 9b.105, 9b.113, 9b.114, 9b.115
			\todo{Content}

			\subsubsection{Minimax, Non-Saturating, and Maximum Likelihood Games} % 9a.18, 9a.19, 9a.20, 9a.21, 9b.106, 9b.107, 9b.108, 9b.109
				\todo{Content}
			% end

			\subsubsection{Discriminator Strategy} % 9a.22
				\todo{Content}
			% end

			\subsubsection{Mode Collapse} % 9a.29, 9b.110, 9b.111, 9b.112
				\todo{Content}
			% end
		% end

		\subsection{Convolutional Architectures} % 9b.118, 9b.119, 9b.120, 9b.121
			\todo{Content}
		% end

		\subsection{Vector Space Arithmetic} % 9a.28, 9b.122, 9b.123, 9b.124, 9b.125, 9b.126
			\todo{Content}
		% end
	% end

	\section{Optimization and Games} % 9a.37, 9a.38
		\todo{Content}

		\subsection{Nash Equilibrium} % 9a.39
			\todo{Content}
		% end

		\subsection{Well-Studies Cases} % 9a.40
			\todo{Content}

			\subsubsection{Continuous Minimax Game} % 9a.41
				\todo{Content}
			% end

			\subsubsection{Local Differential Nash Equilibria} % 9a.42
				\todo{Content}
			% end

			\subsubsection{Gradient Descent Convergence} % 9a.43, 9a.44
				\todo{Content}
			% end
		% end

		\subsection{Heuristics} % 9a.45, 9a.46
			\todo{Content}
		% end

		\subsection{Other Games in AI} % 9a.47
			\todo{Content}
		% end
	% end
% end

\chapter{Probabilistic Graphical Models} % 10b.6, 10b.7, 10b.8, 10b.9, 10b.10, 10b.11, 10b.12, 10b.13, 10b.14, 10b.15, 10b.16, 10b.17, 10b.18, 10b.19, 10a.1, 10a.2, 10a.3, 10a.6, 10a.13, 10a.14, 10a.22, 10a.23, 10a.24, 10a.69
	\todo{Content}

	\section{(Conditional) Independency} % 10a.15, 10a.16
		\todo{Content}
	% end

	\section{Tractability vs. Expressiveness} % 10b.60, 10b.61, 10b.62, 10b.63
		\todo{Content}

		\subsection{Inference and Queries} % 10a.19
			\todo{Content}

			\paragraph{Complete Evidence Queries (EVI)} % 10b.20, 10b.21, 10b.22
				\todo{Content}
			% end

			\paragraph{Marginal Queries (MAR)} % 10b.31, 10b.32, 10b.33
				\todo{Content}
			% end

			\paragraph{Conditional Queries (CON)} % 10b.34, 10b.35, 10b.36
				\todo{Content}
			% end

			\paragraph{Maximum A-Posteriori (MAP)} % 10b.46, 10b.47, 10b.48, 10b.49
				\todo{Content}
			% end

			\paragraph{Marginal MAP (MMAP)} % 10b.50, 10b.51, 10b.52, 10b.53
				\todo{Content}
			% end

			\paragraph{Advanced Queries} % 10b.54, 10b.55, 10b.56, 10b.57, 10b.58
				\todo{Content}
			% end
		% end

		\subsection{Models} % N/A
			\todo{Content}

			\subsubsection{Generative Adversarial Networks} % 10b.23,, 10b.24
				\todo{Content}
			% end

			\subsubsection{Variational Autoencoders} % 10b.25, 10b.26
				\todo{Content}
			% end

			\subsubsection{Probabilistic Graphical Models: Markov and Bayes Networks} % 10b.27, 10b.28, 10b.29, 10b.30, 10b.37, 10b.38, 10a.7, 10a.17, 10a.18, 10a.19, 10a.25
				\todo{Content}

				\paragraph{Variable Elimination} % 10a.20, 10a.21
					\todo{Content}
				% end
			% end

			\subsubsection{Low-Tree-Width PGMs: Trees} % 10b.39, 10b.40, 10b.41
				\todo{Content}
			% end

			\subsubsection{Mixtures} % 10b.42, 10b.43, 10b.44, 10b.45
				\todo{Content}
			% end

			\subsubsection{Fully Factorized Models} % 10b.59
				\todo{Content}
			% end
		% end
	% end

	\section{Probabilistic Circuits} % 10b.64, 10b.65, 10b.66, 10b.67, 10b.68, 10b.69, 10b.70, 10b.71, 10b.72, 10b.73, 10b.74, 10b.75, 10b.76, 10b.77, 10b.78, 10b.79, 10b.80, 10b.81, 10b.82, 10b.104, 10b.126
		\todo{Content}

		\subsection{Ensuring Tractability} % 10b.83
			\todo{Content}

			\subsubsection{Decomposability and Smoothness} % 10b.84, 10b.85
				\todo{Content}

				\paragraph{Tractable MAR/CON} % 10b.86, 10b.87, 10b.88, 10b.89
					\todo{Content}
				% end
			% end

			\subsubsection{Determinism} % 10b.90
				\todo{Content}

				\paragraph{Tractable MAP} % 10b.91, 10b.92, 10b.93, 10b.94, 10b.95, 10b.96, 10b.97, 10b.98
					\todo{Content}
				% end

				\paragraph{Approximate MAP} % 10b.99
					\todo{Content}
				% end
			% end

			\subsubsection{Structured Decomposability} % 10b.100, 10b.101, 10b.102, 10b.103
				\todo{Content}
			% end
		% end

		\subsection{Logical Circuits} % 10b.105, 10b.106, 10b.107, 10b.108
			\todo{Content}

			\subsubsection{Weighted Model Counting (WMC)} % 10b.109
				\todo{Content}
			% end

			\subsubsection{From Trees to Circuits} % 10b.110, 10b.111, 10b.112, 10b.113, 10b.114, 10b.115, 10b.116, 10b.117
				\todo{Content}
			% end

			\subsubsection{Low-Tree-Width PGMs} % 10b.118
				\todo{Content}
			% end

			\subsubsection{Arithmetic Circuits (ACs)} % 10b.119
				\todo{Content}
			% end

			\subsubsection{Sum-Product Networks (SPNs)} % 10b.120, 10a.26, 10a.27, 10a.28, 10a.29, 10a.30, 10a.31, 10a.32, 10a.33, 10a.53, 10a.54, 10a.55, 10a.56, 10a.57
				\todo{Content}

				\paragraph{Semantics} % 10a.34
					\todo{Content}
				% end

				\paragraph{Linear Inference} % 10a.35, 10a.36, 10a.37, 10a.38
					\todo{Content}
				% end

				\paragraph{Image Completion} % 10a.40, 10a.41, 10a.42, 10a.43, 10a.44
					\todo{Content}
				% end

				\paragraph{Variants} % 10a.45, 10a.46, 10a.47
					\todo{Content}
				% end

				\paragraph{Symbolic Evaluation} % 10a.51, 10a.52
					\todo{Content}
				% end
			% end

			\subsubsection{Cutset Networks (CNets)} % 10b.121, 10b.122, 10b.123
				\todo{Content}
			% end

			\subsubsection{Probabilistic Sentential Decision Diagrams} % 10b.124
				\todo{Content}
			% end
		% end

		\subsection{Expressiveness} % 10b.127, 10b.128
			\todo{Content}
		% end
	% end

	\section{Building Circuits} % 10b.129, 10b.130, 10b.131, 10b.132, 10b.133, 10b.134, 10b.135, 10b.136, 10a.39
		\todo{Content}

		\subsection{Hard/Soft Parameter Updating: Gradient Descent and EM} % 10b.137, 10b.138
			\todo{Content}
		% end

		\subsection{(Bayesian) Parameter Learning} % 10b.139, 10b.140, 10b.141
			\todo{Content}
		% end

		\subsection{Structure Learning} % 10b.142
			\todo{Content}

			\subsubsection{LearnSPN} % 10b.143, 10b.144, 10b.145, 10b.146
				\todo{Content}

				\paragraph{ID-SPN} % 10b.148
					\todo{Content}
				% end

				\paragraph{Other Variants} % 10b.147, 10b.149
					\todo{Content}
				% end
			% end

			\subsubsection{Cut(e)set Network} % 10b.150
				\todo{Content}
			% end

			\subsubsection{PSDD Structure Learning} % 10b.151, 10b.152
				\todo{Content}
			% end

			\subsubsection{LearnPSDD} % 10b.153
				\todo{Content}
			% end

			\subsubsection{Learning Logistic Circuits} % 10b.154, 10b.155
				\todo{Content}
			% end

			\subsubsection{Bayesian Structure Learning} % 10b.156
				\todo{Content}
			% end

			\subsubsection{Automatic Bayesian Density Analysis (ABDA)} % 10b.157, 10b.158
				\todo{Content}
			% end

			\subsubsection{Bayesian SPNs} % 10b.159, 10b.160
				\todo{Content}
			% end

			\subsubsection{Randomized Structure Learning: RAT-SPNs} % 10b.161, 19b.162
				\todo{Content}
			% end

			\subsubsection{Extremely Randomized CNets: XCNets} % 10b.166, 10b.167
				\todo{Content}
			% end

			\subsubsection{Learning (Tree-)SPNs} % 10a.47, 10a.48, 10a.49, 10a.50
				\todo{Content}
			% end
		% end

		\subsection{Ensembles of Probabilistic Circuits} % 10b.163, 10b.164, 10b.165
			\todo{Content}
		% end

		\subsection{Online Learning} % 10b.168
			\todo{Content}
		% end

		\subsection{Knowledge Compilation} % 10b.169, 10b.170
			\todo{Content}
		% end

		\subsection{Hybridizing TPMs with Intractable Models} % 10b.171
			\todo{Content}

			\subsubsection{Sum-Product Graphical Model (SPGM)} % 10b.172
				\todo{Content}
			% end

			\subsubsection{Sum-Product Variational Auto-Encoder (SPVAE)} % 10b.173
				\todo{Content}
			% end
		% end
	% end

	\section{Applications} % 10b.174, 10b.175, 10b.176, 10b.177, 10b.181, 10b.188, 10b.191
		\todo{Content}

		\paragraph{Computer Vision} % 10b.178, 10a.5
			\todo{Content}
		% end

		\paragraph{Image Segmentation} % 10b.179
			\todo{Content}
		% end

		\paragraph{Scene Understanding: Su-PAIR} % 10b.180
			\todo{Content}
		% end

		\paragraph{Activity Recognition} % 10b.182
			\todo{Content}
		% end

		\paragraph{Speec Reconstriction and Extension} % 10b.183, 10a.4
			\todo{Content}
		% end

		\paragraph{Sequence Labeling} % 10b.184
			\todo{Content}
		% end

		\paragraph{Robotics} % 10b.185
			\todo{Content}
		% end

		\paragraph{SOP: Preference Learning} % 10b.186
			\todo{Content}
		% end

		\paragraph{SOP: Routing} % 10b.187
			\todo{Content}
		% end

		\paragraph{Probabilistic Programming} % 10b.189
			\todo{Content}
		% end

		\paragraph{And more\dots} % 10b.190
			\todo{Content}
		% end
	% end

	\section{Takeaways and Open Challenges} % 10b.192, 10b.193, 10b.194, 10b.195
		\todo{Content}
	% end
% end

\chapter{Natural Language Processing} % 11.1, 11.2, 11.82, 11.84
	\todo{Content}

	\section{Text Semantics} % 11.3, 11.4, 11.39
		\todo{Content}

		\subsection{Propositional Semantics} % 11.5, 11.6, 11.7, 11.8, 11.9, 11.10, 11.11, 11.12, 11.13, 11.14
			\todo{Content}

			\subsubsection{Vector Embeddings and Similarity} % 11.15, 11.16, 11.17
				\todo{Content}
			% end

			\subsubsection{Latent Semantic Analysis} % 11.18, 11.19, 11.20, 11.21, 11.22, 11.23
				\todo{Content}
			% end
		% end

		\subsection{Word2Vec} % 11.24, 11.25, 11.26, 11.27, 11.28, 11.29, 11.30
			\todo{Content}

			\subsubsection{Learned Relations} % 11.31, 11.32, 11.33, 11.34, 11.35, 11.36, 11.37, 11.38
				\todo{Content}
			% end
		% end

		\subsection{Skip-Thought Vectors} % 11.40, 11.41, 11.42, 11.43
			\todo{Content}

			\subsubsection{Sentence Similarity and Relatedness} % 11.44, 11.45, 11.46, 11.47
				\todo{Content}
			% end
		% end

		\subsection{Siamese Models} % 11.48, 11.49, 11.50, 11.51, 11.52
			\todo{Content}

			\subsubsection{Semantic Entailment} % 11.53, 11.54
				\todo{Content}
			% end
		% end
	% end

	\section{Translation Models} % 11.55
		\todo{Content}

		\subsection{Sequence-to-Sequence RNNs} % 11.56, 11.57
			\todo{Content}

			\subsubsection{Bleu Scores for Translation} % 11.58, 11.59, 11.60, 11.61, 11.62, 11.63, 11.64, 11.65, 11.66
				\todo{Content}
			% end

			\subsubsection{Sequence-to-Sequence Model Translation} % 11.67, 11.68, 11.69, 11.70
				\todo{Content}
			% end
		% end

		\subsection{State-of-the-Art Neural Machine Translation} % 11.71, 11.72
			\todo{Content}

			\subsubsection{Parsing} % 11.73, 11.74, 11.75
				\todo{Content}
			% end

			\subsubsection{Sequence-to-Sequence Parser} % 11.76, 11.77, 11.78, 11.79
				\todo{Content}
			% end

			\subsubsection{Neural Entity-Relation Extraction} % 11.80, 11.81, 11.82
				\todo{Content}
			% end
		% end
	% end

	\section{Attention Models} % 12.1, 12.3, 12.4, 12.5, 12.6, 13.2, 13.3, 13.4, 13.5
		\todo{Content}

		\subsection{Hard Attention for Recognition} % 12.7, 12.8, 12.9
			\todo{Content}
		% end

		\subsection{Soft Attention for Translation} % 12.10, 12.11, 12.12, 12.13, 12.14, 12.15, 12.16, 12.17, 12.18, 12.19, 12.20, 12.21, 12.22, 12.23
			\todo{Content}
		% end

		\subsection{Global and Local Attention Model} % 12.24, 12.25, 12.26
			\todo{Content}
		% end

		\subsection{Soft Attention for Captioning} % 12.27, 12.28, 12.29, 12.30, 12.31, 12.35
			\todo{Content}

			\subsubsection{Soft Attention for Video} % 12.32, 12.33, 12.34
				\todo{Content}
			% end
		% end

		\subsection{Attending to Arbitrary Regions} % 12.36, 12.37
			\todo{Content}

			\subsubsection{DRAW} % 12.38, 12.39
				\todo{Content}
			% end

			\subsubsection{Spatial Transformer Networks} % 12.40, 12.41, 12.42, 12.43, 12.44
				\todo{Content}
			% end
		% end

		\subsection{Takeaways} % 12.45, 12.46
			\todo{Content}
		% end
	% end

	\section{Transformer Networks} % 13.1, 13.6, 13.7, 13.8, 13.9, 13.10, 13.11
		\todo{Content}

		\subsection{GPT and GPT-2} % 13.12
			\todo{Content}
		% end

		\subsection{BERT (Bidirectional Encoder Representation from Transformers)} % 13.13
			\todo{Content}
		% end
	% end
% end
